{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n al Proyecto de Data Warehouse: Optimizaci\u00f3n del Proceso de Datos","text":""},{"location":"#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento tiene como objetivo proporcionar una visi\u00f3n integral sobre el proyecto de optimizaci\u00f3n del proceso de datos mediante la implementaci\u00f3n de un Data Warehouse (DWH). El enfoque principal de este proyecto es consolidar, transformar y almacenar datos provenientes de diversas fuentes para mejorar la integridad, calidad y disponibilidad de la informaci\u00f3n, facilitando el an\u00e1lisis de datos y la toma de decisiones estrat\u00e9gicas.</p> <p>En la era de los datos, un DWH proporciona una base s\u00f3lida que permite a las organizaciones gestionar grandes vol\u00famenes de informaci\u00f3n de manera eficiente. Este proyecto est\u00e1 estructurado en bloques funcionales, cada uno de ellos con un enfoque espec\u00edfico, que garantiza que los datos se integren de manera ordenada, estandarizada y precisa, asegurando su utilidad para el an\u00e1lisis y las consultas empresariales.</p> <p>Este proyecto est\u00e1 alineado con la iniciativa de Gobierno de Datos del proyecto de optimizaci\u00f3n CAJAMAG. Se enfoca en la automatizaci\u00f3n de los procesos de carga y transformaci\u00f3n de datos para la Bodega Temporal de Base de Datos Pivote, utilizando herramientas como Python y SQL, tal como se detalla en el documento t\u00e9cnico funcional correspondiente.</p>"},{"location":"#objetivos-del-proyecto","title":"Objetivos del Proyecto","text":"<ol> <li>Consolidaci\u00f3n de Datos desde M\u00faltiples Fuentes: Integrar datos provenientes de diversas bases, tales como datos de beneficiarios, empresas, servicios sociales, instituciones educativas, entre otros, asegurando una estructura \u00fanica y coherente.</li> <li>Mejorar la Calidad de los Datos: Implementar validaciones, transformaciones y procesos de limpieza que aseguren la integridad, consistencia y calidad de la informaci\u00f3n.</li> <li>Automatizaci\u00f3n de Procesos ETL: Utilizar herramientas como <code>Pandas</code>, <code>SQLAlchemy</code> y funciones personalizadas para automatizar la extracci\u00f3n, transformaci\u00f3n y carga (ETL) de los datos, optimizando as\u00ed el flujo de trabajo.</li> <li>Estandarizaci\u00f3n y Documentaci\u00f3n de los Procesos: Proporcionar una documentaci\u00f3n clara y organizada de cada bloque de procesamiento de datos para facilitar su uso, replicabilidad y mantenimiento.</li> <li>Facilitar la Toma de Decisiones: Proveer informaci\u00f3n estructurada y de alta calidad que soporte el an\u00e1lisis de indicadores clave y la toma de decisiones basada en datos.</li> </ol>"},{"location":"#estructura-del-proyecto-por-bloques","title":"Estructura del Proyecto por Bloques","text":""},{"location":"#bloque-1x-datos-fijos-y-calendario-mensual","title":"Bloque 1.X: Datos Fijos y Calendario Mensual","text":"<ul> <li>Tablas involucradas: <code>DimDatosFijos</code>, <code>DimCalendarioMensual</code>.</li> <li>Descripci\u00f3n: Estos bloques se enfocan en la consolidaci\u00f3n de informaci\u00f3n est\u00e1tica y de calendario mensual, crucial para el an\u00e1lisis temporal y el enriquecimiento de los datos transaccionales.</li> </ul>"},{"location":"#bloque-2x-aportes-cuotas-y-subsidios","title":"Bloque 2.X: Aportes, Cuotas y Subsidios","text":"<ul> <li>Tablas involucradas: <code>AportesEmpresasAfiliadas</code>, <code>HistoricoAportesTotales</code>, <code>HistoricoCuotas</code>, <code>EstadoGiroCuota</code>, <code>SubsidioVivienda</code>, <code>Fosfec</code>, <code>HistoricoMoraEmpresas</code>.</li> <li>Descripci\u00f3n: Integran los datos relacionados con aportes empresariales, subsidios, cuotas y morosidad. Aseguran la correcta transformaci\u00f3n y carga de estos datos para apoyar los an\u00e1lisis financieros y la gesti\u00f3n de riesgos.</li> </ul>"},{"location":"#bloque-3x-ventas-de-servicios","title":"Bloque 3.X: Ventas de Servicios","text":"<ul> <li>Tabla involucrada: <code>FactTransaccionesVentas</code>.</li> <li>Descripci\u00f3n: Gestiona la informaci\u00f3n de ventas de servicios, incluyendo los detalles de cada transacci\u00f3n, y almacena estos datos en el DWH para su posterior an\u00e1lisis. Adem\u00e1s, proporciona un contexto detallado sobre las caracter\u00edsticas de las ventas, lo cual permite realizar an\u00e1lisis de tendencias y desempe\u00f1o.</li> </ul>"},{"location":"#bloque-4x-instituciones-educativas","title":"Bloque 4.X: Instituciones Educativas","text":"<ul> <li>Tablas involucradas: <code>FactColegio_fijos</code>, <code>FactColegio</code>.</li> <li>Descripci\u00f3n: Integra datos de estudiantes, acudientes, matr\u00edculas y grados provenientes de instituciones educativas, garantizando que la informaci\u00f3n est\u00e9 normalizada y sea f\u00e1cil de analizar. Esto facilita el seguimiento del progreso acad\u00e9mico y el an\u00e1lisis de la efectividad de las iniciativas educativas.</li> </ul>"},{"location":"#bloque-5x-servicio-social-y-datos-personales","title":"Bloque 5.X: Servicio Social y Datos Personales","text":"<ul> <li>Tablas involucradas: <code>FactServicioSocial_Fijos</code>, <code>FactServicioSocial</code>.</li> <li>Descripci\u00f3n: Administra datos personales relacionados con servicios sociales, asegurando la consistencia y calidad de la informaci\u00f3n para facilitar la consulta y el an\u00e1lisis detallado. Estos datos permiten evaluar el impacto de los programas de asistencia social y la cobertura de los beneficiarios.</li> </ul>"},{"location":"#bloque-6x-encuestas-y-opiniones-de-afiliados","title":"Bloque 6.X: Encuestas y Opiniones de Afiliados","text":"<ul> <li>Tablas involucradas: <code>FactEncuestas</code>, <code>Fact_Encuesta_afil_empleador</code>.</li> <li>Descripci\u00f3n: Carga y transforma los datos de encuestas realizadas a afiliados y empleadores, permitiendo la evaluaci\u00f3n de la satisfacci\u00f3n y el rendimiento de los servicios brindados. Las respuestas de las encuestas son fundamentales para identificar \u00e1reas de mejora en los servicios ofrecidos por la organizaci\u00f3n.</li> </ul>"},{"location":"#tecnologias-y-herramientas-utilizadas","title":"Tecnolog\u00edas y Herramientas Utilizadas","text":"<p>Para asegurar la eficiencia y automatizaci\u00f3n de los procesos, el proyecto utiliza un conjunto de herramientas tecnol\u00f3gicas y librer\u00edas de Python, incluyendo:</p> <ul> <li><code>sqlalchemy</code>: Para la conexi\u00f3n y manipulaci\u00f3n de bases de datos.</li> <li><code>pandas</code>: Para la manipulaci\u00f3n y an\u00e1lisis de datos, permitiendo aplicar transformaciones complejas de manera eficiente.</li> <li><code>numpy</code>: Para operaciones matem\u00e1ticas y manejo de arreglos num\u00e9ricos, esenciales para c\u00e1lculos estad\u00edsticos y otras operaciones.</li> <li><code>logging</code>: Para registrar las actividades y errores durante la ejecuci\u00f3n del proceso ETL, proporcionando un historial detallado de las actividades realizadas.</li> <li><code>concurrent.futures</code>: Para la paralelizaci\u00f3n de tareas y optimizaci\u00f3n del tiempo de procesamiento, lo que permite un manejo eficiente de grandes vol\u00famenes de datos.</li> <li><code>beautifulsoup4</code>: Para limpiar el texto HTML y asegurar la calidad de los datos ingresados desde fuentes web, eliminando elementos innecesarios y garantizando una entrada homog\u00e9nea de datos.</li> </ul> <p>Adicionalmente, se utilizan funciones personalizadas contenidas en el archivo <code>Funciones.py</code> que permiten automatizar y simplificar tareas comunes del proceso ETL, tales como la conexi\u00f3n a bases de datos, la validaci\u00f3n de duplicados y la limpieza de direcciones.</p> <p>El proceso de transformaci\u00f3n de datos se lleva a cabo principalmente en MySQL, con algunas operaciones en Python mediante Jupyter notebooks, tal como se indica en el detalle del modelo de transformaci\u00f3n de la Bodega Temporal.</p>"},{"location":"#logica-del-modelo-y-transformaciones","title":"L\u00f3gica del Modelo y Transformaciones","text":"<p>El modelo de datos de la Bodega Temporal se basa en la consolidaci\u00f3n y transformaci\u00f3n de m\u00faltiples fuentes de datos en tablas pivote, dise\u00f1adas para soportar an\u00e1lisis posteriores en el DWH. Las transformaciones incluyen:</p> <ul> <li>Estandarizaci\u00f3n de Direcciones: Utilizando funciones como <code>estandarizar_direccion()</code> y <code>marcar_direcciones_estandarizadas()</code> para garantizar consistencia en la entrada de datos de localizaci\u00f3n. Esta estandarizaci\u00f3n es esencial para asegurar que los datos geogr\u00e1ficos sean precisos y f\u00e1ciles de analizar.</li> <li>Validaci\u00f3n y Detecci\u00f3n de Duplicados: Mediante <code>StoreDuplicated()</code> se identifican y almacenan registros duplicados para an\u00e1lisis posteriores, lo cual asegura que solo datos \u00fanicos se almacenen en las tablas destino. Esto previene problemas de inconsistencia en los an\u00e1lisis.</li> <li>Conexi\u00f3n y Carga Automatizada: <code>guardar_en_dwh()</code> y <code>cargar_tablas()</code> se utilizan para realizar la carga de datos en el DWH, reemplazando o insertando los registros seg\u00fan las necesidades de cada bloque. Esta automatizaci\u00f3n reduce significativamente los errores humanos y garantiza que los datos siempre est\u00e9n actualizados.</li> <li>Paralelizaci\u00f3n de Procesos: Se utiliza la librer\u00eda <code>concurrent.futures</code> para procesar consultas SQL y ejecutar transformaciones en paralelo, optimizando as\u00ed el tiempo de ejecuci\u00f3n. Esto es particularmente \u00fatil cuando se manejan grandes vol\u00famenes de informaci\u00f3n que requieren procesamiento simult\u00e1neo.</li> <li>Normalizaci\u00f3n de Tablas: Se aplican procesos de normalizaci\u00f3n para dividir datos en estructuras m\u00e1s simples, evitando redundancia y facilitando las actualizaciones. La normalizaci\u00f3n tambi\u00e9n mejora la eficiencia del almacenamiento y simplifica el mantenimiento de los datos.</li> </ul>"},{"location":"#tablas-del-modelo","title":"Tablas del Modelo","text":"<p>Las tablas principales del modelo incluyen:</p> <ul> <li>DimDatosFijos: Tabla que almacena informaci\u00f3n est\u00e1tica sobre los afiliados y empresas, proporcionando un contexto estable para los an\u00e1lisis.</li> <li>DimCalendarioMensual: Incluye las fechas de cada mes con informaci\u00f3n relevante para el an\u00e1lisis temporal, tales como d\u00edas laborables y feriados.</li> <li>FactTransaccionesVentas: Tabla que almacena las transacciones de ventas de servicios realizadas, proporcionando un detalle granular de cada venta, incluyendo montos y tipos de servicios ofrecidos.</li> <li>FactServicioSocial: Integra informaci\u00f3n sobre los servicios sociales, proporcionando datos relevantes sobre estudiantes, acudientes y beneficiarios. Esto ayuda a evaluar el impacto de los servicios sociales sobre diferentes grupos de la poblaci\u00f3n.</li> <li>FactColegio: Almacena datos sobre estudiantes, sus acudientes y detalles de matr\u00edculas, facilitando an\u00e1lisis educativos, como la tasa de retenci\u00f3n escolar y el rendimiento acad\u00e9mico de los estudiantes.</li> <li>FactEncuestas: Re\u00fane informaci\u00f3n sobre las respuestas a encuestas realizadas, apoyando el an\u00e1lisis de satisfacci\u00f3n de los afiliados y empleadores. Estas respuestas proporcionan informaci\u00f3n cualitativa que ayuda a mejorar los servicios.</li> </ul>"},{"location":"#resultados-esperados","title":"Resultados Esperados","text":"<p>Este proyecto de optimizaci\u00f3n del Data Warehouse permitir\u00e1:</p> <ul> <li>Mejorar la calidad y consistencia de los datos: Los procesos de transformaci\u00f3n y validaci\u00f3n asegurar\u00e1n que los datos cargados en el DWH sean precisos y confiables. Se eliminar\u00e1n errores y duplicados, mejorando la integridad de la informaci\u00f3n.</li> <li>Optimizar la gesti\u00f3n de los datos: Al automatizar las tareas de ETL, se reducir\u00e1n significativamente los tiempos de procesamiento, aumentando la eficiencia operativa. Los procesos automatizados garantizan una mayor rapidez y menor margen de error en la gesti\u00f3n de la informaci\u00f3n.</li> <li>Facilitar la toma de decisiones: Un DWH bien dise\u00f1ado proporcionar\u00e1 informaci\u00f3n clara y relevante que apoyar\u00e1 la toma de decisiones estrat\u00e9gicas en diferentes \u00e1reas del negocio. Los an\u00e1lisis podr\u00e1n realizarse de manera m\u00e1s r\u00e1pida y precisa, mejorando la capacidad de respuesta ante cambios del entorno.</li> <li>Aumentar la confiabilidad de los an\u00e1lisis: Al centralizar y normalizar los datos, se garantiza que todas las \u00e1reas de la empresa trabajen con la misma informaci\u00f3n, eliminando discrepancias y asegurando un \u00fanico punto de verdad.</li> </ul>"},{"location":"#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":""},{"location":"#diagrama-de-secuencia-del-proceso-etl_1","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - General del Proyecto\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Fuente_DB as Base de Datos Fuente\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Fuente_DB: Conecta a la base de datos fuente\n    Fuente_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Fuente_DB: Ejecuta consulta SQL para extraer datos\n    Fuente_DB --&gt;&gt; ETL_Script: Retorna datos de la consulta\n    ETL_Script -&gt;&gt; ETL_Script: Limpia, transforma y valida los datos\n    ETL_Script -&gt;&gt; DWH: Conecta al Data Warehouse\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos transformados en las tablas correspondientes\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"#conclusion","title":"Conclusi\u00f3n","text":"<p>La implementaci\u00f3n de un Data Warehouse eficiente y bien optimizado es un pilar fundamental para cualquier organizaci\u00f3n que desee tomar decisiones basadas en datos fiables. Mediante la consolidaci\u00f3n y limpieza de informaci\u00f3n, este proyecto no solo busca mejorar la calidad de los datos, sino tambi\u00e9n optimizar los procesos de negocio mediante una estructura clara y automatizada de gesti\u00f3n de informaci\u00f3n.</p> <p>En resumen, el proyecto tiene como objetivo principal la automatizaci\u00f3n de la Bodega Temporal de Base de Datos Pivote, que forma parte del marco del Gobierno de Datos de CAJAMAG, para garantizar una fuente de datos confiable y consistente que facilite la toma de decisiones estrat\u00e9gicas y mejore el rendimiento de los procesos de negocio mediante una estructura organizada y un modelo relacional unificado.</p>"},{"location":"buenaspracticas/","title":"BUENAS PRACTICAS","text":""},{"location":"buenaspracticas/#introduccion","title":"Introducci\u00f3n","text":"<p>El proyecto de Data Warehouse para CAJAMAG se centra en optimizar y gestionar de manera avanzada el flujo de datos mediante procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga). Esta soluci\u00f3n tiene como objetivo centralizar y consolidar datos de m\u00faltiples fuentes, como bases de datos de beneficiarios, instituciones educativas, servicios sociales y datos transaccionales. De esta manera, se facilita la toma de decisiones estrat\u00e9gicas y operativas, garantizando la integridad, consistencia y disponibilidad de los datos.</p> <p>Para lograr un sistema robusto y escalable, se han implementado las mejores pr\u00e1cticas de programaci\u00f3n \"clean code\" y t\u00e9cnicas de optimizaci\u00f3n, asegurando que el c\u00f3digo cumpla con los requerimientos de procesamiento de datos, a la vez que sea claro, eficiente y mantenible. Cada bloque funcional del proyecto sigue una estructura modular que facilita su reutilizaci\u00f3n y actualizaci\u00f3n. Adem\u00e1s, se ha adoptado un enfoque orientado a la calidad en el manejo de logs e informes, proporcionando informaci\u00f3n en tiempo real sobre el estado del proceso ETL.</p> <p>En resumen, este proyecto provee una plataforma s\u00f3lida para el almacenamiento y consulta de datos, implementando buenas pr\u00e1cticas en el desarrollo de software que garantizan una infraestructura de datos confiable y de alta calidad que apoya el crecimiento y la eficiencia organizacional de CAJAMAG.</p>"},{"location":"buenaspracticas/#1-modularidad-y-estructura-de-codigo","title":"1. Modularidad y Estructura de C\u00f3digo","text":"<ul> <li> <p>Separaci\u00f3n de Responsabilidades: Cada notebook (\u201cDimDatosFijos\u201d, \u201cDimCalendarioMensual\u201d, etc.) est\u00e1 dise\u00f1ado para cumplir con un bloque funcional espec\u00edfico del proyecto. Las funciones en \u201cFunciones.py\u201d siguen el principio de Single Responsibility Principle (SRP), permitiendo que cada funci\u00f3n gestione un aspecto espec\u00edfico del flujo ETL, como la conexi\u00f3n a la base de datos o la validaci\u00f3n de datos.</p> </li> <li> <p>Uso de Decoradores y Context Managers: Se utilizaron decoradores y context managers para optimizar la ejecuci\u00f3n y centralizar acciones repetitivas, como la gesti\u00f3n de conexiones a bases de datos y el manejo de errores. Esto evita la repetici\u00f3n de c\u00f3digo y mantiene la claridad en la implementaci\u00f3n de funciones comunes.</p> </li> <li> <p>Uso de M\u00f3dulo Centralizado de Credenciales: Se centralizaron las credenciales de conexi\u00f3n en un m\u00f3dulo \u00fanico, consumible desde todos los scripts del ETL, lo cual facilita la gesti\u00f3n y actualizaci\u00f3n de las credenciales de manera segura y evita la duplicaci\u00f3n innecesaria. A continuaci\u00f3n, se muestra un ejemplo de c\u00f3mo se estructura este m\u00f3dulo:   <pre><code># Diccionario con las configuraciones de cada base de datos\nconfiguraciones_bd = {\n    'minerva': {\n        'usuario': '***',\n        'contrase\u00f1a': '**********',\n        'host': '***.***.***.*',\n        'base_de_datos': ''  # Sin base de datos especificada para Minerva\n    },\n    'dwh': {\n        'usuario': '***',\n        'contrase\u00f1a': '**********',\n        'host': '***.***.***.*',\n        'base_de_datos': 'dwh'\n    },\n    'neith': {\n        'usuario': '***',\n        'contrase\u00f1a': '**********',\n        'host': '***.***.***.*',\n        'base_de_datos': ''  # Sin base de datos especificada para Neith\n    }\n}\n</code></pre>   Este m\u00f3dulo permite centralizar la configuraci\u00f3n de conexi\u00f3n a diferentes bases de datos, lo que facilita el mantenimiento y mejora la seguridad al evitar tener credenciales distribuidas en m\u00faltiples scripts.</p> </li> </ul>"},{"location":"buenaspracticas/#2-buenas-practicas-en-nombres-y-documentacion","title":"2. Buenas Pr\u00e1cticas en Nombres y Documentaci\u00f3n","text":"<ul> <li> <p>Nombres Descriptivos: Las funciones y variables usan convenciones que mejoran la comprensi\u00f3n, como \u201cguardar_en_dwh\u201d o \u201cobtener_conexion\u201d. Esto mantiene la uniformidad en Python y facilita el mantenimiento y uso del c\u00f3digo.</p> </li> <li> <p>Docstrings en Funciones: Cada funci\u00f3n en \u201cFunciones.py\u201d incluye un docstring detallado, alineado con el formato de Google Docstring o reStructuredText. Esto permite que cualquier desarrollador entienda r\u00e1pidamente los objetivos de cada funci\u00f3n, los par\u00e1metros necesarios y el valor de retorno.</p> </li> </ul>"},{"location":"buenaspracticas/#3-optimizacion-del-codigo","title":"3. Optimizaci\u00f3n del C\u00f3digo","text":"<ul> <li> <p>Ejecuci\u00f3n Paralela: Utilizando \u201cconcurrent.futures\u201d, se aplicaron t\u00e9cnicas de paralelizaci\u00f3n en operaciones intensivas de lectura y carga de datos, optimizando as\u00ed la duraci\u00f3n del proceso ETL sin comprometer la calidad del procesamiento.</p> </li> <li> <p>Uso Eficiente de Pandas: Se utilizaron m\u00e9todos vectorizados en lugar de bucles para mejorar el rendimiento en la transformaci\u00f3n de datos. Esto garantiza un procesamiento eficiente y legible, siguiendo las mejores pr\u00e1cticas recomendadas.</p> </li> </ul>"},{"location":"buenaspracticas/#4-logging-estructurado-y-detallado","title":"4. Logging Estructurado y Detallado","text":"<ul> <li> <p>Niveles de Logging: Se configur\u00f3 \u201clogging\u201d para capturar eventos a diferentes niveles (\u201cINFO\u201d, \u201cDEBUG\u201d, \u201cERROR\u201d), permitiendo un seguimiento detallado de cada etapa del proceso ETL. Cada notebook registra mensajes en un archivo de log centralizado (\u201cetl_proceso.log\u201d).</p> </li> <li> <p>Salida de Informaci\u00f3n al Cliente: Los registros de log incluyen detalles sobre la duraci\u00f3n de cada proceso, la cantidad de registros procesados y el estado del procesamiento. Esta informaci\u00f3n proporciona una visibilidad clara del rendimiento y calidad del flujo de datos, estandarizando la forma en la que se muestra cada carga y descarga de informaci\u00f3n. Por ejemplo:</p> </li> <li>Conexi\u00f3n y Carga de Datos:     <pre><code>2024-10-27 09:32:35,975 - INFO - CONEXION A BASE MINERVA\n2024-10-27 09:32:36,521 - INFO - Cargando Tra \n2024-10-27 09:41:15,061 - INFO - Cargada Tra, 282,958 registros finales obtenidos. --- 8.64 minutes ---\n</code></pre></li> <li> <p>Descarga y Almacenamiento de Datos:     <pre><code>2024-10-27 09:53:25,218 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 09:53:25,222 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 09:53:26,237 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-27 09:56:32,425 - INFO - Tabla almacenada correctamente. 970,252 registros finales obtenidos.\n2024-10-27 09:56:33,514 - INFO - ALMACENAMIENTO ---  --- 3.14 minutes ---\n</code></pre>   Estos mensajes estandarizados permiten un seguimiento consistente y detallado del estado de cada carga y descarga, con una clara indicaci\u00f3n del rendimiento temporal y los registros afectados.</p> </li> <li> <p>Formato Personalizado de Logging: El formato de salida incluye <code>%(asctime)s - %(name)s - %(levelname)s - %(message)s</code>, garantizando que cada mensaje en el log sea claro respecto al origen, la fecha y la hora del evento.</p> </li> </ul>"},{"location":"buenaspracticas/#5-manejo-robusto-de-errores","title":"5. Manejo Robusto de Errores","text":"<ul> <li> <p>Excepciones Espec\u00edficas y Personalizadas: Se definieron excepciones espec\u00edficas como \u201cDBConnectionError\u201d para gestionar problemas de conexi\u00f3n a la base de datos, proporcionando informaci\u00f3n detallada en los logs y facilitando la soluci\u00f3n r\u00e1pida de fallas.</p> </li> <li> <p>Bloques \u201ctry-except\u201d Acotados: Cada bloque \u201ctry\u201d abarca solo las l\u00edneas necesarias, siguiendo las recomendaciones de Clean Code. Esto facilita la detecci\u00f3n de errores y asegura que solo las secciones susceptibles est\u00e1n protegidas.</p> </li> </ul>"},{"location":"buenaspracticas/#6-validacion-y-estandarizacion-de-datos","title":"6. Validaci\u00f3n y Estandarizaci\u00f3n de Datos","text":"<ul> <li> <p>Validaci\u00f3n de Registros Duplicados: Mediante \u201cStoreDuplicated\u201d, se detectan y almacenan registros duplicados para su revisi\u00f3n posterior, garantizando la consistencia en el Data Warehouse.</p> </li> <li> <p>Estandarizaci\u00f3n de Direcciones: Las direcciones se estandarizan con la funci\u00f3n \u201cestandarizar_direccion\u201d, aplicando etiquetas de validaci\u00f3n para asegurar la precisi\u00f3n en los datos geogr\u00e1ficos.</p> </li> </ul>"},{"location":"buenaspracticas/#conclusion","title":"Conclusi\u00f3n","text":"<p>Estas buenas pr\u00e1cticas de \"clean code\" y estructuraci\u00f3n aseguran un flujo ETL eficiente, documentado y alineado con las mejores pr\u00e1cticas de Python. La salida clara al cliente en los logs y la estructuraci\u00f3n modular de las funciones aseguran altos est\u00e1ndares de calidad y permiten la adaptaci\u00f3n del proyecto para futuras expansiones o modificaciones. La combinaci\u00f3n de modularidad, documentaci\u00f3n detallada y optimizaci\u00f3n de c\u00f3digo sienta las bases para un Data Warehouse escalable y sostenible que satisface las necesidades de CAJAMAG.</p>"},{"location":"instalacion/","title":"Instalaci\u00f3n y Configuraci\u00f3n del Entorno para el Procesamiento de Datos del Data Warehouse","text":""},{"location":"instalacion/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta los pasos necesarios para configurar un entorno que permita la instalaci\u00f3n y configuraci\u00f3n de todas las dependencias necesarias para ejecutar procesos de carga y transformaci\u00f3n de datos dentro de un Data Warehouse (DWH). Adem\u00e1s, se detalla el uso de las funciones contenidas en <code>Funciones.py</code> y c\u00f3mo se aplican en el flujo de trabajo descrito en los notebook.</p>"},{"location":"instalacion/#librerias-necesarias","title":"Librer\u00edas Necesarias","text":"<p>Para ejecutar correctamente el proyecto, es necesario instalar las siguientes librer\u00edas de Python:</p> <ul> <li><code>sqlalchemy version 1.4.39</code>: Para la conexi\u00f3n con bases de datos.</li> <li><code>pandas version 2.0.3</code>: Para la manipulaci\u00f3n y an\u00e1lisis de datos.</li> <li><code>beautifulsoup4 4.12.2</code>: Para la limpieza de datos en formato HTML.</li> <li><code>concurrent.futures</code>: Para la paralelizaci\u00f3n de tareas.</li> <li><code>numpy version 1.24.3</code>: Para el manejo de arreglos y c\u00e1lculos num\u00e9ricos.</li> <li><code>logging</code>: Para el registro de actividades y errores en el proceso ETL.</li> <li><code>os</code>: Para la gesti\u00f3n del sistema operativo y rutas de archivos.</li> <li><code>sys version 3.11.5</code>: Para modificar el <code>sys.path</code> y permitir la importaci\u00f3n de m\u00f3dulos personalizados.</li> <li><code>datetime</code>: Para el manejo de fechas y tiempos.</li> <li><code>dateutil.relativedelta</code>: Para operaciones avanzadas con fechas.</li> </ul>"},{"location":"instalacion/#configuracion-del-entorno","title":"Configuraci\u00f3n del Entorno","text":"<p>Para configurar el entorno, se recomienda usar un entorno virtual que permita mantener las dependencias organizadas y aisladas de otros proyectos. Los pasos para instalar las dependencias son los siguientes:</p>"},{"location":"instalacion/#creacion-de-entorno-virtual","title":"Creaci\u00f3n de Entorno Virtual","text":"<p>Primero, cree un entorno virtual:</p> <pre><code>python -m venv etl_env\n</code></pre> <p>Luego, active el entorno:</p> <ul> <li>En Windows:</li> </ul> <pre><code>etl_env\\Scripts\\activate\n</code></pre> <ul> <li>En Linux/macOS:</li> </ul> <pre><code>source etl_env/bin/activate\n</code></pre>"},{"location":"instalacion/#instalacion-de-dependencias","title":"Instalaci\u00f3n de Dependencias","text":"<p>Una vez que el entorno est\u00e9 activado, instale las dependencias:</p> <pre><code>pip install sqlalchemy pandas beautifulsoup4 numpy logging python-dateutil concurrent.futures\n</code></pre>"},{"location":"instalacion/#importacion-de-funciones-desde-funcionespy","title":"Importaci\u00f3n de Funciones desde <code>Funciones.py</code>","text":"<p>El proyecto utiliza funciones definidas externamente en el archivo <code>Funciones.py</code>. Este archivo incluye un conjunto de funciones personalizadas que facilitan la conexi\u00f3n a las bases de datos, la transformaci\u00f3n de datos y el almacenamiento en el Data Warehouse. Algunas de las funciones principales contenidas en <code>Funciones.py</code> son:</p>"},{"location":"instalacion/#1-guardar_en_dwhdf-tabla-logger-multiple-if_exists","title":"1. <code>guardar_en_dwh(df, tabla, logger, multiple, if_exists)</code>:","text":"<ul> <li>Prop\u00f3sito: Guarda un <code>DataFrame</code> en una tabla espec\u00edfica del Data Warehouse.</li> <li>Argumentos: Permite especificar la tabla, el logger para el registro del proceso y si los datos se deben agregar o reemplazar.</li> </ul>"},{"location":"instalacion/#2-obtener_conexionnombre_bd","title":"2. <code>obtener_conexion(nombre_bd)</code>:","text":"<ul> <li>Prop\u00f3sito: Proporciona la cadena de conexi\u00f3n para conectarse a una base de datos espec\u00edfica.</li> <li>Uso: Utilizado para establecer la conexi\u00f3n con las bases de datos.</li> </ul>"},{"location":"instalacion/#3-setup_loggerlog_filename-log_level","title":"3. <code>setup_logger(log_filename, log_level)</code>:","text":"<ul> <li>Prop\u00f3sito: Configura el logger para registrar eventos y errores durante la ejecuci\u00f3n del proceso ETL.</li> <li>Argumentos: El nombre del archivo de log y el nivel de detalle (INFO, DEBUG, etc.).</li> </ul>"},{"location":"instalacion/#4-limpiar_htmltexto_html","title":"4. <code>limpiar_html(texto_html)</code>:","text":"<ul> <li>Prop\u00f3sito: Utiliza <code>BeautifulSoup</code> para limpiar el texto HTML, extrayendo \u00fanicamente el contenido textual y eliminando etiquetas HTML.</li> </ul>"},{"location":"instalacion/#5-log_tiempologger-mensaje-tiempo","title":"5. <code>log_tiempo(logger, mensaje, tiempo)</code>:","text":"<ul> <li>Prop\u00f3sito: Registra el tiempo transcurrido para una operaci\u00f3n espec\u00edfica en el proceso ETL.</li> <li>Argumentos: Recibe el logger, un mensaje descriptivo y el tiempo transcurrido.</li> </ul>"},{"location":"instalacion/#6-cargar_tablasmotor-queries-df_structure-logger","title":"6. <code>cargar_tablas(motor, queries, df_structure, logger)</code>:","text":"<ul> <li>Prop\u00f3sito: Carga las tablas especificadas en un diccionario de consultas SQL y las almacena en una estructura de datos.</li> <li>Argumentos: Incluye el motor de conexi\u00f3n, las consultas SQL, un diccionario para almacenar los resultados y el logger para registrar el proceso.</li> </ul>"},{"location":"instalacion/#7-storeduplicatednombre_archivo-columnas-dataframe-ruta","title":"7. <code>StoreDuplicated(nombre_archivo, columnas, dataframe, ruta)</code>:","text":"<ul> <li>Prop\u00f3sito: Almacena registros duplicados en un archivo para su an\u00e1lisis posterior.</li> <li>Argumentos: Recibe el nombre del archivo, las columnas a comparar, el <code>DataFrame</code> y la ruta donde guardar los registros duplicados.</li> </ul>"},{"location":"instalacion/#8-testfunciones","title":"8. <code>testfunciones()</code>:","text":"<ul> <li>Prop\u00f3sito: Verifica que todas las funciones importadas desde <code>Funciones.py</code> est\u00e1n operativas.</li> <li>Uso: Se ejecuta al inicio del script para garantizar la correcta importaci\u00f3n de las funciones.</li> </ul>"},{"location":"instalacion/#9-estandarizar_direcciondireccion","title":"9. <code>estandarizar_direccion(direccion)</code>:","text":"<ul> <li>Prop\u00f3sito: Estandariza el formato de una direcci\u00f3n para asegurar la uniformidad de los datos.</li> <li>Argumentos: Recibe la direcci\u00f3n a estandarizar.</li> </ul>"},{"location":"instalacion/#10-marcar_direcciones_estandarizadasdf-columna","title":"10. <code>marcar_direcciones_estandarizadas(df, columna)</code>:","text":"<pre><code>- **Prop\u00f3sito**: Marca las direcciones estandarizadas dentro de un DataFrame.\n- **Argumentos**: Recibe el `DataFrame` y la columna de direcciones a analizar.\n</code></pre> <p>Estas funciones permiten simplificar y automatizar las tareas comunes de un proceso ETL, garantizando un flujo de trabajo eficiente y bien documentado.</p>"},{"location":"instalacion/#configuracion-del-entorno-de-trabajo","title":"Configuraci\u00f3n del Entorno de Trabajo","text":""},{"location":"instalacion/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n Inicial del Logger","text":"<p>La configuraci\u00f3n del logger es fundamental para garantizar el seguimiento de las operaciones que se realizan en el proceso ETL. Utilice la funci\u00f3n <code>setup_logger</code> para registrar los eventos.</p> <pre><code>import logging\nfrom Funciones import setup_logger\n\nlogger = setup_logger(log_filename='etl_proceso.log', log_level=logging.INFO)\nlogger.info('Inicio del proceso ETL')\n</code></pre>"},{"location":"instalacion/#ejecucion-de-funciones-de-conexion-y-procesamiento","title":"Ejecuci\u00f3n de Funciones de Conexi\u00f3n y Procesamiento","text":"<p>A continuaci\u00f3n, se muestra un ejemplo de c\u00f3mo utilizar las funciones para establecer una conexi\u00f3n con el DWH y procesar los datos:</p> <pre><code>from Funciones import obtener_conexion, guardar_en_dwh\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# Establecer conexi\u00f3n al DWH\ncadena_conexion = obtener_conexion('dwh')\nmotor = create_engine(cadena_conexion)\n\n# Procesar y guardar los datos\ndf = pd.read_sql_query(\"SELECT * FROM fuente_datos\", motor)\n\n# Guardar en DWH\nguardar_en_dwh(df, 'tabla_destino', logger, multiple=False, if_exists='append')\n</code></pre>"},{"location":"instalacion/#conclusion","title":"Conclusi\u00f3n","text":"<p>La correcta configuraci\u00f3n del entorno y la instalaci\u00f3n de dependencias son fundamentales para asegurar que los procesos de carga y transformaci\u00f3n de datos se realicen de manera eficiente y sin contratiempos. Al tener un entorno adecuadamente configurado, se minimizan los errores y se facilita la depuraci\u00f3n y mantenimiento del c\u00f3digo. Adem\u00e1s, el uso de funciones centralizadas en <code>Funciones.py</code> permite simplificar la l\u00f3gica de conexi\u00f3n, limpieza y carga de datos, asegurando la reproducibilidad de los procesos y manteniendo un c\u00f3digo m\u00e1s ordenado, estructurado y f\u00e1cil de entender. Esto se traduce en una mayor eficiencia, un menor costo de mantenimiento y una mejor calidad general de los datos procesados, facilitando as\u00ed la toma de decisiones basada en datos confiables y bien organizados.</p>"},{"location":"codigo/funciones/","title":"Documentaci\u00f3n de Funciones para Procesos ETL en Data Warehouse","text":"<p>Este documento proporciona una gu\u00eda detallada de las funciones contenidas en el archivo <code>Funciones.py</code>, utilizadas para procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) en un sistema de Data Warehouse. A continuaci\u00f3n, se describe cada funci\u00f3n, las librer\u00edas utilizadas, un diagrama de los procesos principales, y un ejemplo del c\u00f3digo fuente. Estas funciones est\u00e1n organizadas en categor\u00edas clave seg\u00fan su prop\u00f3sito en el flujo de trabajo.</p>"},{"location":"codigo/funciones/#librerias-utilizadas","title":"Librer\u00edas Utilizadas","text":"<ul> <li>pandas: Manipulaci\u00f3n y transformaci\u00f3n de datos.</li> <li>os: Interacci\u00f3n con el sistema operativo.</li> <li>sqlalchemy: Conexi\u00f3n y manejo de bases de datos SQL.</li> <li>time y datetime: Manejo y registro de tiempos de ejecuci\u00f3n.</li> <li>logging: Registro de eventos para monitoreo y depuraci\u00f3n de procesos ETL.</li> </ul>"},{"location":"codigo/funciones/#categorias-de-funciones-y-descripcion-general","title":"Categor\u00edas de Funciones y Descripci\u00f3n General","text":""},{"location":"codigo/funciones/#1-conexiones-a-bases-de-datos","title":"1. Conexiones a Bases de Datos","text":"<p>Estas funciones generan motores y cadenas de conexi\u00f3n seg\u00fan los detalles de configuraci\u00f3n, permitiendo conectarse a distintas bases de datos.</p> <ul> <li>Funciones Incluidas: <code>Conexion_dwh</code>, <code>Conexion_Minerva</code>, <code>Conexion_neith</code>, <code>obtener_conexion</code></li> <li>Ejemplo de C\u00f3digo:     <pre><code>def obtener_conexion(nombre_bd):\n    config = configuraciones_bd.get(nombre_bd)\n    cadena_conexion = f\"mysql+pymysql://{config['usuario']}:{config['contrase\u00f1a']}@{config['host']}/{config['base_de_datos']}\"\n    return cadena_conexion\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#2-estandarizacion-de-direcciones","title":"2. Estandarizaci\u00f3n de Direcciones","text":"<p>Estas funciones estandarizan y marcan direcciones utilizando abreviaciones comunes, simplificando las comparaciones y el an\u00e1lisis de direcciones.</p> <ul> <li>Funciones Incluidas: <code>estandarizar_direccion</code>, <code>marcar_direcciones_estandarizadas</code></li> <li>Descripci\u00f3n:</li> <li><code>estandarizar_direccion(direccion)</code>: Normaliza la direcci\u00f3n eliminando caracteres especiales y aplicando abreviaciones.</li> <li><code>marcar_direcciones_estandarizadas(df, columna_direccion_limpia)</code>: Etiqueta las direcciones estandarizadas, facilitando la validaci\u00f3n de datos.</li> <li>Ejemplo de C\u00f3digo:     <pre><code>def estandarizar_direccion(direccion):\n    ABREVIACIONES = {\"CALLE\": \"CL\", \"CARRERA\": \"CR\", \"TRANSVERSAL\": \"TV\", \"AVENIDA\": \"AV\"}\n    for palabra, abrev in ABREVIACIONES.items():\n        direccion = direccion.replace(palabra, abrev)\n    return direccion.strip()\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#3-manejo-de-duplicados-y-validaciones","title":"3. Manejo de Duplicados y Validaciones","text":"<p>Estas funciones identifican y eliminan duplicados en los DataFrames y generan archivos de trazabilidad, \u00fatiles para auditor\u00eda y calidad de datos.</p> <ul> <li>Funciones Incluidas: <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, <code>RemoveErrors</code></li> <li>Descripci\u00f3n:</li> <li><code>StoreDuplicated</code>: Identifica y almacena registros duplicados en un archivo CSV.</li> <li><code>RemoveDuplicated</code>: Ordena y elimina duplicados bas\u00e1ndose en una columna espec\u00edfica.</li> <li><code>RemoveErrors</code>: Elimina registros con inconsistencias entre fechas de afiliaci\u00f3n y retiro.</li> <li>Ejemplo de C\u00f3digo:     <pre><code>def StoreDuplicated(columId, ColumnsToCompare, table, NombreArchivo):\n    subsetDuplicate = table.columns.tolist() if columId == 'N/A' else ColumnsToCompare\n    duplicated_rows = table[table.duplicated(subset=subsetDuplicate, keep=False)]\n    duplicated_rows.to_csv(NombreArchivo + '.csv', index=False)\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#4-limpieza-y-normalizacion-de-textos","title":"4. Limpieza y Normalizaci\u00f3n de Textos","text":"<p>Estas funciones normalizan cadenas de texto para asegurar la consistencia en los nombres de columnas y valores textuales en general.</p> <ul> <li>Funciones Incluidas: <code>normalize</code>, <code>RemovePreposition</code>, <code>CorrectionColumnName</code>, <code>convertir_columnas_mayusculas</code></li> <li>Descripci\u00f3n:</li> <li><code>normalize</code>: Elimina acentos en cadenas de texto.</li> <li><code>RemovePreposition</code>: Elimina preposiciones en los nombres de columnas.</li> <li><code>CorrectionColumnName</code>: Genera nombres de columnas basados en su descripci\u00f3n.</li> <li><code>convertir_columnas_mayusculas</code>: Convierte los nombres de columnas a may\u00fasculas.</li> <li>Ejemplo de C\u00f3digo:     <pre><code>def normalize(s):\n    replacements = ((\"\u00e1\", \"a\"), (\"\u00e9\", \"e\"), (\"\u00ed\", \"i\"))\n    for a, b in replacements:\n        s = s.replace(a, b)\n    return s.upper()\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#5-carga-de-tablas-y-almacenamiento-en-el-data-warehouse","title":"5. Carga de Tablas y Almacenamiento en el Data Warehouse","text":"<p>Estas funciones administran el almacenamiento y la carga de datos en el Data Warehouse, incluyendo la opci\u00f3n de registrar el tiempo de carga.</p> <ul> <li>Funciones Incluidas: <code>cargar_tablas</code>, <code>guardar_en_dwh</code>, <code>log_tiempo</code></li> <li>Descripci\u00f3n:</li> <li><code>guardar_en_dwh</code>: Almacena un DataFrame en el Data Warehouse.</li> <li><code>cargar_tablas</code>: Ejecuta una consulta SQL y carga los resultados en un DataFrame.</li> <li><code>log_tiempo</code>: Registra el tiempo de ejecuci\u00f3n de los procesos.</li> <li>Ejemplo de C\u00f3digo:     <pre><code>def guardar_en_dwh(df, tabla, conexion):\n    df.to_sql(tabla, con=conexion, if_exists='replace', index=False)\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#6-configuracion-de-logger-y-herramientas-adicionales","title":"6. Configuraci\u00f3n de Logger y Herramientas Adicionales","text":"<p>Configura el logger para registrar mensajes y permite verificar la disponibilidad de las funciones con un mensaje de prueba.</p> <ul> <li>Funciones Incluidas: <code>setup_logger</code>, <code>testfunciones</code></li> <li>Descripci\u00f3n:</li> <li><code>setup_logger</code>: Configura el logger para registrar en archivo y consola.</li> <li><code>testfunciones</code>: Prueba la importaci\u00f3n de funciones y muestra la fecha y hora actual.</li> <li>Ejemplo de C\u00f3digo:     <pre><code>def setup_logger(log_filename='app.log'):\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    file_handler = logging.FileHandler(log_filename)\n    logger.addHandler(file_handler)\n    return logger\n</code></pre></li> </ul>"},{"location":"codigo/funciones/#conclusion","title":"Conclusi\u00f3n","text":"<p>El archivo <code>Funciones.py</code> contiene herramientas esenciales para procesos ETL, organizadas en categor\u00edas de conectividad, transformaci\u00f3n, almacenamiento y trazabilidad de datos. Cada funci\u00f3n desempe\u00f1a un rol espec\u00edfico, asegurando calidad, consistencia y eficiencia en el procesamiento de datos para su posterior an\u00e1lisis en un sistema de Data Warehouse.</p>"},{"location":"seccion/1.1-DimDatosFijos/","title":"1.1 Dim Datos Fijos","text":""},{"location":"seccion/1.1-DimDatosFijos/#intoduccion","title":"Intoducci\u00f3n","text":"<p>Este proceso ETL (Extract, Transform, Load) tiene como objetivo extraer y transformar datos de m\u00faltiples tablas de entrada, provenientes de las bases <code>xml4</code> y <code>subsidio</code>, y consolidarlas en una tabla de salida, <code>BD_Dim_Datos_Fijos</code>, en el Data Warehouse (DWH). El flujo de trabajo se desarrolla en Python y SQL, configurando y gestionando conexiones a bases de datos. A lo largo del ETL, se extraen datos mediante consultas SQL definidas para cada tabla de entrada (<code>xml4c086</code>, <code>xml4c085</code>, <code>subsi20</code>, <code>xml4c087</code>), que se transforman para normalizar identificadores y clasificar los tipos de documentos. Posteriormente, los datos se depuran, consolidando los registros y manteniendo la versi\u00f3n m\u00e1s actual de cada registro \u00fanico. Una vez finalizadas las transformaciones, el resultado final se almacena en la tabla <code>BD_Dim_Datos_Fijos</code> en el DWH.</p>"},{"location":"seccion/1.1-DimDatosFijos/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<p>A continuaci\u00f3n, se presenta un diagrama de secuencia que detalla el flujo de datos en cada etapa del proceso ETL, mostrando las tablas de entrada y salida involucradas:</p> <pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL de DimDatosFijos\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant Script as Script ETL\n  participant DB as Bases de Datos\n  participant DWH as Data Warehouse\n\n  Usuario-&gt;&gt;Script: Solicitar procesamiento de datos\n  Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c086, xml4c085, subsi20, xml4c087\n  DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n  Script-&gt;&gt;Script: Transformar datos (limpieza, estandarizaci\u00f3n)\n  Script-&gt;&gt;Script: Identificar y corregir registros duplicados\n  Script-&gt;&gt;DWH: Cargar datos transformados en &lt;br&gt; BD_Dim_Datos_Fijos\n  DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n  Script--&gt;&gt;Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama proporciona una visi\u00f3n clara del flujo de datos desde las tablas de entrada hasta la carga final en el DWH, destacando cada paso y su contribuci\u00f3n al objetivo final del proceso ETL.</p>"},{"location":"seccion/1.1-DimDatosFijos/#etl","title":"ETL","text":""},{"location":"seccion/1.1-DimDatosFijos/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para gestionar conexiones a bases de datos y procesamiento de datos. Se importan <code>sqlalchemy</code> para manejar conexiones, <code>pandas</code> y <code>numpy</code> para manipulaci\u00f3n de datos, y m\u00f3dulos como <code>time</code>, <code>os</code>, y <code>logging</code> para control de tiempo, manejo de rutas y registro de eventos. Adem\u00e1s, se agrega el directorio <code>funciones</code> a <code>sys.path</code> para permitir la importaci\u00f3n de funciones personalizadas desde <code>Funciones.py</code>, como <code>guardar_en_dwh</code> y <code>cargar_tablas</code> para el manejo de datos en el Data Warehouse, <code>obtener_conexion</code> para conectar a las bases, y <code>setup_logger</code> para configurar el sistema de registro de eventos y errores. Esto permite un flujo robusto para la carga, manipulaci\u00f3n y almacenamiento de datos en proyectos de an\u00e1lisis y bases de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Estandar_Afiliados.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Estandar_Afiliados.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-27 09:32:35,885 - INFO - Importacion de funciones correcta, 27-10-2024 09:32\n2024-10-27 09:32:35,888 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#definicion-de-consultas-sql-para-extraccion-de-datos","title":"Definici\u00f3n de Consultas SQL para Extracci\u00f3n de Datos","text":"<p>El c\u00f3digo define un conjunto de consultas SQL organizadas en el diccionario <code>qr_structure</code>. Estas consultas extraen y estructuran informaci\u00f3n de diversas fuentes de datos, tales como tablas en las bases <code>xml4</code> y <code>subsidio</code>. Las consultas para cada clave (<code>\"Tra\"</code>, <code>\"Emp\"</code>, <code>\"Con\"</code>, <code>\"Ben\"</code>) generan identificadores \u00fanicos y normalizan los tipos de documento mediante operaciones de concatenaci\u00f3n y <code>CASE</code> para clasificar los c\u00f3digos de documento (<code>coddoc</code>). Adem\u00e1s, se realiza uniones (<code>LEFT JOIN</code>) con otras tablas para obtener informaci\u00f3n adicional, como el estado (<code>FECHA_ESTADO</code>) y el sexo (<code>SEXO</code>), consolidando toda la informaci\u00f3n de cada persona o entidad en un solo registro. Este enfoque permite extraer datos enriquecidos y categorizados para facilitar su an\u00e1lisis o procesamiento posterior.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"Tra\": '''\n        SELECT * FROM\n        (SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.cedtra) as ID,\n            CONCAT(\n            IFNULL(b.prinom, ''), ' ', \n            IFNULL(b.segnom, ''), ' ', \n            IFNULL(b.priape, ''), ' ', \n            IFNULL(b.segape, '')\n            ) AS RAZON_SOCIAL,\n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO,\n            c3.fecest  AS FECHA_ESTADO,\n            1 AS xml4c086, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR) AS CODDOC, \n            b.cedtra AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO,\n            \"xml4.xml4c086\" AS FUENTE\n        FROM \n            xml4.xml4c086 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT CONCAT(\n                    CAST(\n                    CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CD' THEN 8\n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'PE' THEN 9            \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'PT' THEN 15 \n                    WHEN coddoc = 'TI' THEN 2\n                    ELSE coddoc \n                    END AS CHAR\n                    ),\n                    cedtra\n                    ) AS cod, fecest\n            FROM subsidio.subsi15 GROUP BY cod) AS c3\n            ON CONCAT( b.coddoc , b.cedtra ) = c3.cod) AS bf\n            GROUP BY ID\n\n    ''',\n    \"Emp\": '''\n        SELECT * FROM (\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ),\n            b.nit\n            ) AS ID, \n            b.razsoc AS RAZON_SOCIAL, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c085, \n            CAST(\n            CASE \n            WHEN b.tipide = 1 THEN 'CC'\n            WHEN b.tipide = 3 THEN 'RC'\n            WHEN b.tipide = 4 THEN 'CE'\n            WHEN b.tipide = 6 THEN 'PA'\n            WHEN b.tipide = 7 THEN 'NI'            \n            ELSE b.tipide\n            END AS CHAR\n            ) AS CODDOC, \n            b.nit AS DOCUMENTO, \n            c2.prinom AS PRIMER_NOMBRE, \n            c2.segnom AS SEGUNDO_NOMBRE, \n            c2.priape AS PRIMER_APELLIDO, \n            c2.segape AS SEGUNDO_APELLIDO,\n            \"xml4.xml4c085\" AS FUENTE\n        FROM \n            xml4.xml4c085 AS b\n        LEFT JOIN \n            (SELECT CONCAT(\n                CAST(\n                CASE \n                WHEN coddoc = 'CC' THEN 1 \n                WHEN coddoc = 'CE' THEN 4 \n                WHEN coddoc = 'NI' THEN 7 \n                WHEN coddoc = 'PA' THEN 6 \n                WHEN coddoc = 'RC' THEN 3 \n                ELSE coddoc \n                END AS CHAR\n                ),\n                nit\n                ) AS cod, fecest, priape, segape, prinom, segnom\n        FROM subsidio.subsi02 GROUP BY cod) AS c2\n        ON CONCAT(b.tipide, b.nit) = c2.cod) AS bf\n        GROUP BY ID\n\n    ''',\n    \"Con\": '''\n        SELECT \n            CONCAT(coddoc, cedcon) AS ID, \n            #CONCAT(prinom, ' ', segnom, ' ', priape, ' ', segape) AS NOMBRE, \n            CONCAT(\n            IFNULL(prinom, ''), ' ', \n            IFNULL(segnom, ''), ' ', \n            IFNULL(priape, ''), ' ', \n            IFNULL(segape, '')\n            ) AS RAZON_SOCIAL,\n            fecnac AS FECHA_NACIMIENTO, \n            sexo AS SEXO, \n            fecest  AS FECHA_ESTADO, \n            1 AS SUBSI20, \n            coddoc AS CODDOC, \n            cedcon AS DOCUMENTO, \n            prinom AS PRIMER_NOMBRE, \n            segnom AS SEGUNDO_NOMBRE, \n            priape AS PRIMER_APELLIDO, \n            segape AS SEGUNDO_APELLIDO,\n            \"subsidio.subsi20\" AS FUENTE\n        FROM \n            subsidio.subsi20\n    ''', \n    \"Ben\": '''\n        SELECT * FROM (\n        SELECT \n            CONCAT(\n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END as CHAR), \n            b.documento) as ID,\n            CONCAT(\n            IFNULL(b.prinom, ''), ' ', \n            IFNULL(b.segnom, ''), ' ', \n            IFNULL(b.priape, ''), ' ', \n            IFNULL(b.segape, '')\n            ) AS RAZON_SOCIAL,\n            b.fecnac AS FECHA_NACIMIENTO, \n            c1.codsex AS SEXO, \n            c2.fecest  AS FECHA_ESTADO, \n            1 AS xml4c087, \n            CAST(\n            CASE \n            WHEN b.coddoc = 1 THEN 'CC' \n            WHEN b.coddoc = 2 THEN 'TI' \n            WHEN b.coddoc = 3 THEN 'RC' \n            WHEN b.coddoc = 4 THEN 'CE' \n            WHEN b.coddoc = 5 THEN 'NP' \n            WHEN b.coddoc = 6 THEN 'PA' \n            WHEN b.coddoc = 7 THEN 'NI' \n            WHEN b.coddoc = 8 THEN 'CD' \n            WHEN b.coddoc = 9 THEN 'PE' \n            WHEN b.coddoc = 15 THEN 'PT' \n            ELSE b.coddoc END AS CHAR) AS CODDOC, \n            b.documento AS DOCUMENTO, \n            b.prinom AS PRIMER_NOMBRE, \n            b.segnom AS SEGUNDO_NOMBRE, \n            b.priape AS PRIMER_APELLIDO, \n            b.segape AS SEGUNDO_APELLIDO,\n            \"xml4.xml4c087\" AS FUENTE\n        FROM \n            xml4.xml4c087 AS b\n        LEFT JOIN xml4.xml4b005 AS c1\n            ON b.tipgen = c1.tipgen\n        LEFT JOIN \n            (SELECT documento, fecest\n        FROM \n            subsidio.subsi22 GROUP BY documento) AS c2\n            ON b.documento=c2.documento) AS bf\n        GROUP BY ID\n    '''\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-27 09:32:35,903 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 09:32:35,975 - INFO - CONEXION A BASE MINERVA\n2024-10-27 09:32:36,521 - INFO - Cargando Tra \n2024-10-27 09:41:15,061 - INFO - Cargada Tra, 282,958 registros finales obtenidos. --- 8.64 minutes ---\n2024-10-27 09:41:15,063 - INFO - Cargando Emp \n2024-10-27 09:41:35,947 - INFO - Cargada Emp, 20,638 registros finales obtenidos. --- 20.88 seconds ---\n2024-10-27 09:41:35,948 - INFO - Cargando Con \n2024-10-27 09:41:54,188 - INFO - Cargada Con, 198,303 registros finales obtenidos. --- 18.24 seconds ---\n2024-10-27 09:41:54,190 - INFO - Cargando Ben \n2024-10-27 09:51:01,896 - INFO - Cargada Ben, 665,010 registros finales obtenidos. --- 9.13 minutes ---\n2024-10-27 09:51:02,189 - INFO - CARGUE TABLAS DESDE MYSQL --- Ben --- 18.44 minutes ---\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#concatenacion-y-transformacion-de-datos","title":"Concatenaci\u00f3n y Transformaci\u00f3n de Datos","text":"<p>Este bloque concatena los DataFrames almacenados en <code>df_structure</code> en un \u00fanico DataFrame <code>dfTotal</code>, acumulando todos los registros de diferentes consultas. Luego, se crea <code>dfOrigen</code>, un DataFrame que consolida la informaci\u00f3n de origen de cada ID usando el valor m\u00e1ximo por grupo, lo cual ayuda a identificar los datos provenientes de las distintas fuentes (<code>xml4c086</code>, <code>xml4c085</code>, <code>SUBSI20</code>, <code>xml4c087</code>). Finalmente, se corrigen los errores en las columnas de fecha <code>FECHA_ESTADO</code> y <code>FECHA_NACIMIENTO</code>, convirtiendo los valores incorrectos a <code>NaT</code> y estandarizando el formato a <code>datetime</code>. El tiempo de procesamiento para esta transformaci\u00f3n se registra en el <code>logger</code>, permitiendo monitorear la eficiencia del proceso.</p> <pre><code>#Concatenaci\u00f3n\ntratamiento1_time = time.time()\ndfTotal = pd.concat( list(df_structure.values()) , ignore_index = True )\n\n#Informacion De Origenes\ndfOrigen = dfTotal[['ID','xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087']].groupby('ID')[ ['xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087'] ].max().reset_index()\n\n#Cambiar errores en columnas fechas a NaT y convertirlas a formato datetime\ndfTotal['FECHA_ESTADO'] = pd.to_datetime(dfTotal['FECHA_ESTADO'],errors='coerce')\ndfTotal['FECHA_NACIMIENTO'] = pd.to_datetime(dfTotal['FECHA_NACIMIENTO'],errors='coerce')\nlogger.info(f'TRANSFORMACION PARTE 1 --- {time.time() - tratamiento1_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 09:51:04,389 - INFO - TRANSFORMACION PARTE 1 --- 2.19 seconds ---\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#definicion-y-clasificacion-de-columnas-por-tipo","title":"Definici\u00f3n y Clasificaci\u00f3n de Columnas por Tipo","text":"<p>En este bloque, las columnas de <code>dfTotal</code> se clasifican en tres categor\u00edas: columnas num\u00e9ricas (<code>numericColumns</code>), columnas de tipo fecha (<code>datesColumns</code>), y columnas de texto (<code>textColumns</code>). Las columnas num\u00e9ricas se identifican a partir de sus tipos de datos (<code>int</code> y <code>float</code>), y las de fecha mediante el tipo <code>datetime64[ns]</code>. Tambi\u00e9n se define <code>ColumnsToCompare</code>, que excluye columnas de ID y metadatos de origen, con el prop\u00f3sito de centrar el an\u00e1lisis en atributos comparables de los datos. Por \u00faltimo, <code>finalColumns</code> se define como el conjunto de todas las columnas de <code>dfTotal</code>, excluyendo las relacionadas con la fuente de origen, preparando as\u00ed el DataFrame para posteriores an\u00e1lisis y comparaciones.</p> <pre><code>#Definir columnas con sus tipos\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndates = ['datetime64[ns]']\nnumericColumns = dfTotal.select_dtypes(include=numerics).columns.tolist()\ndatesColumns = dfTotal.select_dtypes(include=dates).columns.tolist()\ntextColumns = [x for x in dfTotal.columns.tolist() if x not in (numericColumns + datesColumns ) ]\nColumnsToCompare = [x for x in dfTotal.columns.tolist() if x not in [ 'ID' , 'FECHA_ESTADO' ,'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\nfinalColumns = [x for x in dfTotal.columns.tolist() if x not in [ 'xml4c086', 'xml4c085', 'SUBSI20', 'xml4c087' ] ]\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#segunda-fase-de-transformacion-y-limpieza-de-datos","title":"Segunda fase de Transformaci\u00f3n y Limpieza de Datos","text":"<p>Este bloque realiza una serie de transformaciones y validaciones en <code>dfTotal</code>. Primero, se reemplazan valores <code>None</code> por <code>NaN</code> en las columnas de texto y se estandarizan a may\u00fasculas eliminando los espacios en los extremos. Los valores <code>\"NAN\"</code> se convierten tambi\u00e9n a <code>NaN</code> para evitar inconsistencias. Luego, se construye el <code>Validador</code>, que verifica la unicidad de cada campo por <code>ID</code> y ayuda a identificar registros problem\u00e1ticos en <code>IDProblemas</code>, donde existen duplicados o discrepancias. Estos registros se consolidan en <code>dfRepetidos</code>, que se exporta a un archivo CSV para su an\u00e1lisis.</p> <p>Finalmente, en el proceso de limpieza, se eliminan los duplicados manteniendo solo el registro con la fecha <code>FECHA_ESTADO</code> m\u00e1s reciente para cada <code>ID</code>, asegurando as\u00ed que <code>dfTotal</code> conserve \u00fanicamente los datos m\u00e1s actuales de cada entidad. El tiempo de procesamiento de cada paso se registra en el <code>logger</code> para monitorear el rendimiento.</p> <pre><code>tratamiento2_time = time.time()\n#Cambian None por NaN\ndfTotal[textColumns] = dfTotal[textColumns].where(pd.notna(dfTotal), np.nan)\n\n#Pasar las columnas tipo texto a UPPER\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.upper())\n\n#En las columnas tipo texto, eliminar espacios al comienzo y al final\ndfTotal[textColumns] = dfTotal[textColumns].apply( lambda x: x.astype(str).str.strip())\n\n#Reemplazar los \"NAN\" por NaN\ndfTotal[textColumns] = dfTotal[textColumns].replace('NAN', np.nan)\n\n#ValIDador de campos repetIDos\nValIDador = dfTotal.groupby('ID')[ ColumnsToCompare ].nunique(dropna=False).reset_index()\nValIDador['suma']  = ValIDador.sum(axis=1, numeric_only=True)\n\n#Tablas de informaci\u00f3n para guia transformaci\u00f3n\nIDProblemas = ValIDador[ValIDador['suma'] &gt; len(ColumnsToCompare) ]['ID'].tolist()\ndfRepetIDos = dfTotal[ dfTotal['ID'].isin(IDProblemas)  ].groupby('ID')[ ColumnsToCompare ].agg(['unique']).reset_index()\ndfRepetIDos.columns = dfRepetIDos.columns.droplevel(1)\ntrazaDf = pd.merge(dfRepetIDos,dfOrigen,on = 'ID' )\ntrazaDf.to_csv(os.getcwd() + r'traza_Estandar_Afiliados.csv')\nlogger.info(f'TRANSFORMACION PARTE 2 --- {time.time() - tratamiento2_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 09:53:23,509 - INFO - TRANSFORMACION PARTE 2 --- 139.04 seconds ---\n</code></pre> <pre><code>#Eliminar duplicados dejando el de la fecha m\u00e1s reciente\nlimpieza_time = time.time()\ndfTotal = dfTotal[finalColumns]\ndfTotal = dfTotal.sort_values('FECHA_ESTADO').drop_duplicates('ID',keep='last')\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 09:53:24,794 - INFO - LIMPIEZA --- 1.27 seconds ---\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#refinamiento-final-de-datos","title":"Refinamiento Final de Datos","text":"<p>En este \u00faltimo paso, se eliminan las columnas <code>NOMBRE</code> y <code>FECHA_ESTADO</code> de <code>dfTotal</code>, y se renombra la columna <code>ID</code> a <code>ID_AFILIADO</code> para mejorar la claridad y la consistencia en la nomenclatura. Finalmente, se registra en el <code>logger</code> la cantidad total de registros en <code>dfTotal</code>, lo que proporciona una verificaci\u00f3n r\u00e1pida de los datos finales antes de su almacenamiento o an\u00e1lisis adicional.</p> <pre><code>dfTotal = dfTotal.drop( [ 'NOMBRE', 'FECHA_ESTADO'] , axis = 1 )\ndfTotal.rename(columns={'ID':'ID_AFILIADO'}, inplace=True)\n</code></pre> <pre><code>logger.info(f'Informacion final {dfTotal[\"ID_AFILIADO\"].count()} registros')\n</code></pre> <pre><code>2024-10-27 09:53:25,195 - INFO - Informacion final 970252 registros\n</code></pre>"},{"location":"seccion/1.1-DimDatosFijos/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Dim_Datos_Fijos</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfTotal, 'BD_Dim_Datos_Fijos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-27 09:53:25,218 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 09:53:25,222 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 09:53:26,237 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-27 09:56:32,425 - INFO - Tabla almacenada correctamente. 970,252 registros finales obtenidos.\n2024-10-27 09:56:33,514 - INFO - ALMACENAMIENTO ---  --- 3.14 minutes ---\n2024-10-27 09:56:33,516 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>dfTotal.columns.tolist()\n</code></pre> <pre><code>['ID_AFILIADO',\n 'RAZON_SOCIAL',\n 'FECHA_NACIMIENTO',\n 'SEXO',\n 'CODDOC',\n 'DOCUMENTO',\n 'PRIMER_NOMBRE',\n 'SEGUNDO_NOMBRE',\n 'PRIMER_APELLIDO',\n 'SEGUNDO_APELLIDO',\n 'FUENTE']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 09:56:33,543 - INFO - FINAL ETL --- 1437.80 seconds ---\n</code></pre>"},{"location":"seccion/1.2-DimCalendarioMensual/","title":"1.2 Dim Calendario Mensual","text":""},{"location":"seccion/1.2-DimCalendarioMensual/#intoduccion","title":"Intoducci\u00f3n","text":"<p>Este proceso ETL tiene como finalidad generar y almacenar una tabla de calendario mensual en un Data Warehouse (DWH), facilitando la disponibilidad de un calendario estructurado y estandarizado para an\u00e1lisis de datos temporales. El flujo de trabajo crea un rango de fechas mensual desde hace 18 meses hasta el presente, extrayendo y estructurando campos de a\u00f1o, mes y primer d\u00eda del mes. La informaci\u00f3n final se guarda en la tabla <code>BD_Dim_Calendario</code> del DWH para su uso en an\u00e1lisis de periodos mensuales.</p>"},{"location":"seccion/1.2-DimCalendarioMensual/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<p>A continuaci\u00f3n, se muestra el diagrama de secuencia que detalla el flujo del proceso ETL, desde la solicitud inicial hasta la carga en la tabla de salida en el DWH:</p> <pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL de DimCalendarioMensual\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant Script as Script ETL\n  participant DWH as Data Warehouse\n\n  Usuario-&gt;&gt;Script: Solicitar creaci\u00f3n de calendario mensual\n  Script-&gt;&gt;Script: Crear tabla de fechas &lt;br&gt; (rango de \u00faltimos 18 meses)\n  Script-&gt;&gt;Script: Generar campos de &lt;br&gt; A\u00f1o, Mes, Primer D\u00eda del Mes\n  Script-&gt;&gt;DWH: Cargar tabla generada en &lt;br&gt; BD_Dim_Calendario\n  DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n  Script--&gt;&gt;Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/1.2-DimCalendarioMensual/#etl","title":"ETL","text":""},{"location":"seccion/1.2-DimCalendarioMensual/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno de trabajo importando varias bibliotecas esenciales para la manipulaci\u00f3n de datos y la conexi\u00f3n a bases de datos, como SQLAlchemy, Pandas y <code>dateutil.relativedelta</code> para c\u00e1lculos de fechas. Tambi\u00e9n gestiona rutas mediante <code>sys</code> y <code>os</code> para importar funciones personalizadas desde un archivo externo (<code>Funciones.py</code>). Una vez importadas, se ejecuta <code>testfunciones()</code>, presumiblemente una funci\u00f3n de prueba que verifica el correcto funcionamiento o disponibilidad de las dem\u00e1s funciones importadas, confirmando que est\u00e1n listas para ser utilizadas en el flujo de trabajo.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport time\nimport os\nfrom datetime import date\nimport logging\nfrom dateutil.relativedelta import relativedelta\nstart_time = time.time()\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 22-10-2024 21:23\n</code></pre>"},{"location":"seccion/1.2-DimCalendarioMensual/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Calendario_Mensual.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Calendario_Mensual.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-22 21:23:51,315 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.2-DimCalendarioMensual/#creacion-de-la-tabla-calendario","title":"Creaci\u00f3n de la tabla calendario","text":"<p>Se genera un dataframe <code>dfCalendar</code> con un rango de fechas mensuales desde hace 18 meses hasta el presente. Luego se extraen los campos de a\u00f1o y mes, y se crea un campo <code>Periodo</code> en formato <code>A\u00f1oMes</code>. El proceso se registra en el log.</p> <pre><code>#Creaci\u00f3n tabla calendario\ndfCalendar = pd.DataFrame( pd.date_range(date.today() - relativedelta(months = 18),date.today(), \n              freq='MS').tolist() , columns = ['PRIMER_DIA'] )\ndfCalendar['ANIO'] = dfCalendar['PRIMER_DIA'].dt.year\ndfCalendar['MES'] = dfCalendar['PRIMER_DIA'].dt.month\ndfCalendar['PERIODO'] = dfCalendar['ANIO']*100 + dfCalendar['MES']\ndfCalendar['PERIODO'] = dfCalendar['PERIODO'].astype(str)\nlogger.info('CREACION TABLA CALENDARIO')\n# Nuevo orden de columnas\nnuevo_orden = ['PERIODO', 'ANIO', 'MES','PRIMER_DIA']\n\n# Reordenar usando reindex\ndfCalendar = dfCalendar.reindex(columns=nuevo_orden).copy()\n</code></pre> <pre><code>2024-10-22 21:23:54,528 - INFO - CREACION TABLA CALENDARIO\n</code></pre>"},{"location":"seccion/1.2-DimCalendarioMensual/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfCalendar</code> en la tabla <code>DimCalendario</code> de la base DWH, reemplazando su contenido si la tabla ya existe. Se registra el tiempo que toma esta operaci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(dfCalendar, 'BD_Dim_Calendario', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-22 21:23:58,889 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-22 21:23:59,161 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-22 21:24:01,365 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Calendario\n2024-10-22 21:24:02,285 - INFO - Tabla almacenada correctamente.\n2024-10-22 21:24:02,411 - INFO - ALMACENAMIENTO ---  --- 3.52 seconds ---\n2024-10-22 21:24:02,413 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-17 11:49:13,311 - INFO - FINAL ETL --- 1.15 seconds ---\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/","title":"1.3 Dimensiones","text":""},{"location":"seccion/1.3-Dimensiones/#intoduccion","title":"Intoducci\u00f3n","text":"<p>Este proceso ETL (Extract, Transform, Load) tiene como objetivo consolidar y estructurar datos de diversas fuentes en tablas dimensionales para su almacenamiento en un Data Warehouse (DWH), facilitando el an\u00e1lisis y reporte posterior. Utilizando Python y SQL, el flujo ETL extrae datos de m\u00faltiples tablas de entrada provenientes de bases de datos como <code>subsidio</code>, <code>schoolkits</code>, <code>empresa</code>, y <code>xml4</code>, realizando transformaciones para estandarizar y mejorar la calidad de los datos. Cada tabla de entrada aporta informaci\u00f3n clave sobre personas, actividades econ\u00f3micas, zonas, sucursales y otros atributos necesarios para el an\u00e1lisis.</p> <p>Las tablas de entrada incluyen <code>sat25</code>, que define el tipo de persona; <code>subsi04</code>, que contiene informaci\u00f3n de actividades econ\u00f3micas; y <code>subsi36</code>, que proporciona estados de inactivaci\u00f3n y c\u00f3digos de afiliaci\u00f3n. Tambi\u00e9n se usan <code>SK13</code> para zonas geogr\u00e1ficas, <code>gener18</code> para c\u00f3digos de documentos, <code>xml4c085</code> con informaci\u00f3n de sucursales de empresas, y <code>gener08</code> que ofrece datos de ciudades. Adem\u00e1s, se utilizan tablas en el DWH, como <code>DimAuxEstadoAfiliacion</code> y <code>DimTipoVinculacion</code>, que proveen datos sobre estados de afiliaci\u00f3n y tipos de calidad del aportante.</p> <p>Las tablas resultantes se almacenan en el DWH bajo nombres descriptivos, tales como <code>BD_Dim_Tipo_Persona</code> para la estructura de tipos de personas, <code>BD_Dim_Actividad</code> para actividades econ\u00f3micas, <code>BD_Dim_Zona</code> para zonas geogr\u00e1ficas, <code>BD_Dim_Empresas_Sucursales</code> para detalles de sucursales, y <code>BD_Dim_Ciudades</code> para la estandarizaci\u00f3n de datos de ciudades. Estas dimensiones permiten un an\u00e1lisis organizado de datos estructurados y categorizados para facilitar los reportes y m\u00e9tricas clave.</p>"},{"location":"seccion/1.3-Dimensiones/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL de Dimensiones\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant Script as Script ETL\n  participant DB as Bases de Datos\n  participant DWH as Data Warehouse\n\n  \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar construcci\u00f3n de dimensiones\n  Script-&gt;&gt;DB: Conectar y extraer datos de tablas de entrada &lt;br&gt; (e.g., sat25, subsi04, xml4c085, gener08)\n  DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n  Script-&gt;&gt;Script: Transformar datos (limpieza, estandarizaci\u00f3n)\n  Script-&gt;&gt;DWH: Cargar tablas de salida en DWH &lt;br&gt; (e.g., BD_Dim_Tipo_Persona, BD_Dim_Actividad, BD_Dim_Ciudades)\n  DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n  Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/1.3-Dimensiones/#etl","title":"ETL","text":""},{"location":"seccion/1.3-Dimensiones/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque de c\u00f3digo inicializa el entorno para la conexi\u00f3n y manipulaci\u00f3n de datos con SQLAlchemy y Pandas, junto con <code>numpy</code> para el manejo de datos num\u00e9ricos y <code>logging</code> para la generaci\u00f3n de registros. Se configura el <code>sys.path</code> para importar funciones personalizadas desde un archivo externo (<code>Funciones.py</code>). Entre las funciones importadas se encuentran herramientas espec\u00edficas para el flujo de datos como <code>log_tiempo</code>, <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, y <code>StoreDuplicated</code>, as\u00ed como utilidades para el procesamiento y estandarizaci\u00f3n de direcciones (<code>estandarizar_direccion</code>, <code>marcar_direcciones_estandarizadas</code>). El script ejecuta <code>testfunciones()</code> para verificar que las funciones importadas est\u00e1n listas para su uso, asegurando un flujo controlado y adecuado para el procesamiento y almacenamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\n#from datetime import date\nstart_time = time.time()\n#from dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import log_tiempo, guardar_en_dwh, cargar_tablas, StoreDuplicated, obtener_conexion, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 14:32\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Dimensiones.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Dimensiones.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 14:32:29,302 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#consultas-sql-para-carga-de-datos-dimensionales-en-data-warehouse","title":"Consultas SQL para Carga de Datos Dimensionales en Data Warehouse","text":"<p>Este c\u00f3digo define una serie de consultas SQL en <code>qr_structure</code> que extraen datos desde diversas tablas relacionadas con subsidios y empresas. Cada consulta selecciona columnas relevantes y realiza uniones con otras tablas para enriquecer los datos:</p> <ul> <li><code>sat25</code>, <code>subsi04</code>, y <code>subsi36</code> extraen detalles de tipo de persona, actividad econ\u00f3mica, y estados de inactivaci\u00f3n de afiliados, respectivamente.</li> <li><code>SK13</code> y <code>gener18</code> seleccionan informaci\u00f3n de zonas y c\u00f3digos de documentos.</li> <li><code>xml4c085</code> y <code>gener08</code> consolidan informaci\u00f3n sobre sucursales empresariales y ciudades.</li> </ul> <p>El diccionario <code>dim_names</code> asigna un nombre de tabla para cada consulta en el Data Warehouse, lo que facilita la organizaci\u00f3n y carga de estas tablas en <code>df_structure</code>.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"sat25\":'''select tipper as TIPPER, detalle as TIPO_PERSONA from subsidio.sat25''',\n    \"subsi04\":'''select codact as COD_ACTIVIDAD, detalle as ACTIVIDAD_ECONOMICA from subsidio.subsi04''',\n    \"subsi36\":'''select codest as COD_EST_INAC, detalle as ESTADO_INACTIVACION, tipo as ID_TIPO_AFILIADO, codsat as COD_SAT from subsidio.subsi36''',\n    \"SK13\":'''select CODIGOZONA as COD_ZONA, NOMBREZONA as ZONA from schoolkits.SK13''',\n    \"gener18\": '''select coddoc as CODDOC, detdoc as TIPO_DOCUMENTO, codssf as COD_SUPERSUBSIDIO,codgia as COD_GIASS from empresa.gener18''',\n    \"subsi02\":'''select \n        'nit' as 'NIT',\n        'digver' as 'DIGITODEVERIFICACIONDENITDELAEMPRESA',\n        'tipper' as 'INDICAELTIPODEPERSONA.N=NATURAL,J=JUR\u00cdDICA',\n        'coddoc' as 'TIPODEDOCUMENTO',\n        'razsoc' as 'RAZ\u00d3NSOCIALDELAEMPRESA',\n        'priape' as 'PRIMERAPELLIDO.PARAPERSONASNATURALES.',\n        'segape' as 'SEGUNDOAPELLIDO.PARAPERSONASNATURALES.',\n        'prinom' as 'PRIMERNOMBRE.PARAPERSONASNATURALES.',\n        'segnom' as 'SEGUNDONOMBRE.PARAPERSONASNATURALES.',\n        'sigla' as 'SIGLAEMPRESA',\n        'nomcom' as 'NOMBRECOMERCIAL',\n        'coddocreppri' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALPRINCIPAL',\n        'cedrep' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALPRINCIPAL',\n        'repleg' as 'NOMBREDELREPRESENTANTELEGALPRINCIPAL',\n        'coddocrepsup' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALSUPLENTE',\n        'cedrepsup' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALSUPLENTE',\n        'replegsup' as 'NOMBREDELREPRESENTANTELEGALSUPLENTE',\n        'jefper' as 'NOMBREDELJEFEDEPERSONAL',\n        'direccion' as 'DIRECCIONDEDELAEMPRESAENELFORMULARIODEREGISTRO',\n        'codciu' as 'C\u00d3DIGOCIIUDANE',\n        'codbar' as 'C\u00d3DIGODELBARRIO',\n        'celular' as 'N\u00daMERODECELULARDECONTACTO',\n        'telefono' as 'N\u00daMERODETEL\u00c9FONODECONTACTO',\n        'fax' as 'N\u00daMERODEFAXDECONTACTO',\n        'email' as 'CORREOELECTR\u00d3NICODECONTACTO',\n        'codzon' as 'C\u00d3DIGODELAZONADELAEMPRESA'        \n        from subsidio.subsi02''',\n    \"subsi15\":'''select * from subsidio.subsi15''',    \n    \"xml4c085\":'''select CONCAT(ID_AFILIADO, \"_\", COD_SUCURSAL) as ID_SUCURSAL,\n                    bf.*\n                    from\n                    (select \n                    CONCAT(\n                    CAST(\n                    CASE \n                    WHEN b.tipide = 1 THEN 'CC'\n                    WHEN b.tipide = 3 THEN 'RC'\n                    WHEN b.tipide = 4 THEN 'CE'\n                    WHEN b.tipide = 6 THEN 'PA'\n                    WHEN b.tipide = 7 THEN 'NI'            \n                    ELSE b.tipide\n                    END AS CHAR\n                    ),\n                    b.nit\n                    ) AS ID_AFILIADO,\n                    b.nit as NIT_SUCURSAL,\n                    c4.codsuc as COD_SUCURSAL,\n                    c4.detalle as NOMBRE_SUCURSAL,\n                    c4.direccion as DIRECCION_SUCURSAL,\n                    c1.codciu as CODIGO_CIUDAD,\n                    c4.telefono as TELEFONO_SUCURSAL,\n                    c4.fax as FAX_SUCURSAL,\n                    b.divpol as CODZON_SUCURSAL,\n                    c4.ofiafi  as COD_OFICINA,\n                    c4.nomcon as NOMBRE_CONTACTO,\n                    c4.email as EMAIL_SUCURSAL,\n                    c2.calsuc as COD_CALIDAD_SUCURSAL,\n                    b.codact as COD_ACTIVIDAD,\n                    c4.codind as COD_INDEPENDIENTES,\n                    c4.traapo as TRABAJADORES_APORTANTES,\n                    c4.valapo as VALOR_APORTES,\n                    c4.actapr as ACTA_APROBACION,\n                    c3.perafi as PERIODO_AFILIACION,\n                    c4.fecmod as ULT_FECHA_AFILIACION,\n                    IF(b.estado = 1, \"A\", \"I\") AS ESTADO_SUCURSAL,\n                    c4.resest as FECHA_RESP_ESTADO,\n                    c1.codest COD_ESTADO,\n                    c1.fecest as FECHA_ESTADO,\n                    c4.totapo as TOTAL_APORTES,\n                    c4.observacion as OBSERVACIONES\n                from xml4.xml4c085 as b\n                left join \n                    (select CONCAT(\n                            CAST(\n                            CASE \n                            WHEN coddoc = 'CC' THEN 1 \n                            WHEN coddoc = 'CE' THEN 4 \n                            WHEN coddoc = 'NI' THEN 7 \n                            WHEN coddoc = 'PA' THEN 6 \n                            WHEN coddoc = 'RC' THEN 3 \n                            ELSE coddoc \n                            END AS CHAR\n                            ),\n                            nit\n                            ) AS cod, \n                            codciu, \n                            codest, \n                            fecest\n                    from subsidio.subsi02 group by cod) as c1\n                on CONCAT(b.tipide, b.nit) = c1.cod \n                left join subsidio.subsi48 as c2\n                on b.nit = c2.nit\n                left join \n                    (select CONCAT( tipide , nit ) as cod, \n                    MIN(periodo) as perafi\n                    from xml4.xml4c085 \n                    where estado=1 \n                    group by tipide , nit) as c3\n                on CONCAT(b.tipide, b.nit) = c3.cod\n                left join subsidio.subsi48 as c4\n                on b.nit = c4.nit\n                group by b.tipide, b.nit) as bf''',\n    \"gener08\":'''SELECT * \n                FROM empresa.gener08 as GER08\n                INNER JOIN empresa.gener07 AS GER07 \n                ON substring(GER08.codciu,1,2) COLLATE latin1_spanish_ci = GER07.coddep\n                GROUP BY substring(GER08.codciu,1,2);        \n                '''\n            }\ndim_names = {\n    \"sat25\":'BD_Dim_Tipo_Persona',\n    \"subsi04\":'BD_Dim_Actividad',\n    \"subsi36\":'BD_Dim_Estados_Inactivacion',\n    \"SK13\":'BD_Dim_Zona',\n    \"gener18\":'BD_Dim_Codigo_Documento',\n    \"xml4c085\":'BD_Dim_Empresas_Sucursales',\n    \"BD_Dim_Tipo_Afiliado\":'BD_Dim_Tipo_Afiliado',\n    \"gener08\":'BD_Dim_Ciudades'\n            }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 14:32:29,312 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 14:32:29,387 - INFO - CONEXION A BASE MINERVA\n2024-10-30 14:32:29,722 - INFO - Cargando sat25 \n2024-10-30 14:32:29,753 - INFO - Cargada sat25, 2 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,754 - INFO - Cargando subsi04 \n2024-10-30 14:32:29,855 - INFO - Cargada subsi04, 526 registros finales obtenidos. --- 0.10 seconds ---\n2024-10-30 14:32:29,855 - INFO - Cargando subsi36 \n2024-10-30 14:32:29,883 - INFO - Cargada subsi36, 113 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,884 - INFO - Cargando SK13 \n2024-10-30 14:32:29,913 - INFO - Cargada SK13, 31 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,914 - INFO - Cargando gener18 \n2024-10-30 14:32:29,946 - INFO - Cargada gener18, 10 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,947 - INFO - Cargando subsi02 \n2024-10-30 14:32:35,023 - INFO - Cargada subsi02, 33,088 registros finales obtenidos. --- 5.08 seconds ---\n2024-10-30 14:32:35,024 - INFO - Cargando subsi15 \n2024-10-30 14:34:11,558 - INFO - Cargada subsi15, 433,375 registros finales obtenidos. --- 1.61 minutes ---\n2024-10-30 14:34:11,559 - INFO - Cargando xml4c085 \n2024-10-30 14:34:30,461 - INFO - Cargada xml4c085, 20,638 registros finales obtenidos. --- 18.90 seconds ---\n2024-10-30 14:34:30,462 - INFO - Cargando gener08 \n2024-10-30 14:34:30,496 - INFO - Cargada gener08, 33 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:34:30,554 - INFO - CARGUE TABLAS DESDE MYSQL --- gener08 --- 2.02 minutes ---\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se valida la existencia de registros duplicados en cada una de las tablas cargadas en <code>df_structure</code>. Para ello, se excluye la columna <code>id</code> y se comparan las dem\u00e1s columnas. Los registros duplicados se almacenan usando la funci\u00f3n <code>StoreDuplicated</code>, y posteriormente se eliminan los duplicados del dataframe. El proceso se registra en el log para cada tabla, junto con el tiempo total de ejecuci\u00f3n del validador.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlog_tiempo(logger, f'VALIDADOR DUPLICADOS ', time.time() - validador_time)\n</code></pre> <pre><code>2024-10-30 14:34:30,576 - INFO - VALIDADOR TABLA: sat25\n2024-10-30 14:34:30,585 - INFO - VALIDADOR TABLA: subsi04\n2024-10-30 14:34:30,598 - INFO - VALIDADOR TABLA: subsi36\n2024-10-30 14:34:30,609 - INFO - VALIDADOR TABLA: SK13\n2024-10-30 14:34:30,620 - INFO - VALIDADOR TABLA: gener18\n2024-10-30 14:34:31,274 - INFO - VALIDADOR TABLA: subsi02\n2024-10-30 14:34:44,150 - INFO - VALIDADOR TABLA: subsi15\n2024-10-30 14:34:44,416 - INFO - VALIDADOR TABLA: xml4c085\n2024-10-30 14:34:44,427 - INFO - VALIDADOR TABLA: gener08\n2024-10-30 14:34:44,428 - INFO - VALIDADOR DUPLICADOS  --- 13.87 seconds ---\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#transformacion-de-datos","title":"Transformaci\u00f3n de datos","text":"<p>En esta fase, se realiza una limpieza de los datos en cada tabla de <code>df_structure</code>. Se transforman todas las columnas de tipo texto a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final de los valores, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 14:35:42,956 - INFO - LIMPIEZA --- 58.51 seconds ---\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#creacion-de-tabla-de-dimension-de-tipos-de-afiliados","title":"Creaci\u00f3n de Tabla de Dimensi\u00f3n de Tipos de Afiliados","text":"<p>Este c\u00f3digo crea una tabla de dimensi\u00f3n en <code>df_structure</code> llamada <code>BD_Dim_Tipo_Afiliado</code>, que contiene los tipos \u00fanicos de afiliados obtenidos de la columna <code>ID_TIPO_AFILIADO</code> en <code>subsi36</code>. Para cada tipo de afiliado \u00fanico, asigna un <code>ID_REGISTRO</code> numerado y mapea el valor a una descripci\u00f3n (<code>Trabajadores</code>, <code>Beneficiarios</code>, <code>Empresas</code>) mediante un diccionario.</p> <p>Las columnas se ordenan como <code>ID_REGISTRO</code>, <code>ID_TIPO_AFILIADO</code> y <code>TIPO_AFILIADO</code> para mejorar la organizaci\u00f3n. Esta tabla de dimensi\u00f3n proporciona una referencia estandarizada de los tipos de afiliados para el Data Warehouse.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['subsi36']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ID_TIPO_AFILIADO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ID_TIPO_AFILIADO'])\n\n# Agregar manualmente una columna numerada como 'index'\ndf_valores_unicos['ID_REGISTRO'] = range(len(df_valores_unicos))\n\n# Agregar manualmente el tipo de afiliado\ntipo_afiliado = { 'T':'Trabajadores', 'B': 'Beneficiarios', 'E':'Empresas'}\ndf_valores_unicos['TIPO_AFILIADO'] = df_valores_unicos['ID_TIPO_AFILIADO'].map(tipo_afiliado)\n\n# Ordenar columnas\ncolumnas_orden = ['ID_REGISTRO','ID_TIPO_AFILIADO', 'TIPO_AFILIADO']\ndf_valores_unicos = df_valores_unicos.reindex(columns=columnas_orden)\n\n# Agregar el DataFrame 'df_valores_unicos' a 'df_structure' con una nueva clave\ndf_structure['BD_Dim_Tipo_Afiliado'] = df_valores_unicos\ndf_structure['BD_Dim_Tipo_Afiliado']\n</code></pre> ID_REGISTRO ID_TIPO_AFILIADO TIPO_AFILIADO 0 0 T Trabajadores 1 1 B Beneficiarios 2 2 E Empresas"},{"location":"seccion/1.3-Dimensiones/#renombrado-de-columnas-en-la-tabla-gener08","title":"Renombrado de Columnas en la Tabla <code>gener08</code>","text":"<p>Este c\u00f3digo actualiza los nombres de columnas en el DataFrame <code>gener08</code> dentro de <code>df_structure</code> para mejorar la claridad y consistencia de los datos. Las columnas <code>codciu</code>, <code>detciu</code>, y <code>clarur</code> se renombraron a <code>CODIGO_CIUDAD</code>, <code>NOMBRE_CIUDAD</code>, y <code>TIPO_ZONA</code>, respectivamente. Esto facilita la identificaci\u00f3n de cada campo en <code>gener08</code>, estandarizando su nomenclatura antes de realizar an\u00e1lisis o cargar los datos en el Data Warehouse.</p> <pre><code># Cambiar nombre de la columna\ndf_structure['gener08'].rename(columns={\n    'codciu': 'CODIGO_CIUDAD',\n    'detciu': 'NOMBRE_CIUDAD',\n    'clarur': 'TIPO_ZONA'\n    }, inplace=True)\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#consultas-para-dimensiones-de-estado-de-afiliacion-y-calidad-de-aportante-en-dwh","title":"Consultas para Dimensiones de Estado de Afiliaci\u00f3n y Calidad de Aportante en DWH","text":"<p>Este c\u00f3digo define consultas en <code>qr_structure_dwh</code> para cargar datos de dos dimensiones en el Data Warehouse: <code>DimAuxEstadoAfiliacion</code> y <code>DimTipoVinculacion</code>. Estas consultas extraen los estados de afiliaci\u00f3n (<code>COD_EST_AFIL</code> y <code>ESTADO_AFILIACION</code>) y las categor\u00edas de calidad del aportante (<code>COD_CALIDAD_SUCURSAL</code>, <code>CALIDAD_APORTANTE_1</code>, <code>CALIDAD_APORTANTE_2</code>, <code>CALIDAD_APORTANTE_SUBSI15</code>). Cada consulta se guarda en <code>df_structure_dwh</code> bajo nombres espec\u00edficos definidos en <code>dim_names_dwh</code>, que facilita su identificaci\u00f3n en el DWH.</p> <p>La conexi\u00f3n a la base de datos DWH se establece usando <code>motor_dwh</code>, y las tablas se cargan mediante <code>cargar_tablas</code>, permitiendo integrar estas dimensiones clave para an\u00e1lisis posteriores.</p> <pre><code>#Lista de querys DWH\nqr_structure_dwh = {\n    \"DimAuxEstadoAfiliacion\" : '''select \n        estado_old as COD_EST_AFIL, \n        estado_new as ESTADO_AFILIACION\n        from dwh.gb_DimAuxEstadoAfiliacion''',\n    \"DimTipoVinculacion\" : '''select \n        Tipo as COD_CALIDAD_SUCURSAL,\n        Agrupador1 as CALIDAD_APORTANTE_1,\n        Agrupador2 as CALIDAD_APORTANTE_2,\n        AgrupadorSubsi15 as CALIDAD_APORTANTE_SUBSI15\n        from dwh.gb_DimTipoVinculacion'''\n}\n\ndim_names_dwh = {\n    \"DimAuxEstadoAfiliacion\":'BD_Dim_Estados_Afiliacion',\n    \"DimTipoVinculacion\":'BD_Dim_Calidad_Aportante'\n}\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS DWH')\n\n#Conexion a base DWH\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-30 14:35:43,014 - INFO - LECTURA DE QUERYS DWH\n2024-10-30 14:35:43,017 - INFO - CONEXION A BASE DWH\n2024-10-30 14:35:43,319 - INFO - Cargando DimAuxEstadoAfiliacion \n2024-10-30 14:35:43,346 - INFO - Cargada DimAuxEstadoAfiliacion, 4 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:35:43,348 - INFO - Cargando DimTipoVinculacion \n2024-10-30 14:35:43,378 - INFO - Cargada DimTipoVinculacion, 11 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:35:43,429 - INFO - CARGUE TABLAS DESDE MYSQL --- DimTipoVinculacion --- 0.41 seconds ---\n</code></pre>"},{"location":"seccion/1.3-Dimensiones/#estandarizacion-y-reorganizacion-de-direcciones-en-sucursales","title":"Estandarizaci\u00f3n y Reorganizaci\u00f3n de Direcciones en Sucursales","text":"<p>Este c\u00f3digo realiza una limpieza y estandarizaci\u00f3n de las direcciones en <code>df_sucursales</code> (cargado desde <code>df_structure['xml4c085']</code>), que contiene datos de sucursales de empresas. Primero, renombra <code>DIRECCION_SUCURSAL</code> a <code>DIRECCION_ORIGINAL</code> y asegura que sea de tipo texto. Luego, aplica <code>estandarizar_direccion</code> para generar una columna <code>DIRECCION_LIMPIA</code> con una versi\u00f3n normalizada de las direcciones.</p> <p>La funci\u00f3n <code>marcar_direcciones_estandarizadas</code> verifica si <code>DIRECCION_LIMPIA</code> comienza con ciertas abreviaturas estandarizadas, a\u00f1adiendo una columna <code>DIRECCION_ESTANDARIZADA</code> donde esto se cumple. Despu\u00e9s, se reorganizan las columnas en el orden especificado para mejorar la organizaci\u00f3n del DataFrame.</p> <p>Finalmente, se eliminan los registros donde <code>ID_SUCURSAL</code> es nulo, manteniendo solo las filas con un identificador de sucursal v\u00e1lido. La tabla <code>df_structure['xml4c085']</code> se actualiza con el DataFrame modificado.</p> <pre><code>df_sucursales = df_structure['xml4c085'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_sucursales.rename(columns={'DIRECCION_SUCURSAL':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_sucursales['DIRECCION_ORIGINAL'] = df_sucursales['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_sucursales['DIRECCION_LIMPIA'] = df_sucursales['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_sucursales, 'DIRECCION_LIMPIA')\n\n# Reordenamos las columnas\n\ncolumnas_sucursales = ['ID_SUCURSAL', 'ID_AFILIADO', 'NIT_SUCURSAL', 'COD_SUCURSAL',\n       'NOMBRE_SUCURSAL', 'DIRECCION_ORIGINAL', 'DIRECCION_LIMPIA','DIRECCION_ESTANDARIZADA', 'COD_CIU', 'TELEFONO_SUCURSAL',\n       'FAX_SUCURSAL', 'CODZON_SUCURSAL', 'COD_OFICINA', 'NOMBRE_CONTACTO','EMAIL_SUCURSAL', 'COD_CALIDAD_SUCURSAL', 'COD_ACTIVIDAD',\n       'COD_INDEPENDIENTES', 'TRABAJADORES_APORTANTES', 'VALOR_APORTES','ACTA_APROBACION', 'PERIODO_AFILIACION', 'ULT_FECHA_AFILIACION',\n       'ESTADO_SUCURSAL', 'FECHA_RESP_ESTADO', 'COD_ESTADO', 'FECHA_ESTADO','TOTAL_APORTES', 'OBSERVACIONES']\ndf_sucursales = df_sucursales.reindex(columns=columnas_sucursales)\n\n# Eliminar registros donde ID_SUCURSAL es nulo\ndf_sucursales = df_sucursales.dropna(subset=['ID_SUCURSAL'])\n\ndf_structure['xml4c085'] = df_sucursales.copy()\ndf_structure['xml4c085']\n</code></pre> ID_SUCURSAL ID_AFILIADO NIT_SUCURSAL COD_SUCURSAL NOMBRE_SUCURSAL DIRECCION_ORIGINAL DIRECCION_LIMPIA DIRECCION_ESTANDARIZADA COD_CIU TELEFONO_SUCURSAL ... VALOR_APORTES ACTA_APROBACION PERIODO_AFILIACION ULT_FECHA_AFILIACION ESTADO_SUCURSAL FECHA_RESP_ESTADO COD_ESTADO FECHA_ESTADO TOTAL_APORTES OBSERVACIONES 0 CC1000063770_001 CC1000063770 1000063770 001 PALOMO JIMENEZ DANIEL DAVID MZ K1 CS 14  CONCEPCION MZ K1 CS 14 CONCEPCION Si NaN 6054333300 ... 0.0 NaN 202310 NaN A NaN 53 2023-11-30 0.0 NaN 1 CC1000521027_001 CC1000521027 1000521027 001 CARDONA BIJALBA JESUS ALFONSO KM 38 LT 4  VRDA EMNDIHUACA  CARRERA TRONCAL D... KM 38 LT 4 VRDA EMNDIHUACA CR TRONCAL DEL CARIBE Si NaN 3165233112 ... 0.0 NaN 202309 NaN A NaN 53 2024-01-30 0.0 NaN 2 CC1000697509_001 CC1000697509 1000697509 001 VELEZ HERNANDEZ NICOLAS CARR MINCA CR 1 367 CA MINCA CR 1 367 Si NaN 3232274086 ... 0.0 NaN 202407 NaN A NaN NaN NaN 0.0 NaN 3 CC1001832447_001 CC1001832447 1001832447 001 DE HORTA SANCHEZ JEAN CARLOS CL 31 B  #54   -77      BARRIO OLAYA HERRERA CL 31 B 54 77 BR OLAYA HERRERA Si NaN 3123030265 ... 0.0 NaN 202408 NaN A NaN NaN NaN 0.0 NaN 4 CC1001875363_001 CC1001875363 1001875363 001 ANAYA JIMENEZ EDWIN CR 59 B  #6 A  -105 CR 59 B 6 A 105 Si NaN 3016527856 ... 0.0 NaN 202302 NaN A NaN 14 2024-05-15 0.0 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 20633 CC9877093_001 CC9877093 9877093 001 PAYARES ARIZA JAIRO LUIS CL 30   #26   -56      SAN PEDRO CL 30 26 56 SAN PEDRO Si NaN 3102934954 ... 0.0 NaN 202310 NaN A NaN NaN NaN 0.0 NaN 20634 CC98772638_001 CC98772638 98772638 001 URE?A VESGA DIEGO ANDRES CALLE 17A N? 9-80 BRR PUEBLITO CL 17A 9 80 BR PUEBLITO Si NaN 4215294 ... 0.0 NaN 201812 NaN A NaN 53 2024-04-03 0.0 MIGRACION 20635 CC9877977_001 CC9877977 9877977 001 VASQUEZ CANTILLO WDEYMER CRISTOBAL CLL 3 N\u00b0 11 -75 CLL 3 11 75 No NaN 4158431 ... 0.0 NaN NaN 2019-07-01 I NaN 53 2019-07-31 0.0 MIGRACION 20636 CC9878088_001 CC9878088 9878088 001 CANDANOZA VALENCIA YEISON ENRIQUE CR 5 #15 -25 PRADERA CR 5 15 25 PRADERA Si NaN 3022035444 ... 0.0 NaN 202209 2023-04-20 A NaN 98 2022-09-30 0.0 NaN 20637 CC9878126_001 CC9878126 9878126 001 DE LA CRUZ OROZCO JULIO CESAR MANZANA B CASA 4 MZ B CS 4 Si NaN 5555555 ... 0.0 NaN NaN 2014-09-26 I NaN 110 2019-09-10 0.0 MIGRACION <p>20615 rows \u00d7 29 columns</p>"},{"location":"seccion/1.3-Dimensiones/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 14:35:44,416 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 14:35:44,418 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 14:35:44,724 - INFO - Almacenando tabla sat25 en DWH como BD_Dim_Tipo_Persona\n2024-10-30 14:35:45,488 - INFO - Tabla sat25 almacenada correctamente como BD_Dim_Tipo_Persona.\n2024-10-30 14:35:45,489 - INFO - Almacenando tabla subsi04 en DWH como BD_Dim_Actividad\n2024-10-30 14:35:46,154 - INFO - Tabla subsi04 almacenada correctamente como BD_Dim_Actividad.\n2024-10-30 14:35:46,156 - INFO - Almacenando tabla subsi36 en DWH como BD_Dim_Estados_Inactivacion\n2024-10-30 14:35:46,703 - INFO - Tabla subsi36 almacenada correctamente como BD_Dim_Estados_Inactivacion.\n2024-10-30 14:35:46,704 - INFO - Almacenando tabla SK13 en DWH como BD_Dim_Zona\n2024-10-30 14:35:47,243 - INFO - Tabla SK13 almacenada correctamente como BD_Dim_Zona.\n2024-10-30 14:35:47,244 - INFO - Almacenando tabla gener18 en DWH como BD_Dim_Codigo_Documento\n2024-10-30 14:35:47,856 - INFO - Tabla gener18 almacenada correctamente como BD_Dim_Codigo_Documento.\n2024-10-30 14:35:47,857 - WARNING - La clave subsi02 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-30 14:35:47,858 - WARNING - La clave subsi15 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-30 14:35:47,859 - INFO - Almacenando tabla xml4c085 en DWH como BD_Dim_Empresas_Sucursales\n2024-10-30 14:35:53,588 - INFO - Tabla xml4c085 almacenada correctamente como BD_Dim_Empresas_Sucursales.\n2024-10-30 14:35:53,589 - INFO - Almacenando tabla gener08 en DWH como BD_Dim_Ciudades\n2024-10-30 14:35:54,155 - INFO - Tabla gener08 almacenada correctamente como BD_Dim_Ciudades.\n2024-10-30 14:35:54,156 - INFO - Almacenando tabla BD_Dim_Tipo_Afiliado en DWH como BD_Dim_Tipo_Afiliado\n2024-10-30 14:35:54,848 - INFO - Tabla BD_Dim_Tipo_Afiliado almacenada correctamente como BD_Dim_Tipo_Afiliado.\n2024-10-30 14:35:54,912 - INFO - ALMACENAMIENTO ---  --- 10.50 seconds ---\n2024-10-30 14:35:54,913 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>guardar_en_dwh(df_structure_dwh, dim_names_dwh, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 14:35:54,921 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 14:35:54,922 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 14:35:55,214 - INFO - Almacenando tabla DimAuxEstadoAfiliacion en DWH como BD_Dim_Estados_Afiliacion\n2024-10-30 14:35:55,756 - INFO - Tabla DimAuxEstadoAfiliacion almacenada correctamente como BD_Dim_Estados_Afiliacion.\n2024-10-30 14:35:55,757 - INFO - Almacenando tabla DimTipoVinculacion en DWH como BD_Dim_Calidad_Aportante\n2024-10-30 14:35:56,482 - INFO - Tabla DimTipoVinculacion almacenada correctamente como BD_Dim_Calidad_Aportante.\n2024-10-30 14:35:56,540 - INFO - ALMACENAMIENTO ---  --- 1.62 seconds ---\n2024-10-30 14:35:56,542 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 14:35:56,554 - INFO - FINAL ETL --- 207.26 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/","title":"1.4 Dimensiones xml","text":""},{"location":"seccion/1.4-Dimensiones_xml/#intoduccion","title":"Intoducci\u00f3n","text":"<p>Este proceso ETL (Extract, Transform, Load) tiene como finalidad extraer datos de varias tablas relacionadas con informaci\u00f3n demogr\u00e1fica y de beneficios de bases de datos <code>xml4</code> y consolidarlos en el Data Warehouse (DWH). A trav\u00e9s de SQL y Python, se aplican transformaciones para limpiar y estandarizar los datos antes de cargarlos en tablas dimensionales y de hechos en el DWH.</p> <p>Las tablas de entrada incluyen <code>xml4b093</code>, que contiene estados civiles; <code>xml4b005</code>, que proporciona detalles de g\u00e9nero; <code>xml4b047</code>, que almacena informaci\u00f3n de nivel educativo; y <code>xml4c087</code>, que se utiliza para contar los beneficios \u00fanicos por trabajador, permitiendo obtener una tabla de hechos. La tabla <code>xml4b008</code> aporta informaci\u00f3n de categor\u00edas relevantes. Estas fuentes ofrecen una base diversa de datos que permite construir dimensiones clave para el an\u00e1lisis de poblaciones y beneficios en el DWH.</p> <p>Las tablas de salida se estructuran en el DWH como:</p> <ul> <li> <p>BD_Dim_Estado_Civil: Define los distintos estados civiles.</p> </li> <li> <p>BD_Dim_Genero: Categoriza los g\u00e9neros.</p> </li> <li> <p>BD_Dim_Nivel_Educativo: Clasifica el nivel educativo de los registros.</p> </li> <li> <p>BD_Fact_Beneficio_Por_Trabajador: Tabla de hechos que cuenta los beneficios \u00fanicos asociados a cada trabajador.</p> </li> <li> <p>BD_Dim_Categoria: Estructura y organiza las categor\u00edas de afiliaci\u00f3n.</p> </li> </ul>"},{"location":"seccion/1.4-Dimensiones_xml/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL de Dimensiones XML\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant Script as Script ETL\n  participant DB as Bases de Datos\n  participant DWH as Data Warehouse\n\n  \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar procesamiento de dimensiones y hechos\n  Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4b093, xml4b005, xml4b047, xml4c087, xml4b008\n  DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n  Script-&gt;&gt;Script: Transformar datos (limpieza, estandarizaci\u00f3n)\n  Script-&gt;&gt;DWH: Cargar tablas en DWH: BD_Dim_Estado_Civil, BD_Dim_Genero, &lt;br&gt; BD_Dim_Nivel_Educativo, BD_Fact_Beneficio_Por_Trabajador, BD_Dim_Categoria\n  DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n  Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#etl","title":"ETL","text":""},{"location":"seccion/1.4-Dimensiones_xml/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, entre otras) y un conjunto de funciones personalizadas desde el archivo `Funciones.e.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas,guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>nombre.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='nombre.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 22:45:58,931 - INFO - Importacion de funciones correcta, 26-10-2024 22:45\n2024-10-26 22:45:58,932 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Este c\u00f3digo define una consulta SQL en un diccionario (<code>qr_structure</code>) que extrae informaci\u00f3n de la tabla <code>subsi09</code> y la combina con la tabla <code>Oportunidad</code> mediante un <code>LEFT JOIN</code>. Se incluyen c\u00e1lculos condicionales para determinar la clasificaci\u00f3n del subsidio (<code>tipsub</code> como 'Rural', 'Urbano' o 'No definido') y para identificar si los pagos fueron oportunos o no (<code>Oportunidad</code>). Esto permite analizar tanto la oportunidad de los pagos como el tipo de subsidio entregado a cada beneficiario (<code>codben</code>).</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"estadoCivil\": \"\"\"\n    SELECT * FROM xml4.xml4b093\n    \"\"\",\n    \"genero\":'''\n    SELECT * FROM xml4.xml4b005\n    ''',\n    \"nivelEducativo\": '''\n    SELECT * FROM xml4.xml4b047;    \n    ''',\n    \"benxtra\": '''\n    select CONCAT(codtra,cedtra) as codTrab, COUNT(DISTINCT(CONCAT(coddoc,documento))) as docben from xml4.xml4c087 group by codTrab;\n    ''',\n    \"categoria\": '''\n    SELECT * FROM xml4.xml4b008;\n    '''\n}\n\ndim_names = {\n    \"estadoCivil\":'BD_Dim_Estado_Civil',\n    \"genero\":'BD_Dim_Genero',\n    \"nivelEducativo\":'BD_Dim_Nivel_Educativo',\n    \"benxtra\":'BD_Fact_Beneficio_Por_Trabajador',\n    \"categoria\":'BD_Dim_Categoria'\n            }\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure)\n</code></pre> <pre><code>2024-10-26 22:45:58,951 - INFO - LECTURA DE QUERYS\n\n\n{'estadoCivil': '\\n    SELECT * FROM xml4.xml4b093\\n    ', 'genero': '\\n    SELECT * FROM xml4.xml4b005\\n    ', 'nivelEducativo': '\\n    SELECT * FROM xml4.xml4b047;    \\n    ', 'benxtra': '\\n    select CONCAT(codtra,cedtra) as codTrab, COUNT(DISTINCT(CONCAT(coddoc,documento))) as docben from xml4.xml4c087 group by codTrab;\\n    ', 'categoria': '\\n    SELECT * FROM xml4.xml4b008;\\n    '}\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-26 22:45:58,965 - INFO - CONEXION A BASE MINERVA\n2024-10-26 22:45:59,555 - INFO - Cargando estadoCivil \n2024-10-26 22:45:59,601 - INFO - Cargada estadoCivil, 6 registros finales obtenidos. --- 0.05 seconds ---\n2024-10-26 22:45:59,602 - INFO - Cargando genero \n2024-10-26 22:45:59,647 - INFO - Cargada genero, 4 registros finales obtenidos. --- 0.04 seconds ---\n2024-10-26 22:45:59,648 - INFO - Cargando nivelEducativo \n2024-10-26 22:45:59,695 - INFO - Cargada nivelEducativo, 17 registros finales obtenidos. --- 0.05 seconds ---\n2024-10-26 22:45:59,696 - INFO - Cargando benxtra \n2024-10-26 22:46:40,648 - INFO - Cargada benxtra, 157,632 registros finales obtenidos. --- 40.95 seconds ---\n2024-10-26 22:46:40,649 - INFO - Cargando categoria \n2024-10-26 22:46:40,696 - INFO - Cargada categoria, 12 registros finales obtenidos. --- 0.05 seconds ---\n2024-10-26 22:46:40,785 - INFO - CARGUE TABLAS DESDE MYSQL --- categoria --- 41.82 seconds ---\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#iteracion-y-conversion-de-nombres-de-columnas-a-mayusculas","title":"Iteraci\u00f3n y Conversi\u00f3n de Nombres de Columnas a May\u00fasculas","text":"<p>Este fragmento de c\u00f3digo recorre cada par <code>key</code> y <code>df</code> en el diccionario <code>df_structure</code>, donde <code>key</code> representa la clave del <code>DataFrame</code> y <code>df</code> el <code>DataFrame</code> en s\u00ed. Dentro del bucle, se convierte cada nombre de columna a may\u00fasculas para estandarizar la estructura de datos, asignando a <code>df.columns</code> una lista de nombres en may\u00fasculas generada mediante una comprensi\u00f3n de lista. Finalmente, se imprime el nombre de la clave (<code>key</code>) junto con la lista de nombres de columnas actualizados, permitiendo verificar que la conversi\u00f3n se realiz\u00f3 correctamente para cada <code>DataFrame</code>.</p> <pre><code># Iterar sobre los valores (DataFrames) en el diccionario\nfor key, df in df_structure.items():\n    df.columns = [col.upper() for col in df.columns]\n    print(f\"{key}: {df.columns.tolist()}\")\n</code></pre> <pre><code>estadoCivil: ['ESTCIV', 'NOMBRE', 'CODIGO']\ngenero: ['TIPGEN', 'NOMBRE', 'CODSEX']\nnivelEducativo: ['GRAESC', 'NOMBRE', 'NIVEDU']\nbenxtra: ['CODTRAB', 'DOCBEN']\ncategoria: ['CODCAT', 'NOMBRE', 'CATAFI', 'CATSER']\n</code></pre>"},{"location":"seccion/1.4-Dimensiones_xml/#registro-de-almacenamiento-de-tablas-en-data-warehouse-dwh","title":"Registro de Almacenamiento de Tablas en Data Warehouse (DWH)","text":"<p>En este proceso de almacenamiento en el DWH, cada <code>DataFrame</code> dentro del diccionario <code>df_structure</code> se guarda en una tabla correspondiente en la base de datos. La funci\u00f3n <code>guardar_en_dwh</code>, configurada con <code>multiple=True</code> y <code>if_exists='replace'</code>, permite iterar sobre los <code>DataFrames</code> y guardar cada uno bajo el nombre de tabla especificado en <code>dim_names</code>, sobrescribiendo los datos existentes.</p> <p>Para cada tabla, el <code>logger</code> registra el inicio de la conexi\u00f3n a la base DWH y la confirmaci\u00f3n exitosa de la conexi\u00f3n. A continuaci\u00f3n, se detallan los mensajes de log para cada <code>DataFrame</code>, indicando su nombre de origen y el nombre con el que fue almacenado en la base de datos, lo que facilita el seguimiento del proceso. El tiempo total de ejecuci\u00f3n del almacenamiento fue de 9.96 segundos, y el <code>logger</code> confirma la finalizaci\u00f3n del proceso. Este registro permite verificar la efectividad del almacenamiento, detectar posibles problemas y asegurar que cada tabla se guard\u00f3 correctamente en el DWH.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\ndf_structure['benxtra'].head()\n</code></pre> <pre><code>2024-10-26 22:46:40,807 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 22:46:40,809 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 22:46:41,375 - INFO - Almacenando tabla estadoCivil en DWH como BD_Dim_Estado_Civil\n2024-10-26 22:46:42,166 - INFO - Tabla estadoCivil almacenada correctamente como BD_Dim_Estado_Civil.\n2024-10-26 22:46:42,168 - INFO - Almacenando tabla genero en DWH como BD_Dim_Genero\n2024-10-26 22:46:43,074 - INFO - Tabla genero almacenada correctamente como BD_Dim_Genero.\n2024-10-26 22:46:43,076 - INFO - Almacenando tabla nivelEducativo en DWH como BD_Dim_Nivel_Educativo\n2024-10-26 22:46:43,932 - INFO - Tabla nivelEducativo almacenada correctamente como BD_Dim_Nivel_Educativo.\n2024-10-26 22:46:43,934 - INFO - Almacenando tabla benxtra en DWH como BD_Fact_Beneficio_Por_Trabajador\n2024-10-26 22:46:49,918 - INFO - Tabla benxtra almacenada correctamente como BD_Fact_Beneficio_Por_Trabajador.\n2024-10-26 22:46:49,920 - INFO - Almacenando tabla categoria en DWH como BD_Dim_Categoria\n2024-10-26 22:46:50,670 - INFO - Tabla categoria almacenada correctamente como BD_Dim_Categoria.\n2024-10-26 22:46:50,765 - INFO - ALMACENAMIENTO ---  --- 9.96 seconds ---\n2024-10-26 22:46:50,766 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> CODTRAB DOCBEN 0 11000098717 1 1 11000120248 4 2 11000121261 7 3 11000217724 1 4 11000221130 1 <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 22:46:50,785 - INFO - FINAL ETL --- 51.89 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/","title":"1.5 Fact Datos Contacto","text":""},{"location":"seccion/1.5-FactDatosContacto/#introduccion","title":"Introducci\u00f3n","text":"<p>En este proceso ETL se extraen y transforman datos de tablas relacionadas con informaci\u00f3n de contacto y datos bancarios de afiliados y beneficiarios, para su carga final en una tabla de hechos en el Data Warehouse (DWH), denominada <code>BD_Fact_Datos_Contactos</code>. Las tablas de origen incluyen <code>xml4c086</code>, que contiene datos sobre afiliados, <code>xml4c085</code>, que maneja informaci\u00f3n de sucursales y empresas, y <code>xml4c087</code>, que presenta datos sobre los beneficiarios. La informaci\u00f3n obtenida de estas tablas cubre datos de contacto como tel\u00e9fono, correo electr\u00f3nico y direcci\u00f3n, adem\u00e1s de detalles sobre el estado civil, ubicaci\u00f3n y cuentas bancarias de los registros procesados. </p> <p>La salida final, <code>BD_Fact_Datos_Contactos</code>, almacena estos datos, permitiendo el an\u00e1lisis de los puntos de contacto y detalles bancarios para cada afiliado o beneficiario. Este proceso aplica limpieza de duplicados, estandarizaci\u00f3n de direcciones y transforma las fechas a un formato trimestral, optimizando los datos para su consulta y an\u00e1lisis.</p>"},{"location":"seccion/1.5-FactDatosContacto/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL para Datos de Contacto\n    autonumber\n    participant \ud83d\udc64 Usuario\n    participant Script as Script ETL\n    participant DB as Bases de Datos\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar ETL de Datos de Contacto\n    Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c086, xml4c085, xml4c087\n    DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n    Script-&gt;&gt;Script: Limpiar, estandarizar y transformar los datos\n    Script-&gt;&gt;DWH: Cargar datos en BD_Fact_Datos_Contactos\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n    Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama detalla el flujo del proceso, incluyendo la limpieza y transformaci\u00f3n de datos antes de su carga en el DWH, lo cual asegura que la tabla de salida <code>BD_Fact_Datos_Contactos</code> est\u00e9 lista para el an\u00e1lisis de datos de contacto y bancarios.</p>"},{"location":"seccion/1.5-FactDatosContacto/#etl","title":"ETL","text":""},{"location":"seccion/1.5-FactDatosContacto/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para la manipulaci\u00f3n de datos y conexiones a bases de datos, importando funciones personalizadas del archivo <code>Funciones.py</code> como <code>StoreDuplicated</code>, <code>RemoveDuplicated</code> para gestionar duplicados, y herramientas de estandarizaci\u00f3n de direcciones (<code>estandarizar_direccion</code>, <code>marcar_direcciones_estandarizadas</code>). Configura el <code>sys.path</code> para permitir la importaci\u00f3n del m\u00f3dulo de funciones externas y utiliza <code>testfunciones()</code> para validar que las funciones se cargaron correctamente antes de iniciar procesos de carga y transformaci\u00f3n de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, RemoveDuplicated, obtener_conexion, cargar_tablas, guardar_en_dwh, testfunciones, setup_logger, estandarizar_direccion, marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 22-10-2024 17:59\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>FactDatosContacto.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactDatosContacto.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-22 17:59:30,720 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#definicion-de-consultas-sql-para-datos-en-tablas-xml","title":"Definici\u00f3n de Consultas SQL para Datos en Tablas XML","text":"<p>El c\u00f3digo define un conjunto de consultas SQL en <code>qr_structure</code> para extraer y estructurar datos de las tablas <code>xml4c086</code>, <code>xml4c085</code>, y <code>xml4c087</code>, que contienen informaci\u00f3n de afiliados, sucursales y beneficiarios. En cada consulta, los identificadores \u00fanicos (<code>id</code>) se crean combinando c\u00f3digos de documento y n\u00fameros de identificaci\u00f3n, con nombres y direcciones completados a partir de uniones con otras tablas (<code>subsi15</code>, <code>subsi02</code>, <code>subsi22</code>). </p> <p>Campos faltantes, como <code>codsuc</code>, <code>codzon</code>, y <code>estciv</code>, se completan como <code>NULL</code> cuando no se encuentran en las tablas de origen, asegurando la consistencia del esquema. Estas consultas proporcionan datos clave sobre ubicaci\u00f3n (<code>codciu</code>), contacto (<code>telefono</code>, <code>email</code>, <code>direccion</code>), y fechas de afiliaci\u00f3n (<code>fecafi</code>), preparando as\u00ed el contenido para su consolidaci\u00f3n y an\u00e1lisis en el sistema.</p> <pre><code>#Lista de querys\nqr_structure = {\n##############################xml4c086###############################################\n    \"xml4c086_dc\":'''SELECT \n    CONCAT(\n        CAST(\n            CASE \n                WHEN b.coddoc = 1 THEN 'CC' \n                WHEN b.coddoc = 2 THEN 'TI' \n                WHEN b.coddoc = 3 THEN 'RC' \n                WHEN b.coddoc = 4 THEN 'CE' \n                WHEN b.coddoc = 5 THEN 'NP' \n                WHEN b.coddoc = 6 THEN 'PA' \n                WHEN b.coddoc = 7 THEN 'NI' \n                WHEN b.coddoc = 8 THEN 'CD' \n                WHEN b.coddoc = 9 THEN 'PE' \n                WHEN b.coddoc = 15 THEN 'PT' \n                ELSE b.coddoc \n            END AS CHAR\n        ), \n        b.cedtra\n    ) AS id, \n    c1.codciu, \n    c1.codsuc, \n    b.codzon, \n    c1.estciv, \n    c1.numcue, \n    c1.tipcue, \n    c1.codban,\n    c1.telefono,\n    c1.email,\n    c1.direccion,\n    CONCAT(SUBSTRING(c5.perafi, 1, 4), '-', SUBSTRING(c5.perafi, 5, 2), '-01') as fecafi\nFROM \n    (SELECT \n        cedtra, \n        coddoc, \n        divpol AS codzon, \n        1 AS Trabajador\n    FROM \n        xml4.xml4c086 \n    GROUP BY \n        coddoc, cedtra) AS b\nLEFT JOIN \n    (SELECT \n        CONCAT(\n            CAST(\n                CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CD' THEN 8 \n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'PE' THEN 9 \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'PT' THEN 15 \n                    WHEN coddoc = 'TI' THEN 2 \n                    ELSE coddoc \n                END AS CHAR\n            ), \n            cedtra\n        ) AS cod, \n        codsuc, \n        codciu, \n        codban, \n        numcue, \n        tipcue, \n        estciv,\n        telefono,\n        email,\n        direccion\n    FROM \n        subsidio.subsi15 \n    GROUP BY \n        cod\n    ) AS c1\n    ON CONCAT(b.coddoc, b.cedtra) = c1.cod\nLEFT JOIN \n    (SELECT \n        CONCAT(coddoc, cedtra) AS cod, \n        MIN(periodo) AS perafi \n    FROM \n        xml4.xml4c086 \n    GROUP BY \n        CONCAT(coddoc, cedtra)\n    ) AS c5\n    ON CONCAT(b.coddoc, b.cedtra) = c5.cod;\n    ''',\n\n##############################xml4c085###############################################\n\n    \"xml4c085_dc\":'''SELECT \n    CONCAT(\n        CAST(\n            CASE \n                WHEN b.tipide = 1 THEN 'CC' \n                WHEN b.tipide = 3 THEN 'RC' \n                WHEN b.tipide = 4 THEN 'CE' \n                WHEN b.tipide = 6 THEN 'PA' \n                WHEN b.tipide = 7 THEN 'NI' \n                ELSE b.tipide \n            END AS CHAR\n        ), \n        b.nit\n    ) AS id, \n    c1.codciu, \n    NULL AS codsuc,  -- Campo no encontrado, se deja como NULL\n    b.codzon, \n    NULL AS estciv,  -- Campo no encontrado, se deja como NULL\n    NULL AS numcue,  -- Campo no encontrado, se deja como NULL\n    NULL AS tipcue,  -- Campo no encontrado, se deja como NULL\n    c1.codban,\n    c1.telefono,\n    c1.email,\n    c1.direccion,\n    CONCAT(SUBSTRING(c4.perafi, 1, 4), '-', SUBSTRING(c4.perafi, 5, 2), '-01') as fecafi\nFROM \n    (SELECT \n        tipide, \n        nit,\n        tipapo,\n        tipsec,\n        divpol AS codzon\n    FROM \n        xml4.xml4c085 \n    GROUP BY \n        tipide, nit) AS b\nLEFT JOIN \n    (SELECT \n        CONCAT(\n            CAST(\n                CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'NI' THEN 7 \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'RC' THEN 3 \n                    ELSE coddoc \n                END AS CHAR\n            ), \n            nit\n        ) AS cod, \n        codciu, \n        NULL AS codban,\n        telefono,\n        email,\n        direccion\n    FROM \n        subsidio.subsi02 \n    GROUP BY \n        cod\n    ) AS c1\n    ON CONCAT(b.tipide, b.nit) = c1.cod\nLEFT JOIN \n    xml4.xml4b072 AS c2\n    ON b.tipapo = c2.tipapo\nLEFT JOIN \n    xml4.xml4b001 AS c3\n    ON b.tipsec = c3.tipsec\nLEFT JOIN \n    (SELECT \n        CONCAT(tipide, nit) AS cod, \n        MIN(periodo) AS perafi \n    FROM \n        xml4.xml4c085 \n    WHERE estado = 1 \n    GROUP BY \n        CONCAT(tipide, nit)\n    ) AS c4\n    ON CONCAT(b.tipide, b.nit) = c4.cod\n\n''',\n\n##############################xml4c087###############################################\n\n        \"xml4c087_dc\":'''SELECT \n    CONCAT(\n        CAST(\n            CASE \n                WHEN b.coddoc = 1 THEN 'CC' \n                WHEN b.coddoc = 2 THEN 'TI' \n                WHEN b.coddoc = 3 THEN 'RC' \n                WHEN b.coddoc = 4 THEN 'CE' \n                WHEN b.coddoc = 5 THEN 'NP' \n                WHEN b.coddoc = 6 THEN 'PA' \n                WHEN b.coddoc = 7 THEN 'NI' \n                WHEN b.coddoc = 8 THEN 'CD' \n                WHEN b.coddoc = 9 THEN 'PE' \n                WHEN b.coddoc = 15 THEN 'PT' \n                ELSE b.coddoc \n            END AS CHAR\n        ), \n        b.documento\n    ) AS id, \n    c1.ciunac AS codciu, \n    NULL AS codsuc,  -- Campo no encontrado, se deja como NULL\n    NULL AS codzon,  -- Campo no encontrado, se deja como NULL\n    NULL AS estciv,  -- Campo no encontrado, se deja como NULL\n    NULL AS numcue,  -- Campo no encontrado, se deja como NULL\n    NULL AS tipcue,  -- Campo no encontrado, se deja como NULL\n    NULL AS codban,  -- Campo no encontrado, se deja como NULL\n    c1.telefono, \n    c1.email, \n    c1.direccion,\n    CONCAT(SUBSTRING(c3.perafi, 1, 4), '-', SUBSTRING(c3.perafi, 5, 2), '-01') as fecafi\nFROM \n    (SELECT \n        coddoc, \n        documento, \n        codigo_dicap AS captra, \n        fecnac, \n        1 AS Beneficiario, \n        tipgen\n    FROM \n        xml4.xml4c087 \n    GROUP BY \n        coddoc, documento) AS b\nLEFT JOIN \n    (SELECT \n        documento, \n        codben, \n        fecexp, \n        ciunac, \n        NULL AS telefono, \n        NULL AS email, \n        NULL AS direccion, \n        estado\n    FROM \n        subsidio.subsi22 \n    GROUP BY \n        documento) AS c1\n    ON b.documento = c1.documento\nLEFT JOIN \n    (SELECT \n        coddoc, \n        documento, \n        MIN(periodo) AS perafi \n    FROM \n        xml4.xml4c087 \n    GROUP BY \n        coddoc, documento) AS c3\n    ON b.coddoc = c3.coddoc AND b.documento = c3.documento\n\n'''\n\n\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-22 17:59:30,734 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n# Ejemplo de c\u00f3mo llamarla\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-22 17:59:30,786 - INFO - CONEXION A BASE MINERVA\n2024-10-22 17:59:31,070 - INFO - Cargando xml4c086_dc \n2024-10-22 17:59:51,280 - INFO - Cargada xml4c086_dc --- 20.21 seconds ---\n2024-10-22 17:59:51,281 - INFO - Cargando xml4c085_dc \n2024-10-22 17:59:53,482 - INFO - Cargada xml4c085_dc --- 2.20 seconds ---\n2024-10-22 17:59:53,484 - INFO - Cargando xml4c087_dc \n2024-10-22 18:00:57,447 - INFO - Cargada xml4c087_dc --- 1.07 minutes ---\n2024-10-22 18:00:57,514 - INFO - CARGUE TABLAS DESDE MYSQL --- xml4c087_dc --- 1.45 minutes ---\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#validador-de-campos-repetidos-y-limpieza-de-duplicados","title":"Validador de Campos Repetidos y Limpieza de Duplicados","text":"<p>Este c\u00f3digo valida y limpia registros duplicados en todas las tablas dentro de <code>df_structure</code>. Para cada tabla, verifica si existe una columna <code>id</code>; si no, intenta crearla concatenando <code>numdoc</code> y <code>coddoc</code>, o registra un error en caso de no encontrar esas columnas. Despu\u00e9s de asegurar la existencia de <code>id</code>, se definen <code>ColumnsToCompare</code> como el conjunto de columnas a comparar, excluyendo <code>id</code>, y se procede a almacenar duplicados utilizando <code>StoreDuplicated</code>, que guarda los registros duplicados en un archivo <code>CSV</code> con el prefijo <code>trazaDuplicados_</code>. </p> <p>A continuaci\u00f3n, <code>RemoveDuplicated</code> elimina los duplicados en cada tabla, conservando el registro m\u00e1s reciente basado en la columna <code>fecest</code>. El tiempo total de validaci\u00f3n y limpieza se registra en el <code>logger</code>, proporcionando un resumen del tiempo de procesamiento de este validador de duplicados.</p> <pre><code># Validador de campos repetidos\nvalidador_time = time.time()\n\n# Iterar sobre todas las tablas\nfor ky in [x for x in df_structure.keys()]:\n    df = df_structure[ky]\n\n    # Validar si la columna 'id' existe; si no, crearla\n    if 'id' not in df.columns:\n        if 'numdoc' in df.columns and 'coddoc' in df.columns:\n            # Crear la columna 'id' concatenando 'numdoc' y 'coddoc'\n            df = df.assign(id=df['numdoc'].astype(str) + df['coddoc'].astype(str))\n            logger.info(f\"Columna 'id' creada en el DataFrame: {ky}\")\n        else:\n            # Si no existen las columnas necesarias para crear 'id', saltar la tabla\n            logger.error(f\"No se encontraron las columnas 'numdoc' y 'coddoc' necesarias para crear 'id' en el DataFrame: {ky}\")\n            continue  # Saltar a la siguiente tabla si no se pueden crear los valores de 'id'\n\n    # Asegurarse de que la columna 'id' existe antes de proceder\n    if 'id' in df.columns:\n        # Definir las columnas para comparar, excluyendo 'id'\n        ColumnsToCompare = [x for x in df.columns if x != 'id']\n\n        # Imprimir el nombre de la tabla en proceso\n        print(f\"Procesando tabla: {ky}\")\n\n        # Almacenar duplicados\n        try:\n            StoreDuplicated('id', ColumnsToCompare, df, f'trazaDuplicados_{ky}')\n        except KeyError as e:\n            logger.error(f\"Error en StoreDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Limpieza inicial para quitar duplicados\n        try:\n            df_structure[ky] = RemoveDuplicated('id', 'fecest', df)\n        except KeyError as e:\n            logger.error(f\"Error en RemoveDuplicated para la tabla {ky}: {str(e)}\")\n            continue\n\n        # Registrar la validaci\u00f3n en el log\n        logger.info(f'VALIDADOR TABLA: {ky}')\n    else:\n        logger.error(f\"La columna 'id' no fue creada correctamente en el DataFrame: {ky}\")\n\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>Procesando tabla: xml4c086_dc\nGuardando duplicados\n\n\n2024-10-22 18:00:58,448 - ERROR - Error en RemoveDuplicated para la tabla xml4c086_dc: 'fecest'\n2024-10-22 18:00:58,501 - ERROR - Error en RemoveDuplicated para la tabla xml4c085_dc: 'fecest'\n\n\nDuplicados guardados en: trazaDuplicados_xml4c086_dc.csv\nProcesando tabla: xml4c085_dc\nGuardando duplicados\nDuplicados guardados en: trazaDuplicados_xml4c085_dc.csv\nProcesando tabla: xml4c087_dc\nGuardando duplicados\n\n\n2024-10-22 18:01:01,254 - ERROR - Error en RemoveDuplicated para la tabla xml4c087_dc: 'fecest'\n2024-10-22 18:01:01,255 - INFO - VALIDADOR DUPLICADOS --- 3.73 seconds ---\n\n\nDuplicados guardados en: trazaDuplicados_xml4c087_dc.csv\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#definicion-de-columnas-por-tipo-y-transformacion-de-fechas","title":"Definici\u00f3n de Columnas por Tipo y Transformaci\u00f3n de Fechas","text":"<p>Este bloque clasifica las columnas en cada tabla dentro de <code>df_structure</code> en tres categor\u00edas: num\u00e9ricas (<code>numericColumns</code>), de fechas (<code>datesColumns</code>), y de texto (<code>textColumns</code>). Utiliza los tipos de datos (<code>int</code> y <code>float</code>) para identificar columnas num\u00e9ricas y selecciona las que contienen <code>\"fec\"</code> en el nombre para reconocer las columnas de fecha. Las columnas de texto se definen como aquellas que no pertenecen a las categor\u00edas num\u00e9ricas o de fechas y que incluyen prefijos comunes como <code>\"cod\"</code>, <code>\"ced\"</code>, y <code>\"num\"</code>.</p> <p>Una vez categorizadas, las columnas identificadas como fechas en cada tabla se convierten al tipo <code>datetime</code>, utilizando <code>pd.to_datetime</code> para manejar errores sin interrupciones. Finalmente, se registra en el <code>logger</code> el tiempo total de procesamiento de esta segunda fase de transformaci\u00f3n, permitiendo un monitoreo detallado del flujo de trabajo.</p> <pre><code>#Definir columnas con sus tipos\ntransfor2_time = time.time()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericColumns = dict()\ndatesColumns = dict()\ntextColumns = dict()\n\nfor ky in list(df_structure.keys()):\n    numericColumns[ky] = df_structure[ky].select_dtypes( include = numerics ).columns.tolist()\n    datesColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x.startswith('fec') ]\n    textColumns[ky] = [x for x in df_structure[ky].columns.tolist() if x not in ( numericColumns[ky] + datesColumns[ky] ) ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('cod') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('ced') ]+ [x for x in df_structure[ky].columns.tolist() if x.startswith('num') ]\n    textColumns[ky] = list(set(textColumns[ky]))\n    #Convertir columnas que tengan \"fec\" en el nombre en tipo fecha\n    df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n\nlogger.info(f'TRANSFORMACION 2 --- {time.time() - transfor2_time:.2f} seconds ---')\n</code></pre> <pre><code>C:\\Users\\Consultor_QCS\\AppData\\Local\\Temp\\ipykernel_20420\\3616572111.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  df_structure[ky][datesColumns[ky]] = df_structure[ky][datesColumns[ky]].apply( lambda x: pd.to_datetime( x , errors='coerce' ) )\n2024-10-22 18:01:01,439 - INFO - TRANSFORMACION 2 --- 0.17 seconds ---\n</code></pre> <pre><code>#df_structure['subsi15_dc'].columns.to_series().groupby(df_structure['subsi15_dc'].dtypes).groups\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#transformacion-y-limpieza-de-datos-en-columnas-de-texto","title":"Transformaci\u00f3n y Limpieza de Datos en Columnas de Texto","text":"<p>Este bloque realiza la limpieza y estandarizaci\u00f3n de datos en las columnas de texto en cada tabla dentro de <code>df_structure</code>. Para cada tabla, convierte el contenido de las columnas de texto a may\u00fasculas para garantizar la uniformidad. Luego, elimina los espacios en blanco al inicio y al final de cada entrada y reemplaza valores como <code>\"NAN\"</code> y <code>\"NONE\"</code> con <code>NaN</code> para unificar los datos nulos. Finalmente, se registra en el <code>logger</code> el tiempo total de ejecuci\u00f3n de este proceso de limpieza, asegurando que el flujo de datos se mantenga eficiente y estandarizado.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list( [x for x in df_structure.keys() ]   ):\n    print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NAN', np.nan)\n    df_structure[ky][textColumns[ky]] = df_structure[ky][textColumns[ky]].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>xml4c086_dc\nxml4c085_dc\nxml4c087_dc\n\n\n2024-10-22 18:01:09,367 - INFO - LIMPIEZA --- 7.91 seconds ---\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#concatenacion-de-tablas","title":"Concatenaci\u00f3n de tablas","text":"<p>Se concatenan todas las tablas presentes en el diccionario <code>df_structure</code> en un \u00fanico dataframe llamado <code>df_struc_Total</code>. La concatenaci\u00f3n se realiza ignorando los \u00edndices previos, lo que crea un dataframe consolidado con todos los registros.</p> <pre><code>#Concatenacion Tablas\ndf_struc_Total = pd.concat( list(df_structure.values()) , ignore_index = True )\n</code></pre> <pre><code>#df_struc_Total = df_struc_Total.drop_duplicates(subset='id', keep='last')\n#df_struc_Total.columns.tolist()\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#agregacion-y-transformacion-de-datos-por-periodo","title":"Agregaci\u00f3n y Transformaci\u00f3n de Datos por Periodo","text":"<p>Este c\u00f3digo realiza una agregaci\u00f3n de los datos en <code>df_struc_Total</code> por afiliado (<code>id</code>) y trimestre (<code>Trimestre</code>), facilitando el an\u00e1lisis en per\u00edodos consolidados. Para evitar la fragmentaci\u00f3n en fechas, <code>fecafi</code> se convierte a formato trimestral, y luego se agrupa por <code>id</code> y <code>Trimestre</code>, seleccionando el \u00faltimo valor disponible de cada campo clave, como <code>codciu</code>, <code>telefono</code>, y <code>direccion</code>. Despu\u00e9s de la agrupaci\u00f3n, <code>Trimestre</code> se elimina del DataFrame.</p> <p>El DataFrame resultante <code>df_struc_Total</code> se transforma: todas las columnas se renombraron en may\u00fasculas y algunos nombres clave (como <code>ID</code> a <code>ID_AFILIADO</code>) se ajustaron para mejorar la claridad. Se conservan solo columnas relevantes, y se ordenan los registros por <code>COD_CIU</code> en orden descendente. Adem\u00e1s, se crea una columna <code>ID_REGISTRO</code> con valores secuenciales para identificaci\u00f3n \u00fanica de cada registro, la cual se coloca al inicio para facilitar la navegaci\u00f3n y lectura en el DataFrame reorganizado <code>df_struc_Total</code>.</p> <pre><code># Iniciar la agregaci\u00f3n en el log\nlogger.info('INICIO DE AGREGACI\u00d3N POR PERIODO')\n\n# Convertir 'fecafi' a un formato trimestral para la agrupaci\u00f3n y evitar fragmentaci\u00f3n\ndf_struc_Total = pd.concat([df_struc_Total, pd.to_datetime(df_struc_Total['fecafi'], errors='coerce').dt.to_period('Q').rename('Trimestre')], axis=1)\n\n# Agrupar por 'id' y 'Trimestre'\ndf_agg = df_struc_Total.groupby(['id', 'Trimestre'], as_index=False).agg({\n    'codciu': 'last', \n    'codsuc': 'last',\n    'codzon': 'last',\n    'estciv': 'last',\n    'numcue': 'last',\n    'tipcue': 'last',\n    'codban': 'last',\n    'telefono': 'last',\n    'email': 'last',\n    'direccion': 'last',\n})\n\n# Eliminar columna 'Trimestre' si no se necesita en el DataFrame final\ndf_agg.drop(columns=['Trimestre'], inplace=True)\n\n# Finalizar la agregaci\u00f3n en el log\nlogger.info('AGREGACI\u00d3N POR PERIODO FINALIZADA')\n\n# Asignar el DataFrame agrupado a df_struc_Total para la siguiente etapa\ndf_struc_Total = df_agg\n</code></pre> <pre><code>2024-10-22 18:01:09,506 - INFO - INICIO DE AGREGACI\u00d3N POR PERIODO\n2024-10-22 18:01:12,158 - INFO - AGREGACI\u00d3N POR PERIODO FINALIZADA\n</code></pre> <pre><code>df_struc_Total = df_agg\n# Convertir todos los nombres de las columnas de df_struc_Total a may\u00fasculas\ndf_struc_Total.columns = df_struc_Total.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf_struc_Total = df_struc_Total.rename(columns={\n    'ID': 'ID_AFILIADO', \n    'CODCIU': 'COD_CIU',\n    'CODSUC':'COD_SUCURSAL',\n    'CODZON':'COD_ZON',\n    'ESTCIV':'ESTADO_CIVIL',\n    'NUMCUE':'NUMERO_CUENTA',\n    'TIPCUE':'TIPO_CUENTA',\n    'CODBAN':'COD_BANCO'\n    })\n\n# Seleccionar solo las columnas que deseas conservar\ndf_struc_Total = df_struc_Total[[\n    'ID_AFILIADO', 'COD_CIU', 'COD_SUCURSAL', 'COD_ZON', 'ESTADO_CIVIL', 'NUMERO_CUENTA', 'TIPO_CUENTA', 'COD_BANCO',\n    'TELEFONO','EMAIL','DIRECCION'\n    ]]\n\ndf_struc_Total = df_struc_Total.sort_values(by='COD_CIU', ascending=False)\n</code></pre> <pre><code># Crear una nueva columna 'ID_REGISTRO' con valores secuenciales\ndf_struc_Total['ID_REGISTRO'] = range(1, len(df_struc_Total) + 1)\n\n# Reorganizar las columnas, colocando 'ID_REGISTRO' de primeras\ncolumnas_ordenadas = ['ID_REGISTRO', 'ID_AFILIADO', 'COD_CIU', 'COD_SUCURSAL', 'COD_ZON', 'ESTADO_CIVIL', 'NUMERO_CUENTA', 'TIPO_CUENTA', 'COD_BANCO',\n    'TELEFONO','EMAIL','DIRECCION']\n\n# Reorganizar el DataFrame\ndf_struc_Total = df_struc_Total[columnas_ordenadas]\n</code></pre>"},{"location":"seccion/1.5-FactDatosContacto/#estandarizacion-de-direcciones-en-el-dataframe-df_struc_total","title":"Estandarizaci\u00f3n de Direcciones en el DataFrame <code>df_struc_Total</code>","text":"<p>Este bloque de c\u00f3digo realiza la estandarizaci\u00f3n de la columna <code>DIRECCION</code> en el DataFrame <code>df_struc_Total</code>. Primero, renombra <code>DIRECCION</code> a <code>DIRECCION_ORIGINAL</code> y asegura que esta columna sea de tipo texto. Luego, utiliza la funci\u00f3n <code>estandarizar_direccion</code> para crear una columna adicional, <code>DIRECCION_LIMPIA</code>, con el contenido de las direcciones estandarizadas. Para identificar direcciones que cumplen con ciertos est\u00e1ndares, aplica <code>marcar_direcciones_estandarizadas</code> sobre <code>DIRECCION_LIMPIA</code>, generando la columna <code>DIRECCION_ESTANDARIZADA</code> si alguna direcci\u00f3n comienza con abreviaturas predefinidas. Esto ayuda a mantener una estructura uniforme y facilita el an\u00e1lisis posterior de los datos de direcci\u00f3n.</p> <pre><code># Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_struc_Total.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_struc_Total['DIRECCION_ORIGINAL'] = df_struc_Total['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_struc_Total['DIRECCION_LIMPIA'] = df_struc_Total['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_struc_Total, 'DIRECCION_LIMPIA')\n\n#df_struc_Total\n</code></pre> ID_REGISTRO ID_AFILIADO COD_CIU COD_SUCURSAL COD_ZON ESTADO_CIVIL NUMERO_CUENTA TIPO_CUENTA COD_BANCO TELEFONO EMAIL DIRECCION_ORIGINAL DIRECCION_LIMPIA DIRECCION_ESTANDARIZADA 723666 1 TI1029863043 99773 None None None None None NaN None None None None No 437386 2 NI901529894 99773 None 99773 None None None NaN 3107875138 CONTABILIDAD@SGILTDA.COM CR 16   #12   -96 CENTRO CR 16 12 96 CENTRO Si 676719 3 RC1136869327 99624 None None None None None NaN None None None None No 222996 4 CC1128191701 99001 001 99001 5 None None NaN None None CIUDAD PERDIDA CIUDAD PERDIDA No 814243 5 TI1127394941 99001 None None None None None NaN None None None None No ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 838407 838408 TI99121105639 None None None None None None NaN None None None None No 838408 838409 TI99121201794 None None None None None None NaN None None None None No 838409 838410 TI99121801309 None None None None None None NaN None None None None No 838410 838411 TI996508415031996 None None 47001 None None None NaN None None None None No 838411 838412 TITI None None None None None None NaN None None None None No <p>838412 rows \u00d7 14 columns</p>"},{"location":"seccion/1.5-FactDatosContacto/#guardar-en-la-base-de-datos-dwh-con-manejo-de-errores","title":"Guardar en la base de datos DWH con manejo de errores","text":"<p>Se intenta guardar los primeros 1000 registros del dataframe <code>df_struc_Total</code> en la tabla <code>BD_FactDatosContactos</code> en la base de datos DWH. Si el proceso es exitoso, se registra el tiempo de almacenamiento en el log. En caso de error, se realiza un rollback para evitar transacciones pendientes y se registra el error en el log.</p> <pre><code># Llamada a la funci\u00f3n para un \u00fanico DataFrame\nguardar_en_dwh(df_struc_Total, 'BD_Fact_Datos_Contactos', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-22 18:01:19,441 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-22 18:01:19,443 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-22 18:01:19,712 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Datos_Contactos\n2024-10-22 18:02:17,391 - INFO - Tabla almacenada correctamente.\n2024-10-22 18:02:17,940 - INFO - ALMACENAMIENTO ---  --- 58.50 seconds ---\n2024-10-22 18:02:17,941 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-22 18:02:17,946 - INFO - FINAL ETL --- 167.23 seconds ---\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/","title":"1.6 Fact Historico Afiliacion Empresas","text":""},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#introduccion","title":"Introducci\u00f3n","text":"<p>En este proceso ETL se consolidan y transforman datos relacionados con la afiliaci\u00f3n de empresas y sus empleados, para ser almacenados en una tabla de hechos dentro del Data Warehouse (DWH), denominada <code>BD_Fact_Historico_Afiliacion_Empresas</code>. Se extrajeron datos desde las tablas <code>xml4c085</code> y <code>op_EmpresasSubsi02_v2</code>, que incluyen informaci\u00f3n detallada sobre el estado de afiliaci\u00f3n de las empresas, as\u00ed como la cantidad de trabajadores, su distribuci\u00f3n por g\u00e9nero, y otros detalles relacionados con el c\u00f3digo de documento y zonas geogr\u00e1ficas. Estos datos permitieron analizar el comportamiento hist\u00f3rico de afiliaci\u00f3n, identificar periodos de ingreso y retiro, y consolidar informaci\u00f3n demogr\u00e1fica clave de las empresas afiliadas.</p> <p>La salida final, <code>BD_Fact_Historico_Afiliacion_Empresas</code>, almacena estos datos, permitiendo analizar el estado de las empresas afiliadas en diferentes periodos, el n\u00famero de empleados y la distribuci\u00f3n por g\u00e9nero. El proceso incluy\u00f3 la limpieza de datos, la estandarizaci\u00f3n de nombres de columnas, y la creaci\u00f3n de identificadores \u00fanicos para los registros de empresas y sus empleados.</p>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\ntitle Diagrama de Secuencia del Proceso ETL para Afiliaci\u00f3n de Empresas\nautonumber\nparticipant \ud83d\udc64 Usuario\nparticipant Script as Script ETL\nparticipant DB as Bases de Datos\nparticipant DWH as Data Warehouse\n\nUsuario-&gt;&gt;Script: Solicitar ETL de Hist\u00f3rico de Afiliaci\u00f3n\nScript-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c085, op_EmpresasSubsi02_v2\nDB--&gt;&gt;Script: Retornar datos extra\u00eddos\nScript-&gt;&gt;Script: Limpiar, estandarizar y transformar los datos\nScript-&gt;&gt;DWH: Cargar datos en BD_Fact_Historico_Afiliacion_Empresas\nDWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\nScript--&gt;&gt;Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama muestra las diferentes etapas del proceso ETL, desde la extracci\u00f3n de los datos de las bases hasta la consolidaci\u00f3n de la informaci\u00f3n en el DWH, lo cual facilita el an\u00e1lisis del comportamiento de afiliaci\u00f3n de las empresas y sus empleados.</p>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#etl","title":"ETL","text":""},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script facilita el manejo y la transformaci\u00f3n de datos en una base de datos usando SQLAlchemy y Pandas, complementado con funciones personalizadas que optimizan el flujo de trabajo. Con SQLAlchemy, se gestiona la conexi\u00f3n a bases de datos mediante <code>create_engine</code>, y Pandas permite la manipulaci\u00f3n de datos en formato tabular, incluyendo transformaciones y almacenamiento. La biblioteca <code>time</code> mide la duraci\u00f3n de la ejecuci\u00f3n para monitoreo de rendimiento, mientras que <code>logging</code> permite la captura de eventos importantes y posibles errores. Desde el m\u00f3dulo <code>Funciones.py</code>, se importan funciones clave como <code>convertir_columnas_mayusculas</code> (posiblemente para estandarizar los nombres de las columnas), <code>guardar_en_dwh</code> (para almacenar datos en un Data Warehouse), <code>cargar_tablas</code> (para cargar datos desde diversas fuentes), <code>obtener_conexion</code> (para establecer una conexi\u00f3n), <code>testfunciones</code> (posiblemente para pruebas), y <code>setup_logger</code> (para configurar el sistema de logging). Este enfoque modular asegura un procesamiento de datos flexible y escalable para flujos ETL o an\u00e1lisis.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Empresas.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Empresas.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 11:48:51,409 - INFO - Importacion de funciones correcta, 30-10-2024 11:48\n2024-10-30 11:48:51,411 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#consulta-sql-para-consolidacion-de-datos-de-empresas-y-trabajadores","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Empresas y Trabajadores","text":"<p>Esta consulta SQL extrae y combina informaci\u00f3n de empresas y sus trabajadores a partir de m\u00faltiples tablas en la base <code>xml4</code>. La consulta principal (<code>p1</code>) selecciona datos como <code>NIT</code>, <code>PERIODO</code>, <code>TIPIDE</code>, <code>ESTADO</code>, <code>CODZON</code>, <code>NOMBRE_EMPRESA</code>, y <code>CODCIU</code>, entre otros. La subconsulta interna (<code>p</code>) extrae informaci\u00f3n relacionada con el per\u00edodo de afiliaci\u00f3n y retiro (<code>perafi</code>, <code>perret</code>) y atributos de la empresa (<code>tipemp</code>, <code>tipsoc</code>, <code>nomcom</code>). La subconsulta secundaria (<code>p2</code>) agrega datos sobre el n\u00famero de trabajadores (<code>NumTrab</code>) y el desglose por g\u00e9nero (<code>NumFem</code>, <code>NumMas</code>), permitiendo una vista completa de las caracter\u00edsticas y composici\u00f3n de los trabajadores en cada per\u00edodo.</p> <p>La consulta final une <code>p1</code> con <code>p2</code> en <code>nit</code> y <code>periodo</code>, creando un conjunto de datos consolidado que permite el an\u00e1lisis detallado de empresas y sus trabajadores en per\u00edodos espec\u00edficos, con un enfoque en la segmentaci\u00f3n por g\u00e9nero y afiliaci\u00f3n.</p> <pre><code>qr_structure = {'query': '''\n    SELECT\n        p1.nit, p1.periodo, p1.tipide, p1.tipper, p1.estado, p1.divpol as codzon, p1.cod,\n        p1.nombre as calemp, p1.tipapo, p1.perafi, p1.perret, p2.NumTrab, p1.codciu, \n        p2.NumFem, p2.NumMas, p1.tipemp, p1.tipsoc, p1.nomcom, p1.razsoc, p1.codact, \n        p1.fecsis, p1.persis, p1.codest\n    FROM (\n        SELECT\n            p.nit, p.periodo, p.tipide, p.cod, p.tipper, p.estado, p.divpol,\n            p.nombre, p.tipapo, p.codciu, p.tipemp, p.tipsoc, p.nomcom, p.razsoc, \n            p.codact, p.perafi, p.perret, p.fecsis, p.persis, p.codest\n        FROM (\n            SELECT\n                b.nit, b.periodo, b.tipide, b.cod, b.tipper, b.estado, b.divpol,\n                c.perafi, a.nombre, b.tipapo, b.fecret as perret, s.codciu, \n                d.nombre as tipemp, s.tipsoc, s.nomcom, b.razsoc, b.codact, \n                s.fecsis, s.persis, s.codest\n            FROM (\n                SELECT\n                    nit, tipide, CONCAT(tipide, nit) as cod,\n                    periodo, estado, divpol, IF(tipide=7, 'J', 'N') as tipper,\n                    tipapo, razsoc, codact, tipsec,\n                    IF(estado = 2 OR estado = 4, periodo, NULL) as fecret\n                FROM xml4.xml4c085\n                #WHERE nit='891780093'\n\n                GROUP BY cod, periodo\n            ) as b\n            LEFT JOIN xml4.xml4b072 as a ON b.tipapo = a.tipapo\n            LEFT JOIN xml4.xml4b001 as d ON b.tipsec = d.tipsec\n            LEFT JOIN (\n                SELECT \n                    CONCAT(\n                        CAST(\n                            CASE\n                                WHEN coddoc = 'CC' THEN 1\n                                WHEN coddoc = 'CE' THEN 4\n                                WHEN coddoc = 'NI' THEN 7\n                                WHEN coddoc = 'PA' THEN 6\n                                WHEN coddoc = 'RC' THEN 3\n                                ELSE coddoc\n                            END AS CHAR\n                        ),\n                        nit\n                    ) AS cod, codciu, tipsoc, nomcom, fecsis, \n                    CONCAT(SUBSTRING(fecsis, 1, 4), SUBSTRING(fecsis, 6, 2)) as persis, \n                    codest\n                FROM subsidio.subsi02\n                GROUP BY cod\n            ) as s ON b.cod = s.cod\n            LEFT JOIN (\n                SELECT CONCAT(tipide, nit) as cod, MIN(periodo) as perafi \n                FROM xml4.xml4c085 \n                WHERE estado=1 \n                GROUP BY CONCAT(tipide, nit)\n            ) as c ON b.cod = c.cod\n        ) as p\n        WHERE  \n            (\n                CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n                AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n            ) OR (\n                CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n                AND perret IS NULL\n            )\n    ) as p1\n    LEFT JOIN (\n        SELECT \n            periodo, nit, COUNT(DISTINCT(CONCAT(coddoc, cedtra))) as NumTrab, \n            SUM(sexfem) as NumFem, SUM(sexmas) as NumMas\n        FROM (\n            SELECT \n                periodo, nit, coddoc, cedtra, \n                IF(tipgen = 2, 1, 0) as sexfem, \n                IF(tipgen = 1, 1, 0) as sexmas \n            FROM xml4.xml4c086 \n            GROUP BY coddoc, cedtra, nit, periodo\n        ) as tra\n        GROUP BY nit, periodo\n    ) as p2 ON p1.nit = p2.nit AND p1.periodo = p2.periodo;\n'''}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:48:51,419 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#consulta-sql-para-consolidacion-de-datos-de-empresas-con-segmentacion-por-periodo","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Empresas con Segmentaci\u00f3n por Periodo","text":"<p>Esta consulta en <code>qr_structure2</code> selecciona y agrupa informaci\u00f3n de empresas en diferentes per\u00edodos, consolidando datos de estado de afiliaci\u00f3n y caracter\u00edsticas adicionales. La consulta selecciona campos como <code>periodo</code>, <code>tipide</code>, <code>tipper</code>, <code>codzon</code>, <code>codest</code>, y <code>estado</code>, adem\u00e1s de columnas derivadas como <code>seRetiro</code>, <code>seAfilio</code>, y <code>nuevo</code> que indican cambios de afiliaci\u00f3n por per\u00edodo.</p> <p>La subconsulta (<code>hh</code>) filtra los registros en un rango de 18 meses desde la fecha actual, generando indicadores sobre si la empresa se retir\u00f3 o afili\u00f3 en un per\u00edodo espec\u00edfico. La consulta final agrupa estos datos por <code>periodo</code>, <code>Tipo</code>, <code>cod</code>, y <code>estado</code>, creando una estructura resumida para an\u00e1lisis y segmentaci\u00f3n de empresas en funci\u00f3n de su estado y afiliaci\u00f3n en el Data Warehouse.</p> <pre><code>qr_structure2 = {'query':'''\n    SELECT\n        periodo, nit as id, tipide, tipper, MAX(codzon) as codzon,\n        MAX(codest) as codest, Tipo, cod, estado,\n        MAX(seRetiro) as seRetiro, MAX(seAfilio) as seAfilio,  MAX(nuevo) as nuevo,\n        MAX(calsuc) as calsuc, MAX(tipapo) as tipapo,\n        MAX(NumTrab), MAX(codciu) as codciu, MAX(tipemp) as tipemp , MAX(tipsoc) as tipsoc , \n        MAX(nomcom) as nomcom, MAX(razsoc) as razsoc,\n        MAX(NumFem) as NumFem, MAX(NumMas) as NumMas, MAX(codact) as codact, MIN(fecsis) as fecsis\n    FROM (\n        SELECT nit, tipide, tipper, estado, periodo, codzon, codest, cod, 'Empresa' as Tipo,\n            calemp as calsuc, tipapo, fecsis, perafi, persis, NumTrab, codciu, NumFem, NumMas, \n            tipemp, tipsoc, nomcom, razsoc, codact,\n            IF(perret = periodo, 1, 0) as seRetiro,\n            IF(perafi = periodo, 1, 0) as seAfilio,\n            IF(periodo = persis, 1, 0) as nuevo\n        FROM dwh.op_EmpresasSubsi02_v2 as p\n        WHERE\n            CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n            AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n    ) as hh\n    GROUP BY periodo, Tipo, cod, estado;\n'''}\n\ndf_structure2 = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:48:51,428 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#conexion-a-bases-de-datos-y-carga-de-tablas","title":"Conexi\u00f3n a Bases de Datos y Carga de Tablas","text":"<p>Este c\u00f3digo establece conexiones con dos bases de datos distintas, Minerva y DWH, usando SQLAlchemy para manejar las conexiones y un <code>logger</code> para registrar eventos importantes, como el inicio de las conexiones. Primero, se conecta a la base Minerva utilizando <code>obtener_conexion</code>, y luego carga las consultas definidas en <code>qr_structure</code> en el DataFrame <code>df_structure</code> mediante la funci\u00f3n <code>cargar_tablas</code>. Posteriormente, se realiza una conexi\u00f3n similar con la base de datos DWH, donde se ejecutan las consultas definidas en <code>qr_structure2</code> para cargar los datos en <code>df_structure2</code>. Este proceso asegura que los datos de ambas bases se procesen correctamente y permite el registro eficiente de eventos durante las operaciones.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n\n#Conexion a base Minerva\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor2, qr_structure2, df_structure2, logger)\n</code></pre> <pre><code>2024-10-30 11:48:51,616 - INFO - CONEXION A BASE MINERVA\n2024-10-30 11:48:51,966 - INFO - Cargando query \n2024-10-30 11:49:49,195 - INFO - Cargada query, 565,159 registros finales obtenidos. --- 57.23 seconds ---\n2024-10-30 11:49:49,323 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 57.70 seconds ---\n2024-10-30 11:49:49,325 - INFO - CONEXION A BASE DWH\n2024-10-30 11:49:49,641 - INFO - Cargando query \n2024-10-30 11:50:02,441 - INFO - Cargada query, 199,034 registros finales obtenidos. --- 12.80 seconds ---\n2024-10-30 11:50:02,533 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 13.21 seconds ---\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#estandarizacion-y-creacion-de-identificadores-unicos-en-datos-de-empresas","title":"Estandarizaci\u00f3n y Creaci\u00f3n de Identificadores \u00danicos en Datos de Empresas","text":"<p>Este c\u00f3digo realiza la estandarizaci\u00f3n de nombres de columnas y la creaci\u00f3n de un identificador \u00fanico <code>ID_AFILIADO</code> en el DataFrame <code>op_sec_EmpresasSubsi02_v2</code>. Primero, utiliza un diccionario (<code>_names</code>) para renombrar las columnas clave con <code>convertir_columnas_mayusculas</code>, asegurando consistencia en los nombres y formatos.</p> <p>Luego, define una serie de condiciones (<code>conditions</code>) y valores (<code>choices</code>) para crear el prefijo de documento basado en el c\u00f3digo <code>CODIGO_DOCUMENTO</code>. La funci\u00f3n <code>np.select</code> aplica esta l\u00f3gica de selecci\u00f3n m\u00faltiple, asignando a cada tipo de documento su abreviatura correspondiente (por ejemplo, <code>CC</code> para c\u00e9dula de ciudadan\u00eda, <code>TI</code> para tarjeta de identidad). El resultado, <code>DOC_PREFIX</code>, se concatena con <code>ID</code> para formar el campo <code>ID_AFILIADO</code>, eliminando posteriormente la columna auxiliar <code>DOC_PREFIX</code>.</p> <p>Por \u00faltimo, <code>op_sec_EmpresasSubsi02_v2</code> se reorganiza para incluir columnas seleccionadas, y <code>print(df_standar['PERIODO'].unique())</code> muestra los per\u00edodos \u00fanicos en el DataFrame, permitiendo validar la consistencia de los datos a trav\u00e9s del tiempo.</p> <pre><code>op_EmpresasSubsi02_v2 = df_structure['query']\nop_sec_EmpresasSubsi02_v2 = df_structure2['query']\nop_sec_EmpresasSubsi02_v2\n\n_names = {\n    'tipide': 'CODIGO_DOCUMENTO', \n    'tipper': 'TIPO_PErSONA',  \n    'codzon': 'Codigo_Zona',             \n    'codest': 'COD_EST_INAC',           \n    'seRetiro': 'se_Retiro',             \n    'seAfilio': 'se_Afilio',       \n    'calsuc': 'Calificacion_Sucursal', \n    'codciu': 'Codigo_Ciudad',   \n    'fecsis': 'Fecha_Sistema', \n    'MAX(NumTrab)':  'NUM_TRAB',\n    'estado' : 'COD_EST_AFIL'\n}\n\ndf_standar = convertir_columnas_mayusculas(op_sec_EmpresasSubsi02_v2,_names)\nimport numpy as np\n\n# Definir las condiciones y los valores para la l\u00f3gica CASE\nconditions = [\n    df_standar['CODIGO_DOCUMENTO'] == 1,\n    df_standar['CODIGO_DOCUMENTO'] == 2,\n    df_standar['CODIGO_DOCUMENTO'] == 3,\n    df_standar['CODIGO_DOCUMENTO'] == 4,\n    df_standar['CODIGO_DOCUMENTO'] == 5,\n    df_standar['CODIGO_DOCUMENTO'] == 6,\n    df_standar['CODIGO_DOCUMENTO'] == 7,\n    df_standar['CODIGO_DOCUMENTO'] == 8,\n    df_standar['CODIGO_DOCUMENTO'] == 9,\n    df_standar['CODIGO_DOCUMENTO'] == 15\n]\n\n# Valores correspondientes a cada condici\u00f3n\nchoices = ['CC', 'TI', 'RC', 'CE', 'NP', 'PA', 'NI', 'CD', 'PE', 'PT']\n\n# Aplicar las condiciones con np.select y crear el prefijo correspondiente\ndf_standar['DOC_PREFIX'] = np.select(conditions, choices, default=df_standar['CODIGO_DOCUMENTO'].astype(str))\n\n# Concatenar el prefijo y cedtra para crear la columna ID_AFILIADO\ndf_standar['ID_AFILIADO'] = df_standar['DOC_PREFIX'] + df_standar['ID'].astype(str)\n\n# Eliminar la columna auxiliar DOC_PREFIX si no se necesita\ndf_standar = df_standar.drop(columns=['DOC_PREFIX'])\n\nop_sec_EmpresasSubsi02_v2 = df_standar[[\n    'PERIODO',\n    'ID_AFILIADO',\n    'ID',\n    'CODIGO_DOCUMENTO',\n    'TIPO_PERSONA',\n    'CODIGO_ZONA',\n    'COD_EST_INAC',\n    'TIPO',\n    'COD',\n    'COD_EST_AFIL',\n    'SE_RETIRO',\n    'SE_AFILIO',\n    'NUEVO',\n    'CALIFICACION_SUCURSAL',\n    'TIPAPO',\n    'NUM_TRAB',\n    'CODIGO_CIUDAD',\n    'TIPEMP',\n    'TIPSOC',\n    'NOMCOM',\n    'RAZSOC',\n    'NUMFEM',\n    'NUMMAS',\n    'CODACT',\n    'FECHA_SISTEMA'\n    ]]\n\nprint(df_standar['PERIODO'].unique())\n</code></pre> <pre><code>['202310' '202311' '202312' '202401' '202402' '202403' '202404' '202309'\n '202407' '202408' '202409' '202304' '202305' '202306' '202307' '202308'\n '202405' '202406']\n</code></pre>"},{"location":"seccion/1.6-Fact_Historico_Afiliacion_Empresas/#almacenamiento-de-datos-en-el-data-warehouse","title":"Almacenamiento de Datos en el Data Warehouse","text":"<p>En esta \u00faltima etapa, el c\u00f3digo utiliza la funci\u00f3n <code>guardar_en_dwh</code> para almacenar los DataFrames <code>op_EmpresasSubsi02_v2</code> y <code>op_sec_EmpresasSubsi02_v2</code> en el Data Warehouse. La primera llamada guarda <code>op_EmpresasSubsi02_v2</code> en una tabla con el mismo nombre en el DWH, y la segunda llamada almacena <code>op_sec_EmpresasSubsi02_v2</code> en la tabla <code>BD_Fact_Historico_Afiliacion_Empresas</code>. Los par\u00e1metros <code>multiple=False</code> y <code>if_exists='replace'</code> indican que las tablas se reemplazar\u00e1n si ya existen y que no se manejar\u00e1n datos en m\u00faltiples bloques, optimizando el proceso de inserci\u00f3n. Esta operaci\u00f3n asegura que la informaci\u00f3n est\u00e9 disponible y actualizada para futuras consultas o an\u00e1lisis.</p> <pre><code>guardar_en_dwh(op_EmpresasSubsi02_v2, 'op_EmpresasSubsi02_v2', logger, multiple=False, if_exists='replace')\nguardar_en_dwh(op_sec_EmpresasSubsi02_v2, 'BD_Fact_Historico_Afiliacion_Empresas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 11:50:03,089 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:50:03,091 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:50:03,388 - INFO - Almacenando tabla \u00fanica en DWH como op_EmpresasSubsi02_v2\n2024-10-30 11:51:50,695 - INFO - Tabla almacenada correctamente. 565,159 registros finales obtenidos.\n2024-10-30 11:51:51,500 - INFO - ALMACENAMIENTO ---  --- 1.81 minutes ---\n2024-10-30 11:51:51,501 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-30 11:51:51,503 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:51:51,506 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:51:51,839 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Historico_Afiliacion_Empresas\n2024-10-30 11:52:26,714 - INFO - Tabla almacenada correctamente. 199,034 registros finales obtenidos.\n2024-10-30 11:52:26,925 - INFO - ALMACENAMIENTO ---  --- 35.42 seconds ---\n2024-10-30 11:52:26,927 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>op_sec_EmpresasSubsi02_v2['PERIODO'].unique()\n</code></pre> <pre><code>array(['202310', '202311', '202312', '202401', '202402', '202403',\n       '202404', '202309', '202407', '202408', '202409', '202304',\n       '202305', '202306', '202307', '202308', '202405', '202406'],\n      dtype=object)\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 11:52:26,991 - INFO - FINAL ETL --- 215.59 seconds ---\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/","title":"1.7 Fact Historico Trabajadores 1","text":""},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#introduccion","title":"Introducci\u00f3n","text":"<p>En este proceso ETL se extraen y transforman datos relacionados con trabajadores para almacenarlos en tablas de hechos en el Data Warehouse (DWH). Las consultas SQL se ejecutan sobre tablas de origen, incluyendo <code>xml4c086</code>, <code>subsi15</code>, y otras fuentes, proporcionando informaci\u00f3n detallada sobre los trabajadores, como identificadores \u00fanicos, documentos, estados civiles, per\u00edodos de afiliaci\u00f3n y retiro, y detalles sobre categorizaci\u00f3n y afiliaci\u00f3n.</p> <p>La salida final, <code>BD_Fact_op_Trabajadores_p1</code> y <code>BD_Fact_op_Trabajadores_p2</code>, contiene informaci\u00f3n consolidada de los trabajadores y sus detalles de afiliaci\u00f3n, permitiendo un an\u00e1lisis detallado de la historia laboral de cada afiliado en la base de datos del DWH. Este proceso incluye la estandarizaci\u00f3n de los nombres de columnas, generaci\u00f3n de identificadores \u00fanicos y la carga final de los datos en el DWH.</p>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL para Trabajadores\n    autonumber\n    participant \ud83d\udc64 Usuario\n    participant Script as Script ETL\n    participant DB as Bases de Datos\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar ETL de Trabajadores\n    Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c086, subsi15\n    DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n    Script-&gt;&gt;Script: Limpiar, estandarizar y transformar los datos\n    Script-&gt;&gt;DWH: Cargar datos en BD_Fact_op_Trabajadores_p1 y BD_Fact_op_Trabajadores_p2\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n    Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama describe el flujo del proceso, destacando las fases de limpieza, estandarizaci\u00f3n y carga en el DWH, asegurando que los datos est\u00e9n preparados para su an\u00e1lisis en las tablas <code>BD_Fact_op_Trabajadores_p1</code> y <code>BD_Fact_op_Trabajadores_p2</code>.</p>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#etl","title":"ETL","text":""},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script configura el entorno para cargar y transformar datos utilizando <code>SQLAlchemy</code>, <code>Pandas</code>, y funciones personalizadas definidas en un archivo externo (<code>Funciones.py</code>). La biblioteca <code>SQLAlchemy</code> permite crear conexiones y manejar transacciones en bases de datos relacionales mediante el uso de <code>create_engine</code>, mientras que <code>Pandas</code> se emplea para la manipulaci\u00f3n de datos en formato tabular, ideal para tareas de carga y transformaci\u00f3n. Adem\u00e1s, la biblioteca <code>logging</code> permite configurar un sistema de registros para monitorear el estado y los errores durante la ejecuci\u00f3n. </p> <p>El script extiende el <code>sys.path</code> para que Python incluya el directorio de las funciones personalizadas, lo que permite importar funciones esenciales como <code>convertir_columnas_mayusculas</code> (para estandarizar nombres de columnas), <code>guardar_en_dwh</code> (para almacenar datos en el Data Warehouse), <code>cargar_tablas</code> (para cargar tablas desde distintas fuentes), <code>obtener_conexion</code> (para establecer una conexi\u00f3n de base de datos), <code>testfunciones</code> (para realizar pruebas), y <code>setup_logger</code> (para configurar los registros). Con esta configuraci\u00f3n modular, el script se adapta a flujos de procesamiento de datos din\u00e1micos y escalables, id\u00f3neos para operaciones de ETL o an\u00e1lisis de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>nombre.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Trabajadores.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-28 19:02:25,499 - INFO - Importacion de funciones correcta, 28-10-2024 19:02\n2024-10-28 19:02:25,501 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#definicion-y-ejecucion-de-consultas-sql-para-el-analisis-de-trabajadores","title":"Definici\u00f3n y Ejecuci\u00f3n de Consultas SQL para el An\u00e1lisis de Trabajadores","text":"<p>Este bloque de c\u00f3digo define y estructura un conjunto de consultas SQL en el diccionario <code>qr_structure</code>, permitiendo la extracci\u00f3n de datos de diversas fuentes para un an\u00e1lisis detallado de trabajadores. La consulta <code>op_Trabajadores_p1</code> obtiene informaci\u00f3n detallada sobre los trabajadores, incluyendo su tipo de documento (<code>tipide</code>), identificaci\u00f3n (<code>id</code>), periodo de afiliaci\u00f3n y retiro (<code>perafi</code>, <code>perret</code>), estado civil (<code>estciv</code>), y otros detalles de afiliaci\u00f3n y categorizaci\u00f3n (<code>codcat</code>, <code>divpol</code>, <code>nivedu</code>, entre otros). Los datos se filtran por un rango de fechas espec\u00edfico y se agrupan mediante <code>cod</code>, lo que asegura que cada trabajador tenga una entrada \u00fanica por per\u00edodo.</p> <p>La consulta <code>op_Trabajadores_p2</code> transforma y concatena los tipos de documento (<code>coddoc</code>) y las c\u00e9dulas (<code>cedtra</code>) para estandarizar el c\u00f3digo de trabajador (<code>cod</code>). La estructura <code>dim_names</code> asigna nombres descriptivos a cada consulta, facilitando su posterior almacenamiento en un Data Warehouse bajo identificadores significativos (<code>BD_Fact_op_Trabajadores_p1</code> y <code>BD_Fact_op_Trabajadores_p2</code>). Finalmente, el diccionario <code>df_structure</code> prepara los DataFrames de salida y un registro en el logger indica la lectura de las consultas, permitiendo monitorear la ejecuci\u00f3n y garantizar la trazabilidad de las operaciones.</p> <pre><code># Lista de consultas actualizada con rango de 18 periodos desde la fecha actual\nqr_structure = {\n    \"op_Trabajadores_p1\": \"\"\"\n        SELECT * FROM\n        (select p1.*, p2.perret, p3.perafi,\n        p4.nombre as calsuc from\n        (Select coddoc as tipide, cedtra as id, CONCAT( coddoc , cedtra ) as cod, periodo, 'A' as estado, \n                codcat, divpol as codzon, tipafi, tipgen as sexo, estciv, graesc as nivedu, \n                fecnac, codare as rural  \n            from xml4.xml4c086\n            GROUP BY CONCAT( coddoc , cedtra ), periodo) as p1\n\n        LEFT JOIN (select CONCAT( coddoc , cedtra ) as cod,\n        IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c086), MAX(periodo), NULL) as perret \n        from xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) as p2\n        ON p1.cod = p2.cod \n\n        LEFT JOIN (select CONCAT( coddoc , cedtra ) as cod,\n        MIN(periodo) as perafi from xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) as p3\n        ON p1.cod = p3.cod\n\n        LEFT JOIN xml4.xml4b006 as p4\n        ON p1.tipafi = p4.tipafi\n\n        WHERE (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01'))\n        OR \n        (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n        AND perret IS NULL)) as m\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n            AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01');\n    \"\"\",\n\n    \"op_Trabajadores_p2\": '''\n        SELECT CONCAT(\n            CAST(\n                CASE \n                    WHEN coddoc = 'CC' THEN 1 \n                    WHEN coddoc = 'CD' THEN 8\n                    WHEN coddoc = 'CE' THEN 4 \n                    WHEN coddoc = 'PE' THEN 9            \n                    WHEN coddoc = 'PA' THEN 6 \n                    WHEN coddoc = 'PT' THEN 15 \n                    WHEN coddoc = 'TI' THEN 2\n                    ELSE coddoc \n                END AS CHAR\n            ),\n            cedtra\n        ) AS cod, codciu, codest, fecsis, CONCAT(SUBSTRING(fecsis, 1, 4), SUBSTRING(fecsis, 6, 2)) as persis \n        from subsidio.subsi15 \n        WHERE DATE_FORMAT(CURDATE(), '%Y%m') &gt;= DATE_FORMAT(fecsis, '%Y%m') \n            AND DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m') &lt;= DATE_FORMAT(fecsis, '%Y%m')\n        group by cod\n    '''\n}\n\ndim_names = {\n    \"op_Trabajadores_p1\": \"BD_Fact_op_Trabajadores_p1\",\n    \"op_Trabajadores_p2\": \"BD_Fact_op_Trabajadores_p2\"\n}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure)\n</code></pre> <pre><code>2024-10-28 19:02:25,517 - INFO - LECTURA DE QUERYS\n\n\n{'op_Trabajadores_p1': \"\\n        SELECT * FROM\\n        (select p1.*, p2.perret, p3.perafi,\\n        p4.nombre as calsuc from\\n        (Select coddoc as tipide, cedtra as id, CONCAT( coddoc , cedtra ) as cod, periodo, 'A' as estado, \\n                codcat, divpol as codzon, tipafi, tipgen as sexo, estciv, graesc as nivedu, \\n                fecnac, codare as rural  \\n            from xml4.xml4c086\\n            GROUP BY CONCAT( coddoc , cedtra ), periodo) as p1\\n\\n        LEFT JOIN (select CONCAT( coddoc , cedtra ) as cod,\\n        IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c086), MAX(periodo), NULL) as perret \\n        from xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) as p2\\n        ON p1.cod = p2.cod \\n\\n        LEFT JOIN (select CONCAT( coddoc , cedtra ) as cod,\\n        MIN(periodo) as perafi from xml4.xml4c086 GROUP BY CONCAT( coddoc , cedtra )) as p3\\n        ON p1.cod = p3.cod\\n\\n        LEFT JOIN xml4.xml4b006 as p4\\n        ON p1.tipafi = p4.tipafi\\n\\n        WHERE (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01'))\\n        OR \\n        (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\\n        AND perret IS NULL)) as m\\n        WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\\n            AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01');\\n    \", 'op_Trabajadores_p2': \"\\n        SELECT CONCAT(\\n            CAST(\\n                CASE \\n                    WHEN coddoc = 'CC' THEN 1 \\n                    WHEN coddoc = 'CD' THEN 8\\n                    WHEN coddoc = 'CE' THEN 4 \\n                    WHEN coddoc = 'PE' THEN 9            \\n                    WHEN coddoc = 'PA' THEN 6 \\n                    WHEN coddoc = 'PT' THEN 15 \\n                    WHEN coddoc = 'TI' THEN 2\\n                    ELSE coddoc \\n                END AS CHAR\\n            ),\\n            cedtra\\n        ) AS cod, codciu, codest, fecsis, CONCAT(SUBSTRING(fecsis, 1, 4), SUBSTRING(fecsis, 6, 2)) as persis \\n        from subsidio.subsi15 \\n        WHERE DATE_FORMAT(CURDATE(), '%Y%m') &gt;= DATE_FORMAT(fecsis, '%Y%m') \\n            AND DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m') &lt;= DATE_FORMAT(fecsis, '%Y%m')\\n        group by cod\\n    \"}\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#conexion-a-base-de-datos-y-carga-de-tablas-en-dataframes","title":"Conexi\u00f3n a Base de Datos y Carga de Tablas en DataFrames","text":"<p>Este fragmento de c\u00f3digo establece una conexi\u00f3n a la base de datos Minerva utilizando SQLAlchemy y la funci\u00f3n <code>obtener_conexion</code>, que genera una cadena de conexi\u00f3n basada en los par\u00e1metros de esta base. La conexi\u00f3n, representada por <code>motor</code>, facilita el acceso a la base y permite realizar consultas. Tras establecerse, el logger registra la conexi\u00f3n exitosa a la base de datos.</p> <p>Luego, se invoca <code>cargar_tablas</code>, funci\u00f3n encargada de ejecutar cada consulta en <code>qr_structure</code> sobre <code>motor</code> y cargar los resultados en el diccionario <code>df_structure</code>. As\u00ed, cada consulta almacena su respectivo DataFrame en <code>df_structure</code>, permitiendo procesar los datos en memoria y prepararlos para an\u00e1lisis o almacenamiento posterior. Este proceso modular optimiza la administraci\u00f3n y escalabilidad del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-28 19:02:25,531 - INFO - CONEXION A BASE MINERVA\n2024-10-28 19:02:26,062 - INFO - Cargando op_Trabajadores_p1 \n2024-10-28 19:04:32,936 - INFO - Cargada op_Trabajadores_p1, 2,456,632 registros finales obtenidos. --- 2.11 minutes ---\n2024-10-28 19:04:32,937 - INFO - Cargando op_Trabajadores_p2 \n2024-10-28 19:04:35,439 - INFO - Cargada op_Trabajadores_p2, 36,359 registros finales obtenidos. --- 2.50 seconds ---\n2024-10-28 19:04:35,528 - INFO - CARGUE TABLAS DESDE MYSQL --- op_Trabajadores_p2 --- 2.17 minutes ---\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#estandarizacion-y-generacion-de-identificadores-en-dataframes-de-trabajadores","title":"Estandarizaci\u00f3n y Generaci\u00f3n de Identificadores en DataFrames de Trabajadores","text":"<p>Este c\u00f3digo toma los resultados de las consultas <code>op_Trabajadores_p1</code> y <code>op_Trabajadores_p2</code> almacenados en <code>df_structure</code>, estandariza los nombres de sus columnas y genera identificadores \u00fanicos para cada trabajador.</p> <p>Para la consulta <code>op_Trabajadores_p1</code>, se emplea el diccionario <code>_names</code> en la funci\u00f3n <code>convertir_columnas_mayusculas</code> para renombrar las columnas y asegurar consistencia en el esquema. Mediante <code>np.select</code>, se aplica una l\u00f3gica de caso sobre la columna <code>CODIGO_DOCUMENTO</code> que asigna un prefijo (<code>DOC_PREFIX</code>) seg\u00fan el tipo de documento del trabajador. Este prefijo se concatena con la columna <code>ID</code> para crear la columna <code>ID_AFILIADO</code>, un identificador \u00fanico. Finalmente, se elimina la columna auxiliar <code>DOC_PREFIX</code>, y el DataFrame estandarizado se guarda como <code>df_BD_Fact_op_Trabajadores_p1</code>.</p> <p>Para la consulta <code>op_Trabajadores_p2</code>, el mismo diccionario <code>_names</code> permite estandarizar las columnas. La estructura resultante se guarda como <code>df_BD_Fact_op_Trabajadores_p2</code>. La conversi\u00f3n garantiza que ambos DataFrames compartan un esquema coherente para su posterior an\u00e1lisis o almacenamiento en el Data Warehouse.</p> <pre><code>df = df_structure['op_Trabajadores_p1']\n\n_names = {\n    'tipide': 'CODIGO_DOCUMENTO',                  # No coincide en _names, se mantiene original\n    'tipper': 'TIPO_PErSONA',                  # No coincide en _names, se mantiene original\n    'codzon': 'Codigo_Zona',             # Coincide en _names, se renombra\n    'codest': 'Codigo_Estado',           # Coincide en _names, se renombra\n    'seRetiro': 'se_Retiro',              # No coincide en _names, se mantiene original\n    'seAfilio': 'se_Afilio',              # No coincide en _names, se mantiene original\n    'calsuc': 'Calificacion_Sucursal',   # Coincide en _names, se renombra\n    'codciu': 'Codigo_Ciudad',           # Coincide en _names, se renombra\n    'fecsis': 'Fecha_Sistema', # Coincide en _names, se renombra\n    'MAX(NumTrab)':  'NUM_TRAB' # No coincide en _names, se mantiene original\n}\n\ndf_standar = convertir_columnas_mayusculas(df,_names)\n\nimport numpy as np\n\n# Definir las condiciones y los valores para la l\u00f3gica CASE\nconditions = [\n    df_standar['CODIGO_DOCUMENTO'] == 1,\n    df_standar['CODIGO_DOCUMENTO'] == 2,\n    df_standar['CODIGO_DOCUMENTO'] == 3,\n    df_standar['CODIGO_DOCUMENTO'] == 4,\n    df_standar['CODIGO_DOCUMENTO'] == 5,\n    df_standar['CODIGO_DOCUMENTO'] == 6,\n    df_standar['CODIGO_DOCUMENTO'] == 7,\n    df_standar['CODIGO_DOCUMENTO'] == 8,\n    df_standar['CODIGO_DOCUMENTO'] == 9,\n    df_standar['CODIGO_DOCUMENTO'] == 15\n]\n\n# Valores correspondientes a cada condici\u00f3n\nchoices = ['CC', 'TI', 'RC', 'CE', 'NP', 'PA', 'NI', 'CD', 'PE', 'PT']\n\n# Aplicar las condiciones con np.select y crear el prefijo correspondiente\ndf_standar['DOC_PREFIX'] = np.select(conditions, choices, default=df_standar['CODIGO_DOCUMENTO'].astype(str))\n\n# Concatenar el prefijo y cedtra para crear la columna ID_AFILIADO\ndf_standar['ID_AFILIADO'] = df_standar['DOC_PREFIX'] + df_standar['ID'].astype(str)\n\n# Eliminar la columna auxiliar DOC_PREFIX si no se necesita\ndf_BD_Fact_op_Trabajadores_p1 = df_standar.drop(columns=['DOC_PREFIX'])\n\ndf_BD_Fact_op_Trabajadores_p1.columns.tolist()\n</code></pre> <pre><code>['CODIGO_DOCUMENTO',\n 'ID',\n 'COD',\n 'PERIODO',\n 'ESTADO',\n 'CODCAT',\n 'CODIGO_ZONA',\n 'TIPAFI',\n 'SEXO',\n 'ESTCIV',\n 'NIVEDU',\n 'FECNAC',\n 'RURAL',\n 'PERRET',\n 'PERAFI',\n 'CALIFICACION_SUCURSAL',\n 'ID_AFILIADO']\n</code></pre> <pre><code>df = df_structure['op_Trabajadores_p2']\n\n_names = {\n    'tipide': 'CODIGO_DOCUMENTO',                  # No coincide en _names, se mantiene original\n    'tipper': 'TIPO_PErSONA',                  # No coincide en _names, se mantiene original\n    'codzon': 'Codigo_Zona',             # Coincide en _names, se renombra\n    'codest': 'Codigo_Estado',           # Coincide en _names, se renombra\n    'seRetiro': 'se_Retiro',              # No coincide en _names, se mantiene original\n    'seAfilio': 'se_Afilio',              # No coincide en _names, se mantiene original\n    'calsuc': 'Calificacion_Sucursal',   # Coincide en _names, se renombra\n    'codciu': 'Codigo_Ciudad',           # Coincide en _names, se renombra\n    'fecsis': 'Fecha_Sistema', # Coincide en _names, se renombra\n    'MAX(NumTrab)':  'NUM_TRAB' # No coincide en _names, se mantiene original\n}\n\ndf_BD_Fact_op_Trabajadores_p2 = convertir_columnas_mayusculas(df,_names)\n\ndf_BD_Fact_op_Trabajadores_p2.columns.tolist()\n</code></pre> <pre><code>['COD', 'CODIGO_CIUDAD', 'CODIGO_ESTADO', 'FECHA_SISTEMA', 'PERSIS']\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores1/#almacenamiento-de-dataframes-de-trabajadores-en-el-data-warehouse","title":"Almacenamiento de DataFrames de Trabajadores en el Data Warehouse","text":"<p>Estas l\u00edneas de c\u00f3digo utilizan la funci\u00f3n <code>guardar_en_dwh</code> para almacenar los DataFrames <code>df_BD_Fact_op_Trabajadores_p1</code> y <code>df_BD_Fact_op_Trabajadores_p2</code> en el Data Warehouse. Cada DataFrame se guarda en una tabla correspondiente (<code>BD_Fact_op_Trabajadores_p1</code> y <code>BD_Fact_op_Trabajadores_p2</code>) y se configura el par\u00e1metro <code>if_exists='replace'</code> para reemplazar cualquier tabla existente con el mismo nombre, asegurando as\u00ed que los datos est\u00e9n actualizados. El par\u00e1metro <code>multiple=False</code> optimiza la inserci\u00f3n al manejar los datos en un solo bloque. Este proceso asegura que la informaci\u00f3n estandarizada y enriquecida est\u00e9 lista para su an\u00e1lisis en el sistema de almacenamiento de datos.</p> <pre><code>guardar_en_dwh(df_BD_Fact_op_Trabajadores_p1, 'BD_Fact_op_Trabajadores_p1', logger, multiple=False, if_exists='replace')\n\nguardar_en_dwh(df_BD_Fact_op_Trabajadores_p2, 'BD_Fact_op_Trabajadores_p2', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-28 19:04:36,999 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-28 19:04:37,001 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-28 19:04:37,546 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_op_Trabajadores_p1\n2024-10-28 19:12:06,086 - INFO - Tabla almacenada correctamente. 2,456,632 registros finales obtenidos.\n2024-10-28 19:12:08,794 - INFO - ALMACENAMIENTO ---  --- 7.53 minutes ---\n2024-10-28 19:12:08,795 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-28 19:12:08,797 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-28 19:12:08,799 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-28 19:12:09,357 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_op_Trabajadores_p2\n2024-10-28 19:12:13,624 - INFO - Tabla almacenada correctamente. 36,359 registros finales obtenidos.\n2024-10-28 19:12:13,735 - INFO - ALMACENAMIENTO ---  --- 4.94 seconds ---\n2024-10-28 19:12:13,736 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\ndf_BD_Fact_op_Trabajadores_p1['PERIODO'].unique()\n</code></pre> <pre><code>2024-10-28 19:13:00,271 - INFO - FINAL ETL --- 634.80 seconds ---\n\n\n\n\n\narray(['202304', '202305', '202306', '202307', '202308', '202309',\n       '202310', '202311', '202312', '202401', '202402', '202403',\n       '202404', '202405', '202406', '202407', '202408', '202409'],\n      dtype=object)\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/","title":"1.7 Fact Historico Trabajadores 2","text":""},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#introduccion","title":"Introducci\u00f3n","text":"<p>En este proceso ETL se extraen y transforman datos relacionados con trabajadores para almacenarlos en una tabla de hechos en el Data Warehouse (DWH). Los datos se obtienen desde la tabla <code>BD_Fact_op_Trabajadores_p1</code> y se transforman para proporcionar informaci\u00f3n consolidada sobre los trabajadores, como identificadores \u00fanicos, documentos, estados civiles, per\u00edodos de afiliaci\u00f3n y retiro, y detalles adicionales relevantes.</p> <p>La salida final, <code>BD_Fact_op_Trabajadores_p3</code>, contiene informaci\u00f3n estandarizada y enriquecida, permitiendo un an\u00e1lisis detallado de la historia laboral de cada afiliado. Este proceso incluye la estandarizaci\u00f3n de los nombres de columnas, la clasificaci\u00f3n de trabajadores seg\u00fan su ubicaci\u00f3n (rural o urbano) y la identificaci\u00f3n de eventos clave como afiliaciones y retiros en el per\u00edodo especificado.</p>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL para Trabajadores\n    autonumber\n    participant \ud83d\udc64 Usuario\n    participant Script as Script ETL\n    participant DB as Bases de Datos\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar ETL de Trabajadores\n    Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; BD_Fact_op_Trabajadores_p1\n    DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n    Script-&gt;&gt;Script: Limpiar, estandarizar y transformar los datos\n    Script-&gt;&gt;DWH: Cargar datos en BD_Fact_op_Trabajadores_p3\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n    Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama describe el flujo del proceso, destacando las fases de limpieza, estandarizaci\u00f3n y carga en el DWH, asegurando que los datos estandarizados est\u00e9n preparados para su an\u00e1lisis en la tabla <code>BD_Fact_op_Trabajadores_p3</code>.</p>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#etl","title":"ETL","text":""},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script inicializa el entorno para cargar, transformar y almacenar datos utilizando <code>SQLAlchemy</code>, <code>Pandas</code> y funciones personalizadas definidas en un m\u00f3dulo externo (<code>Funciones.py</code>). La biblioteca <code>SQLAlchemy</code> facilita la creaci\u00f3n de conexiones y el manejo de transacciones en bases de datos relacionales mediante <code>create_engine</code>, mientras que <code>Pandas</code> permite la manipulaci\u00f3n de datos en formato tabular, ideal para tareas de carga y transformaci\u00f3n. La biblioteca <code>logging</code> permite configurar un sistema de registros, esencial para monitorear el estado y los posibles errores durante la ejecuci\u00f3n. </p> <p>El script extiende el <code>sys.path</code> para incluir el directorio donde se encuentran las funciones personalizadas, lo cual permite importar funciones clave como <code>convertir_columnas_mayusculas</code> (para estandarizar nombres de columnas), <code>guardar_en_dwh</code> (para almacenar datos en un Data Warehouse), <code>cargar_tablas</code> (para cargar datos desde diversas fuentes), <code>obtener_conexion</code> (para establecer una conexi\u00f3n a la base de datos), <code>testfunciones</code> (para pruebas) y <code>setup_logger</code> (para configurar los registros). Esta estructura modular y organizada permite un flujo de trabajo de ETL o an\u00e1lisis de datos que es din\u00e1mico y escalable.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>nombre.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Trabajadores.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 16:29:47,542 - INFO - Importacion de funciones correcta, 26-10-2024 16:29\n2024-10-26 16:29:47,546 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#extraccion-y-transformacion-de-datos-de-trabajadores-en-el-data-warehouse","title":"Extracci\u00f3n y Transformaci\u00f3n de Datos de Trabajadores en el Data Warehouse","text":"<p>Este c\u00f3digo define una consulta SQL (<code>op_Trabajadores_p3</code>) para extraer datos de trabajadores almacenados en la tabla <code>BD_Fact_op_Trabajadores_p1</code> dentro del Data Warehouse (DWH). La consulta selecciona varios campos relevantes, como <code>CODIGO_DOCUMENTO</code>, <code>ID</code>, <code>COD</code>, <code>PERRET</code>, <code>ESTADO</code>, <code>PERIODO</code>, <code>CODCAT</code>, <code>CODIGO_ZONA</code>, <code>CALIFICACION_SUCURSAL</code>, <code>TIPAFI</code>, <code>SEXO</code>, <code>ESTCIV</code>, <code>NIVEDU</code> y <code>FECNAC</code>. Adem\u00e1s, incorpora transformaciones para clasificar a los trabajadores como \"Rural\" o \"Urbano\" seg\u00fan el campo <code>RURAL</code>, y marca si un trabajador se retir\u00f3 (<code>SE_RETIRO</code>) o se afili\u00f3 (<code>SE_AFILIO</code>) en el per\u00edodo especificado.</p> <p>La consulta se ejecuta mediante <code>cargar_tablas</code>, que carga los datos desde <code>qr_structure2</code> en el DataFrame <code>df_structure2</code> utilizando una conexi\u00f3n SQLAlchemy (<code>motor</code>) a la base de datos del DWH. Despu\u00e9s de extraer los datos, el DataFrame se estandariza usando <code>convertir_columnas_mayusculas</code>, lo que asegura que los nombres de las columnas tengan un formato consistente. Finalmente, <code>df_standar.columns.tolist()</code> imprime los nombres de las columnas, permitiendo una r\u00e1pida verificaci\u00f3n de los cambios realizados antes de continuar con el procesamiento o an\u00e1lisis de datos adicionales.</p> <pre><code>#Lista de querys\nqr_structure2 = {\n    \"op_Trabajadores_p3\": '''\n        SELECT CODIGO_DOCUMENTO, ID,COD,PERRET, ESTADO, PERIODO, CODCAT, CODIGO_ZONA, \n        CALIFICACION_SUCURSAL, TIPAFI, SEXO, ESTCIV, NIVEDU, FECNAC, 'Trabajador' as TIPO,\n                IF( RURAL = 2 , \"Rural\" , \"Urbano\" ) as RURAL,\n                IF( PERRET = PERIODO , 1 , 0 ) as SE_RETIRO,\n                IF( perafi = PERIODO , 1 , 0 ) as SE_AFILIO\n                #IF( PEIODO = persis, 1 , 0 ) as NUEVO\n        FROM  dwh.BD_Fact_op_Trabajadores_p1  as p;\n    '''\n\n    }\n\n\ndim_names2 = {\n    \"op_Trabajadores_p3\": \"BD_Fact_op_Trabajadores_p3\"\n}\ndf_structure2 = dict()\nlogger.info('LECTURA DE QUERYS')\n\n\n\n#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor, qr_structure2, df_structure2, logger)\n</code></pre> <pre><code>2024-10-26 17:32:38,669 - INFO - LECTURA DE QUERYS\n2024-10-26 17:32:38,671 - INFO - CONEXION A BASE DWH\n2024-10-26 17:32:39,204 - INFO - Cargando op_Trabajadores_p3 \n2024-10-26 17:36:55,005 - INFO - Cargada op_Trabajadores_p3 --- 4.26 minutes ---\n2024-10-26 17:36:55,332 - INFO - CARGUE TABLAS DESDE MYSQL --- op_Trabajadores_p3 --- 4.28 minutes ---\n</code></pre> <pre><code>df = df_structure2['op_Trabajadores_p3']\n\ndf_standar = convertir_columnas_mayusculas(df)\n\ndf_standar.columns.tolist()\n</code></pre> <pre><code>['CODIGO_DOCUMENTO',\n 'ID',\n 'COD',\n 'PERRET',\n 'ESTADO',\n 'PERIODO',\n 'CODCAT',\n 'CODIGO_ZONA',\n 'CALIFICACION_SUCURSAL',\n 'TIPAFI',\n 'SEXO',\n 'ESTCIV',\n 'NIVEDU',\n 'FECNAC',\n 'TIPO',\n 'RURAL',\n 'SE_RETIRO',\n 'SE_AFILIO']\n</code></pre>"},{"location":"seccion/1.7-Fact_Historico_Trabajadores2/#almacenamiento-de-datos-estandarizados-en-el-data-warehouse","title":"Almacenamiento de Datos Estandarizados en el Data Warehouse","text":"<p>Este \u00faltimo paso guarda el DataFrame <code>df_standar</code> en el Data Warehouse dentro de la tabla <code>BD_Fact_op_Trabajadores_p3</code>. La funci\u00f3n <code>guardar_en_dwh</code> se encarga de realizar esta operaci\u00f3n, donde el par\u00e1metro <code>if_exists='replace'</code> indica que la tabla ser\u00e1 reemplazada si ya existe, garantizando que los datos almacenados est\u00e9n actualizados. El par\u00e1metro <code>multiple=False</code> optimiza la operaci\u00f3n de inserci\u00f3n en una \u00fanica transacci\u00f3n. Adem\u00e1s, el uso del <code>logger</code> permite registrar esta operaci\u00f3n, lo que facilita el monitoreo del proceso de carga y asegura la trazabilidad. Este flujo garantiza que los datos estandarizados y transformados est\u00e9n disponibles en el DWH para futuros an\u00e1lisis o reportes.</p> <pre><code>guardar_en_dwh(df_standar, 'BD_Fact_op_Trabajadores_p3', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-26 17:37:31,951 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 17:37:31,953 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 17:37:32,497 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_op_Trabajadores_p3\n2024-10-26 17:45:50,667 - INFO - Tabla almacenada correctamente.\n2024-10-26 17:45:52,983 - INFO - ALMACENAMIENTO ---  --- 8.35 minutes ---\n2024-10-26 17:45:52,985 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 17:46:01,636 - INFO - FINAL ETL --- 4574.19 seconds ---\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/","title":"1.8 Fact Historico Beneficiarios","text":""},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#introduccion","title":"Introducci\u00f3n","text":"<p>En el proceso ETL de la tabla <code>Fact_HistoricoBeneficiarios</code>, se extraen y transforman datos clave sobre beneficiarios desde el sistema de subsidios y se almacenan en un Data Warehouse (DWH). La estructura de entrada est\u00e1 conformada principalmente por datos detallados de afiliaci\u00f3n y retiro, as\u00ed como estados de beneficiarios de diferentes tablas de origen. Las consultas SQL procesan informaci\u00f3n de afiliados para agregar valores como categor\u00eda, estado y c\u00f3digos de afiliaci\u00f3n, entre otros, optimizando la informaci\u00f3n para su an\u00e1lisis a lo largo del tiempo.</p> <p>La salida se estructura en dos tablas de hechos en el DWH: <code>BD_Fact_Beneficiarios_p1</code> y <code>BD_Fact_Beneficiarios_p2</code>. Estas tablas consolidan datos hist\u00f3ricos de los beneficiarios, permitiendo una visibilidad completa sobre el comportamiento de la afiliaci\u00f3n, incluyendo el estado de retiro o continuidad de los beneficiarios y la categorizaci\u00f3n en niveles de educaci\u00f3n y ciudad, entre otros aspectos clave.</p>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<p>El diagrama a continuaci\u00f3n resume el flujo de datos desde la extracci\u00f3n y transformaci\u00f3n de los beneficiarios hasta su consolidaci\u00f3n en el DWH:</p> <pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL para Fact Hist\u00f3rico Beneficiarios\n    autonumber\n    participant \ud83d\udc64 Usuario\n    participant Script as Script ETL\n    participant DB as Bases de Datos\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar ETL de Fact Hist\u00f3rico Beneficiarios\n    Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c087, subsi22, BD_Fact_Beneficiarios_p1\n    DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n    Script-&gt;&gt;Script: Procesar y transformar datos\n    Script-&gt;DWH: Cargar datos en BD_Fact_Beneficiarios_p1\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga p1 exitosa\n    Script-&gt;DWH: Cargar datos en BD_Fact_Beneficiarios_p2\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga p2 exitosa\n    Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este flujo ilustra la secuencia de transformaci\u00f3n y carga en el DWH, donde las tablas de salida <code>BD_Fact_Beneficiarios_p1</code> y <code>BD_Fact_Beneficiarios_p2</code> contienen el registro hist\u00f3rico de afiliaci\u00f3n y retiro de beneficiarios, permitiendo un an\u00e1lisis detallado en m\u00faltiples dimensiones.</p>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#etl","title":"ETL","text":""},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo realiza las siguientes tareas principales: establece la conexi\u00f3n a una base de datos mediante SQLAlchemy, importa m\u00f3dulos y funciones espec\u00edficas para manipular y transformar datos en Pandas y NumPy, y ejecuta una serie de funciones personalizadas para cargar, transformar y guardar los datos en un Data Warehouse (DWH).</p> <p>La importaci\u00f3n del m\u00f3dulo <code>Funciones.py</code> incluye funciones que probablemente se encargan de conectar a la base de datos (<code>obtener_conexion</code>), transformar datos (<code>convertir_columnas_mayusculas</code>), y manejar el guardado en el DWH (<code>guardar_en_dwh</code>). Adem\u00e1s, el logger (<code>setup_logger</code>) ayuda a monitorear la ejecuci\u00f3n y errores del script, y <code>testfunciones</code> podr\u00eda verificar el estado de estas funciones o probar su correcto funcionamiento.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger, convertir_columnas_mayusculas\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>nombre.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='nombre.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 11:08:56,051 - INFO - Importacion de funciones correcta, 30-10-2024 11:08\n2024-10-30 11:08:56,053 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#consulta-sql-para-extraccion-de-datos-de-beneficiarios-y-periodos-de-afiliacion","title":"Consulta SQL para Extracci\u00f3n de Datos de Beneficiarios y Per\u00edodos de Afiliaci\u00f3n","text":"<p>Esta consulta SQL selecciona y consolida datos de beneficiarios y sus per\u00edodos de afiliaci\u00f3n en funci\u00f3n de m\u00faltiples uniones entre tablas (<code>xml4c087</code>, <code>subsi22</code>, <code>subsi23</code>, <code>subsi15</code>). La consulta construye un identificador <code>docben</code> a partir de los campos <code>coddocben</code> y <code>documento</code>, y calcula <code>persis</code>, que representa el a\u00f1o y mes del campo <code>fecsis</code>.</p> <p>Se extraen campos como <code>codben</code>, <code>periodo</code>, <code>estado</code>, <code>sexo</code>, <code>nivedu</code>, y <code>codzon</code>, as\u00ed como indicadores de afiliaci\u00f3n (<code>perafi</code>) y retiro (<code>perret</code>). El filtro final asegura que los resultados se encuentren en un rango de 18 meses desde la fecha actual, permitiendo analizar los beneficiarios activos y el historial de afiliaciones o retiros recientes.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n\nSELECT * FROM(\n    SELECT p2.*, CONCAT(p2.coddocben,p2.documento) as docben, CONCAT( SUBSTRING(p2.fecsis,1,4) , SUBSTRING(p2.fecsis,6,2) ) as persis, p3.codciu \n FROM  (SELECT p1.*, d5.fecsis   \n FROM  (SELECT d2.codben, periodo, 'A' as estado, d1.codtra as coddoc, d1.cedtra, d2.codest, d1.documento, d1.coddoc as coddocben, d1.tipgen as sexo, d1.codigo_dicap as captra, d2.nivedu, d1.fecnac, d1.parent, d1.codcat, d1.divpol AS codzon,\n                                d4.perafi, d3.perret, d5.nombre as calsuc, d1.tipafi, SUM(d1.numcuo) as numcuo, SUM(d1.subliq) as subliq\n                                FROM\n                                xml4.xml4c087 as d1 \n        LEFT JOIN (SELECT documento, codben, codest, nivedu\nfrom subsidio.subsi22\ngroup by documento) as d2\n\n                                ON d1.documento=d2.documento\n                                LEFT JOIN (select coddoc, documento,\n                                IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c087), MAX(periodo), NULL) as perret from xml4.xml4c087 GROUP BY coddoc, documento) as d3\nON d1.coddoc = d3.coddoc\nAND d1.documento = d3.documento\nLEFT JOIN (select coddoc, documento,\nMIN(periodo) as perafi from xml4.xml4c087 GROUP BY coddoc, documento) as d4\nON d1.coddoc = d4.coddoc\nAND d1.documento = d4.documento\n\nLEFT JOIN xml4.xml4b006 as d5\nON d1.tipafi = d5.tipafi\n\n#where d1.documento in('1085104776', '1084473954', '1084473707', '1104383175')\n\n                                GROUP BY \n    d1.documento, d1.coddoc, periodo) as p1\n     LEFT JOIN\n                                subsidio.subsi23 as d5\n                                ON p1.codben=d5.codben) as p2\n                                LEFT JOIN subsidio.subsi15 p3\n    ON p2.cedtra = p3.cedtra\n    AND p2.coddoc = p3.coddoc\n\n    WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR))\n        OR (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND perret IS NULL)) as m\n WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR);    \n        \"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:08:56,060 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#consulta-sql-para-consolidacion-de-datos-de-beneficiarios-por-periodo","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Beneficiarios por Per\u00edodo","text":"<p>Esta consulta en <code>qr_structure2</code> agrupa y consolida informaci\u00f3n de beneficiarios en el Data Warehouse para cada per\u00edodo (<code>PERIODO</code>). Se seleccionan datos clave, como <code>ID_AFILIADO</code>, <code>CODIGO_DOCUMENTO</code>, <code>CODIGO_BENEFICIARIO</code>, <code>CODIGO_ZONA</code>, <code>CODIGO_ESTADO</code>, y <code>SEXO</code>, junto con informaci\u00f3n de afiliaci\u00f3n y retiros (<code>SE_RETIRO</code>, <code>SE_AFILIO</code>). Indicadores adicionales, como <code>NUEVO</code> (si el beneficiario fue registrado en el sistema en el mismo per\u00edodo) y atributos personales (<code>CAPACIDAD_TRABAJO</code>, <code>NIVEL_EDUCATIVO</code>, <code>FECHA_NACIMIENTO</code>), tambi\u00e9n se incluyen.</p> <p>Los datos se agrupan para obtener valores \u00fanicos mediante la funci\u00f3n <code>MAX</code> en cada campo, y se establece un grupo por <code>PERIODO</code>, <code>TIPO</code>, <code>COD</code>, y <code>ESTADO</code>, generando una vista resumida de beneficiarios en el Data Warehouse. Esta estructura facilita el an\u00e1lisis de cambios en la afiliaci\u00f3n, retiros, y caracter\u00edsticas generales de la poblaci\u00f3n de beneficiarios.</p> <pre><code># Lista de consultas actualizada\nqr_structure2 = {\n    \"query\": \n    \"\"\"\n        SELECT PERIODO, \n               ID_AFILIADO,\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \n               MAX(ID) as ID, \n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \n               MAX(CODCAT) as CODIGO_CATEGORIA, \n               MAX(CODZON) as CODIGO_ZONA, \n               MAX(CODEST) as CODIGO_ESTADO, \n               TIPO, \n               COD, \n               ESTADO,\n               MAX(SE_RETIRO) as SE_RETIRO, \n               MAX(SE_AFILIO) as SE_AFILIO,  \n               MAX(NUEVO) as NUEVO, \n               MAX(SEXO) as SEXO, \n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \n               MAX(FECNAC) as FECHA_NACIMIENTO, \n               MAX(PARENT) as PARENTESCO, \n               MAX(CODCIU) as CODIGO_CIUDAD, \n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \n               MAX(TIPAFI) as TIPO_AFILIACION, \n               MAX(NUMCUO) as NUMERO_CUOTAS, \n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \n        FROM (\n            SELECT \n                   ID_AFILIADO,\n                   CODIGO_DOCUMENTO, \n                   DOCUMENTO as ID, \n                   CODIGO_BENEFICIARIO as CODBEN, \n                   ESTADO, \n                   PERIODO, \n                   CODIGO_ZONA as CODZON, \n                   CODIGO_ESTADO as CODEST, \n                   DOCUMENTO_BENEFICIARIO as COD, \n                   CODIGO_CATEGORIA as CODCAT, \n                   SEXO,\n                   CAPACIDAD_TRABAJO as CAPTRA,\n                   NIVEL_EDUCACION as NIVEL_EDU, \n                   FECHA_NACIMIENTO as FECNAC, \n                   PARENTESCO as PARENT, \n                   CODIGO_CIUDAD as CODCIU, \n                   'Beneficiario' as TIPO, \n                   CALIFICACION_SUCURSAL as CALSUC, \n                   TIPO_AFILIACION as TIPAFI, \n                   NUMERO_CUOTAS as NUMCUO, \n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\n        ) as h\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\n    \"\"\"\n}\n\ndf_structure2 = dict()\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE ')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 11:08:56,130 - INFO - CONEXION A BASE \n2024-10-30 11:08:56,427 - INFO - Cargando query \n2024-10-30 11:17:36,688 - INFO - Cargada query, 3,829,359 registros finales obtenidos. --- 8.67 minutes ---\n2024-10-30 11:17:37,068 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 8.68 minutes ---\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#consulta-de-codigos-de-documentos-y-conexion-a-base-de-datos-dwh","title":"Consulta de C\u00f3digos de Documentos y Conexi\u00f3n a Base de Datos DWH","text":"<p>Este bloque de c\u00f3digo ejecuta una consulta para obtener todos los registros de la tabla <code>BD_Dim_Codigo_Documento</code> en el Data Warehouse (<code>dwh</code>), con el objetivo de cargar los c\u00f3digos de documentos en un diccionario <code>df_codigos</code>. La funci\u00f3n <code>create_engine</code> se utiliza para establecer una conexi\u00f3n con la base de datos mediante la funci\u00f3n personalizada <code>obtener_conexion</code>, la cual devuelve la cadena de conexi\u00f3n al DWH.</p> <p>Una vez establecida la conexi\u00f3n, la funci\u00f3n <code>cargar_tablas</code> se ejecuta con los par\u00e1metros <code>motor2</code>, <code>qr_structure_cod</code>, <code>df_codigos</code> y <code>logger</code>, encarg\u00e1ndose de ejecutar la consulta definida en <code>qr_structure_cod</code> y almacenar los resultados en <code>df_codigos</code>. Esta funci\u00f3n probablemente maneja la ejecuci\u00f3n de la consulta y el procesamiento de los datos, almacen\u00e1ndolos en un DataFrame u otra estructura.</p> <pre><code>#Lista de querys\nqr_structure_cod = {\n    \"query\": \"\"\"\nSELECT * FROM dwh.BD_Dim_Codigo_Documento;\n    \"\"\"\n}\ndf_codigos = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure2['query'])\n\n\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor2, qr_structure_cod, df_codigos, logger)\n</code></pre> <pre><code>2024-10-30 11:17:37,079 - INFO - LECTURA DE QUERYS\n2024-10-30 11:17:37,081 - INFO - CONEXION A BASE DWH\n\n\n\n        SELECT PERIODO, \n               ID_AFILIADO,\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \n               MAX(ID) as ID, \n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \n               MAX(CODCAT) as CODIGO_CATEGORIA, \n               MAX(CODZON) as CODIGO_ZONA, \n               MAX(CODEST) as CODIGO_ESTADO, \n               TIPO, \n               COD, \n               ESTADO,\n               MAX(SE_RETIRO) as SE_RETIRO, \n               MAX(SE_AFILIO) as SE_AFILIO,  \n               MAX(NUEVO) as NUEVO, \n               MAX(SEXO) as SEXO, \n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \n               MAX(FECNAC) as FECHA_NACIMIENTO, \n               MAX(PARENT) as PARENTESCO, \n               MAX(CODCIU) as CODIGO_CIUDAD, \n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \n               MAX(TIPAFI) as TIPO_AFILIACION, \n               MAX(NUMCUO) as NUMERO_CUOTAS, \n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \n        FROM (\n            SELECT \n                   ID_AFILIADO,\n                   CODIGO_DOCUMENTO, \n                   DOCUMENTO as ID, \n                   CODIGO_BENEFICIARIO as CODBEN, \n                   ESTADO, \n                   PERIODO, \n                   CODIGO_ZONA as CODZON, \n                   CODIGO_ESTADO as CODEST, \n                   DOCUMENTO_BENEFICIARIO as COD, \n                   CODIGO_CATEGORIA as CODCAT, \n                   SEXO,\n                   CAPACIDAD_TRABAJO as CAPTRA,\n                   NIVEL_EDUCACION as NIVEL_EDU, \n                   FECHA_NACIMIENTO as FECNAC, \n                   PARENTESCO as PARENT, \n                   CODIGO_CIUDAD as CODCIU, \n                   'Beneficiario' as TIPO, \n                   CALIFICACION_SUCURSAL as CALSUC, \n                   TIPO_AFILIACION as TIPAFI, \n                   NUMERO_CUOTAS as NUMCUO, \n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\n        ) as h\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\n\n\n\n2024-10-30 11:17:37,377 - INFO - Cargando query \n2024-10-30 11:17:37,407 - INFO - Cargada query, 10 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 11:17:37,458 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#union-de-dataframes-para-vincular-informacion-de-codigos-de-documentos","title":"Uni\u00f3n de DataFrames para Vincular Informaci\u00f3n de C\u00f3digos de Documentos","text":"<p>Este c\u00f3digo transforma las columnas <code>coddoc</code> en <code>df</code> y <code>COD_SUPERSUBSIDIO</code> en <code>df_cod</code> a texto en may\u00fasculas para asegurar consistencia. Luego, realiza una uni\u00f3n (<code>merge</code>) entre <code>df</code> y <code>df_cod</code>, usando <code>coddoc</code> en <code>df</code> y <code>COD_SUPERSUBSIDIO</code> en <code>df_cod</code> para agregar datos de documentos desde <code>df_cod</code> a <code>df</code>.</p> <p>La uni\u00f3n es del tipo <code>left</code>, conservando todos los registros de <code>df</code> y anexando las coincidencias de <code>df_cod</code> cuando est\u00e1n disponibles. La funci\u00f3n <code>head()</code> muestra las primeras filas de <code>df_merged</code>, permitiendo verificar visualmente los resultados de la fusi\u00f3n.</p> <pre><code>df_cod = df_codigos['query']\n\ndf = df_structure['query']\n\n# Convertir 'coddoc' y 'CODDOC' a tipo string y a may\u00fasculas\ndf['coddoc'] = df['coddoc'].astype(str).str.upper()\ndf_cod['COD_SUPERSUBSIDIO'] = df_cod['COD_SUPERSUBSIDIO'].astype(str).str.upper()\n\n# Realizar el merge usando 'CODDOC' de df_cod\ndf_merged = df.merge(df_cod[['COD_SUPERSUBSIDIO', 'CODDOC']], left_on='coddoc', right_on='COD_SUPERSUBSIDIO', how='left')\n\n# Mostrar las primeras filas para verificar el resultado\ndf_merged.head()\n</code></pre> codben periodo estado coddoc cedtra codest documento coddocben sexo captra ... calsuc tipafi numcuo subliq fecsis docben persis codciu COD_SUPERSUBSIDIO CODDOC 0 406348.0 202305 A 1 1000120248 None 1084067407 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2020-02-08 51084067407 202002 None 1 CC 1 NaN 202305 A 1 1000120248 None 1128144889 1 2 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11128144889 None None 1 CC 2 NaN 202305 A 1 1000121261 None 1007860991 1 2 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11007860991 None None 1 CC 3 364243.0 202305 A 1 1000121261 None 1083040303 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2017-11-02 51083040303 201711 None 1 CC 4 364243.0 202305 A 1 1000121261 None 1083040303 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2023-08-28 51083040303 202308 None 1 CC <p>5 rows \u00d7 27 columns</p>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#creacion-de-identificador-unico-de-afiliado-y-estandarizacion-de-columnas","title":"Creaci\u00f3n de Identificador \u00danico de Afiliado y Estandarizaci\u00f3n de Columnas","text":"<p>Este c\u00f3digo combina los valores de <code>CODDOC</code> y <code>cedtra</code> para crear una columna <code>ID_AFILIADO</code> que representa un identificador \u00fanico de afiliado en <code>df_final</code>. Posteriormente, estandariza los nombres de columnas en <code>df_final</code> mediante un diccionario (<code>_names</code>) con la funci\u00f3n <code>convertir_columnas_mayusculas</code>, asegurando consistencia y claridad en la nomenclatura de campos.</p> <p>El DataFrame <code>df_standar</code> resultante se filtra para incluir \u00fanicamente las columnas deseadas en un orden espec\u00edfico, y <code>df_final.columns.to_list()</code> proporciona la lista de nombres de columnas finales, confirmando que los nombres y el orden sean correctos antes de proceder con el an\u00e1lisis o almacenamiento en el Data Warehouse.</p> <pre><code># Crear 'ID_AFILIADO' en un solo paso y asignar directamente a df_final\ndf_final = df_merged\ndf_final['ID_AFILIADO'] = df_final['CODDOC'].astype(str) + df_final['cedtra'].astype(str)\n\n# Mostrar el resultado\n_names = {\n    'codben': 'CODIGO_BENEFICIARIO',\n    'coddoc': 'Codigo_Documento',\n    'cedtra': 'Cedula_Trabajador',\n    'codest': 'Codigo_Estado',\n    'captra': 'Capacidad_Trabajo',\n    'nivedu': 'Nivel_Educacion',\n    'fecnac': 'Fecha_Nacimiento',\n    'parent': 'Parentesco',\n    'codcat': 'Codigo_Categoria',\n    'codzon': 'Codigo_Zona',\n    'perafi': 'Periodo_Afiliacion',\n    'perret': 'Periodo_Retiro',\n    'calsuc': 'Calificacion_Sucursal',\n    'tipafi': 'Tipo_Afiliacion',\n    'numcuo': 'Numero_Cuotas',\n    'subliq': 'Subsidio_Liquidado',\n    'fecsis': 'Fecha_Sistema',\n    'docben': 'Documento_Beneficiario',\n    'persis': 'Periodo_Sistema',\n    'codciu': 'Codigo_Ciudad',\n    'CODDOC': 'Codigo_Documento_Afiliado'\n}\n\n\ndf_standar = convertir_columnas_mayusculas(df_final,_names)\ndf_final = df_standar[[\n    'CODIGO_BENEFICIARIO',\n    'ID_AFILIADO',\n    'CEDULA_TRABAJADOR',\n    'PERIODO',\n    'ESTADO',\n    'CODIGO_DOCUMENTO',\n    'CODIGO_ESTADO',\n    'DOCUMENTO',\n    'CODDOCBEN',\n    'SEXO',\n    'CAPACIDAD_TRABAJO',\n    'NIVEL_EDUCACION',\n    'FECHA_NACIMIENTO',\n    'PARENTESCO',\n    'CODIGO_CATEGORIA',\n    'CODIGO_ZONA',\n    'PERIODO_AFILIACION',\n    'PERIODO_RETIRO',\n    'CALIFICACION_SUCURSAL',\n    'TIPO_AFILIACION',\n    'NUMERO_CUOTAS',\n    'SUBSIDIO_LIQUIDADO',\n    'FECHA_SISTEMA',\n    'DOCUMENTO_BENEFICIARIO',\n    'PERIODO_SISTEMA',\n    'CODIGO_CIUDAD',\n    'COD_SUPERSUBSIDIO',\n    'CODIGO_DOCUMENTO_AFILIADO',\n    ]]\ndf_final.columns.to_list()\n</code></pre> <pre><code>['CODIGO_BENEFICIARIO',\n 'ID_AFILIADO',\n 'CEDULA_TRABAJADOR',\n 'PERIODO',\n 'ESTADO',\n 'CODIGO_DOCUMENTO',\n 'CODIGO_ESTADO',\n 'DOCUMENTO',\n 'CODDOCBEN',\n 'SEXO',\n 'CAPACIDAD_TRABAJO',\n 'NIVEL_EDUCACION',\n 'FECHA_NACIMIENTO',\n 'PARENTESCO',\n 'CODIGO_CATEGORIA',\n 'CODIGO_ZONA',\n 'PERIODO_AFILIACION',\n 'PERIODO_RETIRO',\n 'CALIFICACION_SUCURSAL',\n 'TIPO_AFILIACION',\n 'NUMERO_CUOTAS',\n 'SUBSIDIO_LIQUIDADO',\n 'FECHA_SISTEMA',\n 'DOCUMENTO_BENEFICIARIO',\n 'PERIODO_SISTEMA',\n 'CODIGO_CIUDAD',\n 'COD_SUPERSUBSIDIO',\n 'CODIGO_DOCUMENTO_AFILIADO']\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_nombre</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_Beneficiarios_p1', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\ndf_final\n</code></pre> <pre><code>2024-10-30 11:17:45,787 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:17:45,788 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:17:46,088 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Beneficiarios_p1\n2024-10-30 11:28:18,350 - INFO - Tabla almacenada correctamente. 3,832,674 registros finales obtenidos.\n2024-10-30 11:28:22,516 - INFO - ALMACENAMIENTO ---  --- 10.61 minutes ---\n2024-10-30 11:28:22,518 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> CODIGO_BENEFICIARIO ID_AFILIADO CEDULA_TRABAJADOR PERIODO ESTADO CODIGO_DOCUMENTO CODIGO_ESTADO DOCUMENTO CODDOCBEN SEXO ... CALIFICACION_SUCURSAL TIPO_AFILIACION NUMERO_CUOTAS SUBSIDIO_LIQUIDADO FECHA_SISTEMA DOCUMENTO_BENEFICIARIO PERIODO_SISTEMA CODIGO_CIUDAD COD_SUPERSUBSIDIO CODIGO_DOCUMENTO_AFILIADO 0 406348.0 CC1000120248 1000120248 202305 A 1 None 1084067407 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2020-02-08 51084067407 202002 None 1 CC 1 NaN CC1000120248 1000120248 202305 A 1 None 1128144889 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11128144889 None None 1 CC 2 NaN CC1000121261 1000121261 202305 A 1 None 1007860991 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11007860991 None None 1 CC 3 364243.0 CC1000121261 1000121261 202305 A 1 None 1083040303 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2017-11-02 51083040303 201711 None 1 CC 4 364243.0 CC1000121261 1000121261 202305 A 1 None 1083040303 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2023-08-28 51083040303 202308 None 1 CC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3832669 8647200.0 CC9910609 9910609 202409 A 1 None 1105473400 2 1 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 1.0 44491.0 2021-12-16 21105473400 202112 None 1 CC 3832670 NaN CC9910609 9910609 202409 A 1 None 49597057 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 None 149597057 None None 1 CC 3832671 314997.0 CC9910731 9910731 202409 A 1 None 1082877639 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 2015-05-07 11082877639 201505 None 1 CC 3832672 NaN CC9910731 9910731 202409 A 1 None 1082939020 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 None 11082939020 None None 1 CC 3832673 310091.0 CC9910731 9910731 202409 A 1 None 1083018512 2 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 1.0 44491.0 2015-02-12 21083018512 201502 None 1 CC <p>3832674 rows \u00d7 28 columns</p>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#conexion-a-la-base-de-datos-dwh-y-carga-de-tablas","title":"Conexi\u00f3n a la Base de Datos DWH y Carga de Tablas","text":"<p>Este c\u00f3digo establece una conexi\u00f3n con la base de datos de Data Warehouse (DWH) utilizando <code>create_engine</code> con los detalles de conexi\u00f3n proporcionados por la funci\u00f3n <code>obtener_conexion</code>. La funci\u00f3n personalizada <code>cargar_tablas</code> se encarga de ejecutar las consultas definidas en <code>qr_structure2</code>, almacenando los resultados en el diccionario <code>df_structure2</code>.</p> <p>Al final, se ejecuta el m\u00e9todo <code>head()</code> en <code>df_structure2['query']</code> para mostrar las primeras filas del DataFrame resultante, permitiendo una verificaci\u00f3n r\u00e1pida de los datos cargados. Esto asegura que la conexi\u00f3n a la base de datos y la carga de datos se han realizado correctamente antes de proceder con el an\u00e1lisis o transformaci\u00f3n de datos.</p> <pre><code>qr_structure2\n</code></pre> <pre><code>{'query': \"\\n        SELECT PERIODO, \\n               ID_AFILIADO,\\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \\n               MAX(ID) as ID, \\n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \\n               MAX(CODCAT) as CODIGO_CATEGORIA, \\n               MAX(CODZON) as CODIGO_ZONA, \\n               MAX(CODEST) as CODIGO_ESTADO, \\n               TIPO, \\n               COD, \\n               ESTADO,\\n               MAX(SE_RETIRO) as SE_RETIRO, \\n               MAX(SE_AFILIO) as SE_AFILIO,  \\n               MAX(NUEVO) as NUEVO, \\n               MAX(SEXO) as SEXO, \\n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \\n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \\n               MAX(FECNAC) as FECHA_NACIMIENTO, \\n               MAX(PARENT) as PARENTESCO, \\n               MAX(CODCIU) as CODIGO_CIUDAD, \\n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \\n               MAX(TIPAFI) as TIPO_AFILIACION, \\n               MAX(NUMCUO) as NUMERO_CUOTAS, \\n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \\n        FROM (\\n            SELECT \\n                   ID_AFILIADO,\\n                   CODIGO_DOCUMENTO, \\n                   DOCUMENTO as ID, \\n                   CODIGO_BENEFICIARIO as CODBEN, \\n                   ESTADO, \\n                   PERIODO, \\n                   CODIGO_ZONA as CODZON, \\n                   CODIGO_ESTADO as CODEST, \\n                   DOCUMENTO_BENEFICIARIO as COD, \\n                   CODIGO_CATEGORIA as CODCAT, \\n                   SEXO,\\n                   CAPACIDAD_TRABAJO as CAPTRA,\\n                   NIVEL_EDUCACION as NIVEL_EDU, \\n                   FECHA_NACIMIENTO as FECNAC, \\n                   PARENTESCO as PARENT, \\n                   CODIGO_CIUDAD as CODCIU, \\n                   'Beneficiario' as TIPO, \\n                   CALIFICACION_SUCURSAL as CALSUC, \\n                   TIPO_AFILIACION as TIPAFI, \\n                   NUMERO_CUOTAS as NUMCUO, \\n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \\n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\\n        ) as h\\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\\n    \"}\n</code></pre> <pre><code>#Conexion a base DWH\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A DWH ')\ncargar_tablas(motor, qr_structure2, df_structure2, logger)\ndf_structure2['query'].head()\n</code></pre> <pre><code>2024-10-30 11:28:26,277 - INFO - CONEXION A DWH \n2024-10-30 11:28:26,577 - INFO - Cargando query \n2024-10-30 11:31:42,403 - INFO - Cargada query, 3,371,398 registros finales obtenidos. --- 3.26 minutes ---\n2024-10-30 11:31:42,714 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 3.27 minutes ---\n</code></pre> PERIODO ID_AFILIADO CODIGO_DOCUMENTO ID CODIGO_BENEFICIARIO CODIGO_CATEGORIA CODIGO_ZONA CODIGO_ESTADO TIPO COD ... SEXO CAPACIDAD_TRABAJO NIVEL_EDUCATIVO FECHA_NACIMIENTO PARENTESCO CODIGO_CIUDAD CALIFICACION_SUCURSAL TIPO_AFILIACION NUMERO_CUOTAS SUBSIDIO_LIQUIDADO 0 202305 CC1000120248 1 1084067407 406348.0 1 47001 None Beneficiario 51084067407 ... 1 2 01 2019-12-14 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 1 202305 CC1000120248 1 1128144889 NaN 1 47001 None Beneficiario 11128144889 ... 2 2 None 2002-03-07 5 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2 202305 CC1000121261 1 1007860991 NaN 1 47001 None Beneficiario 11007860991 ... 2 2 None 1998-11-12 5 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 3 202305 CC1000121261 1 1083040303 364243.0 1 47001 None Beneficiario 51083040303 ... 1 2 01 2016-10-27 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 4 202305 CC1000121261 1 1084469342 8626426.0 1 47001 None Beneficiario 51084469342 ... 1 2 01 2019-10-22 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 <p>5 rows \u00d7 24 columns</p> <pre><code>print(df_structure2['query']['PERIODO'].unique())\n</code></pre> <pre><code>['202305' '202306' '202307' '202308' '202309' '202310' '202311' '202312'\n '202401' '202402' '202403' '202404' '202405' '202406' '202407' '202408'\n '202409']\n</code></pre>"},{"location":"seccion/1.8-Fact_HistoricoBeneficiarios/#guardado-de-datos-en-dwh-y-verificacion-de-columnas","title":"Guardado de Datos en DWH y Verificaci\u00f3n de Columnas","text":"<p>En este c\u00f3digo, el DataFrame <code>df_final2</code> (cargado desde <code>df_structure2['query']</code>) se guarda en la base de datos DWH en la tabla <code>BD_Fact_Beneficiarios_p2</code> utilizando la funci\u00f3n <code>guardar_en_dwh</code>. Esta funci\u00f3n recibe <code>multiple=False</code> para realizar una carga \u00fanica y <code>if_exists='replace'</code> para reemplazar la tabla si ya existe en la base de datos, asegurando que los datos m\u00e1s recientes reemplacen cualquier versi\u00f3n anterior.</p> <p>Al final, <code>df_final2.columns.to_list()</code> se usa para obtener una lista de los nombres de columnas en <code>df_final2</code>, permitiendo verificar r\u00e1pidamente que las columnas est\u00e1n en el orden y con los nombres esperados antes de completar el proceso de carga.</p> <pre><code>df_final2 = df_structure2['query']\n\nguardar_en_dwh(df_final2, 'BD_Fact_Beneficiarios_p2', logger, multiple=False, if_exists='replace')\ndf_final2.columns.to_list()\n</code></pre> <pre><code>2024-10-30 11:31:43,061 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:31:43,062 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:31:43,387 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Beneficiarios_p2\n2024-10-30 11:40:19,134 - INFO - Tabla almacenada correctamente. 3,371,398 registros finales obtenidos.\n2024-10-30 11:40:22,203 - INFO - ALMACENAMIENTO ---  --- 8.65 minutes ---\n2024-10-30 11:40:22,204 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['PERIODO',\n 'ID_AFILIADO',\n 'CODIGO_DOCUMENTO',\n 'ID',\n 'CODIGO_BENEFICIARIO',\n 'CODIGO_CATEGORIA',\n 'CODIGO_ZONA',\n 'CODIGO_ESTADO',\n 'TIPO',\n 'COD',\n 'ESTADO',\n 'SE_RETIRO',\n 'SE_AFILIO',\n 'NUEVO',\n 'SEXO',\n 'CAPACIDAD_TRABAJO',\n 'NIVEL_EDUCATIVO',\n 'FECHA_NACIMIENTO',\n 'PARENTESCO',\n 'CODIGO_CIUDAD',\n 'CALIFICACION_SUCURSAL',\n 'TIPO_AFILIACION',\n 'NUMERO_CUOTAS',\n 'SUBSIDIO_LIQUIDADO']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 11:40:22,218 - INFO - FINAL ETL --- 1886.17 seconds ---\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/","title":"1.9 Dim Parametricas","text":""},{"location":"seccion/1.9-Dim_Parametricas/#introduccion","title":"Introducci\u00f3n","text":"<p>El c\u00f3digo est\u00e1 dise\u00f1ado para realizar operaciones de carga y transformaci\u00f3n de datos principalmente desde archivos Excel. Para ello, emplea funciones personalizadas que gestionan la conexi\u00f3n con el Data Warehouse mediante <code>SQLAlchemy</code>, y utiliza la librer\u00eda <code>Pandas</code> para manipular los <code>DataFrames</code> generados a partir de estos archivos.</p> <p>Cada funci\u00f3n sigue un patr\u00f3n estructurado: se carga un archivo Excel en un <code>DataFrame</code>, se aplican las transformaciones necesarias, y finalmente se almacena el resultado en tablas espec\u00edficas dentro del Data Warehouse mediante la funci\u00f3n <code>guardar_en_dwh</code>. Este proceso permite manejar par\u00e1metros como el m\u00e9todo de inserci\u00f3n, lo que facilita la opci\u00f3n de agregar o reemplazar datos en funci\u00f3n de las necesidades de cada carga.</p> <p>La naturaleza modular y automatizada de este enfoque permite la integraci\u00f3n eficiente de datos de m\u00faltiples fuentes y asegura que el proceso sea escalable y adaptable.</p> <p>El proceso ETL descrito est\u00e1 optimizado para la carga masiva de archivos CSV y Excel, aprovechando <code>SQLAlchemy</code> y <code>Pandas</code> para consolidar datos de distintas fuentes en un Data Warehouse centralizado. Las etapas abarcan la extracci\u00f3n, transformaci\u00f3n (incluyendo limpieza y estandarizaci\u00f3n) y carga en tablas espec\u00edficas del Data Warehouse.</p> <p>Las tablas de entrada provienen de archivos variados, como <code>Mercurio_beneficiario.csv</code> y <code>Metricas.xlsx</code>. Los datos se procesan mediante funciones dedicadas que preparan los <code>DataFrames</code> y los almacenan en tablas estructuradas en el Data Warehouse, donde cada tabla de salida representa un conjunto de datos procesados, como <code>Fact_Mercurio_Beneficiarios</code> y <code>BD_Dim_Metricas</code>. Este enfoque modular permite aplicar transformaciones espec\u00edficas (como la conversi\u00f3n de nombres de columnas a may\u00fasculas o ajustes en los nombres de columnas para alinearse con est\u00e1ndares internos) de manera eficiente y consistente.</p>"},{"location":"seccion/1.9-Dim_Parametricas/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL - Carga y Transformaci\u00f3n de Archivos\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant ETL\n  participant BD as Base de Datos\n  Usuario-&gt;&gt;ETL: Solicitar procesamiento de datos de archivos de entrada\n  ETL-&gt;&gt;ETL: Leer archivos Excel y CSV (Pandas)\n  ETL-&gt;&gt;ETL: Aplicar transformaci\u00f3n de datos (limpieza, conversi\u00f3n de columnas, ajustes de formato)\n  ETL-&gt;&gt;BD: Cargar datos en tablas espec\u00edficas (guardar_en_dwh)\n  BD--&gt;&gt;ETL: Confirmaci\u00f3n de carga exitosa\n  ETL--&gt;&gt;Usuario: Respuesta con mensaje de \u00e9xito en la carga de datos</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#etl","title":"ETL","text":""},{"location":"seccion/1.9-Dim_Parametricas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>El c\u00f3digo configura el entorno para ejecutar funciones personalizadas importadas desde el archivo <code>Funciones.py</code>. Tras agregar el directorio correspondiente al <code>sys.path</code> para asegurar que Python pueda encontrar el m\u00f3dulo, se importan funciones como <code>guardar_en_dwh</code> y <code>testfunciones</code>, que probablemente manejan la carga de datos y pruebas unitarias. Adem\u00e1s, <code>setup_logger</code> permite configurar el sistema de logs para registrar eventos y errores. El uso de <code>time</code> permite medir el tiempo de ejecuci\u00f3n, lo que es \u00fatil para monitorear el rendimiento del proceso.</p> <pre><code>import pandas as pd\nimport time\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, testfunciones, setup_logger, convertir_columnas_mayusculas\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Parametricas.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Parametricas.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\nsys.path.append(os.path.abspath('../Parametricas'))\n</code></pre> <pre><code>2024-10-25 22:17:39,728 - INFO - Importacion de funciones correcta, 25-10-2024 22:17\n2024-10-25 22:17:39,730 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#mercurio_beneficiarios","title":"Mercurio_Beneficiarios","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-de-datos-desde-un-csv","title":"Carga de Datos desde un CSV","text":"<p>Este fragmento de c\u00f3digo utiliza la funci\u00f3n <code>pd.read_csv()</code> de Pandas para cargar un archivo CSV llamado <code>Mercurio_beneficiario.csv</code>, que se encuentra en el directorio <code>../Parametricas</code>. La opci\u00f3n <code>low_memory=False</code> desactiva la carga por fragmentos para evitar posibles problemas de tipo en las columnas cuando se trabaja con grandes vol\u00famenes de datos. Tras cargar el archivo, se imprime el DataFrame resultante, permitiendo visualizar los datos importados y verificar su correcta lectura.</p> <pre><code>parametica_time = time.time()\ndf_b = pd.read_csv(\"../Parametricas/Mercurio_beneficiario.csv\", low_memory=False)\ndf_b\n</code></pre> radicado tipdoc numdoc coddocrelvin relvin tipcli fecsol estado tipvin tippar 0 830 TI 1082879929 CC 84453898 persona 2020-06-27 P beneficiario hijo 1 830 TI 1082879929 CC 84453898 persona 2020-07-06 D beneficiario hijo 2 830 TI 1082879929 CC 84453898 persona 2022-02-23 P beneficiario hijo 3 830 TI 1082879929 CC 84453898 persona 2022-03-02 A beneficiario hijo 4 1222 TI 1082859410 CC 84456479 persona 2022-02-03 P beneficiario hijo ... ... ... ... ... ... ... ... ... ... ... 70021 54783 CC 12551028 CC 1082948733 persona 2024-02-01 D beneficiario padre 70022 54783 CC 12551028 CC 1082948733 persona 2024-02-06 P beneficiario padre 70023 54783 CC 12551028 CC 1082948733 persona 2024-02-08 A beneficiario padre 70024 54784 TI 1081922640 CC 1081919446 persona 2024-02-01 P beneficiario hijo 70025 54784 TI 1081922640 CC 1081919446 persona 2024-02-10 A beneficiario hijo <p>70026 rows \u00d7 10 columns</p>"},{"location":"seccion/1.9-Dim_Parametricas/#funcion-para-guardar-beneficiarios-de-mercurio-en-data-warehouse","title":"Funci\u00f3n para Guardar Beneficiarios de Mercurio en Data Warehouse","text":"<p>La funci\u00f3n <code>Mercurio_Beneficiarios</code> carga el DataFrame <code>df_b</code> (previamente le\u00eddo desde el archivo CSV <code>Mercurio_beneficiario.csv</code>) y lo almacena en una tabla del Data Warehouse llamada <code>Fact_Mercurio_Beneficiarios</code>. La funci\u00f3n <code>guardar_en_dwh</code> es utilizada para insertar los datos en la base de datos, y se configuran las opciones <code>multiple=False</code> para especificar que no se est\u00e1n enviando m\u00faltiples registros de una vez, e <code>if_exists='replace'</code> para a\u00f1adir los nuevos datos a la tabla existente en lugar de reemplazarla.</p> <pre><code>def Mercurio_Beneficiarios(): \n    Mercurio_Beneficiarios  = df_b\n    guardar_en_dwh(Mercurio_Beneficiarios, 'Fact_Mercurio_Beneficiarios', logger, multiple=False, if_exists='replace')\n    logger.info(f'Tiempo Fact_Mercurio_Beneficiarios --- {time.time() - parametica_time:.2f} seconds ---')\nMercurio_Beneficiarios()\n</code></pre> <pre><code>2024-10-25 22:18:23,128 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:18:23,131 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:18:23,667 - INFO - Almacenando tabla \u00fanica en DWH como Fact_Mercurio_Beneficiarios\n2024-10-25 22:18:32,311 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:18:32,877 - INFO - ALMACENAMIENTO ---  --- 9.75 seconds ---\n2024-10-25 22:18:32,879 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:18:32,881 - INFO - Tiempo Fact_Mercurio_Beneficiarios --- 48.65 seconds ---\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#mercurio_trabajadores","title":"Mercurio_Trabajadores","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-de-datos-desde-un-csv-trabajadores","title":"Carga de Datos desde un CSV - Trabajadores","text":"<p>Este bloque de c\u00f3digo carga un archivo CSV llamado <code>Mercurio_Trabajadores.csv</code> que se encuentra en el directorio <code>../Parametricas</code>. Se utiliza <code>pd.read_csv()</code> de Pandas para leer los datos, con la opci\u00f3n <code>low_memory=False</code>, la cual previene problemas relacionados con la asignaci\u00f3n de tipos de datos en columnas cuando el archivo es muy grande. Despu\u00e9s de cargar el archivo, el contenido del DataFrame <code>df_t</code> es impreso para revisar y verificar los datos le\u00eddos del archivo CSV.</p> <pre><code>parametica_time = time.time()\ndf_t = pd.read_csv(\"../Parametricas/Mercurio_Trabajadores.csv\", low_memory=False)\n\ndf_t\n</code></pre> radicado tipdoc cedtra relvin tipcli fecsol estado tipvin tippar 0 474 CC 57433308 4979946 Trabajador 2020-04-03 P empleado NaN 1 474 CC 57433308 4979946 Trabajador 2020-04-11 P empleado NaN 2 474 CC 57433308 4979946 Trabajador 2020-04-17 D empleado NaN 3 474 CC 57433308 4979946 Trabajador 2023-05-02 P empleado NaN 4 474 CC 57433308 4979946 Trabajador 2023-05-02 A empleado NaN ... ... ... ... ... ... ... ... ... ... 332204 387333 CC 12696041 901549079 Trabajador 2024-02-07 A empleado NaN 332205 387334 CC 1002321162 901549079 Trabajador 2024-01-31 P empleado NaN 332206 387334 CC 1002321162 901549079 Trabajador 2024-02-09 A empleado NaN 332207 387336 CC 1082879603 800104552 Trabajador 2024-01-31 P empleado NaN 332208 387336 CC 1082879603 800104552 Trabajador 2024-02-07 A empleado NaN <p>332209 rows \u00d7 9 columns</p>"},{"location":"seccion/1.9-Dim_Parametricas/#funcion-para-almacenar-datos-de-trabajadores-de-mercurio-en-el-data-warehouse","title":"Funci\u00f3n para Almacenar Datos de Trabajadores de Mercurio en el Data Warehouse","text":"<p>El c\u00f3digo define una funci\u00f3n que carga los datos del DataFrame <code>df_t</code> y los guarda en una tabla del Data Warehouse llamada <code>Fact_Mercurio_Trabajadores</code>. Para evitar la colisi\u00f3n de nombres entre la funci\u00f3n y el DataFrame, se hace una asignaci\u00f3n clara, separando el DataFrame bajo un nombre distinto. La funci\u00f3n utiliza <code>guardar_en_dwh</code> para a\u00f1adir los datos de forma eficiente, asegurando que se agreguen a la tabla sin sobrescribir los datos existentes, controlando el flujo con el registro adecuado.</p> <pre><code>def Mercurio_Trabajadores(_metod): \n    guardar_en_dwh(df_t, 'Fact_Mercurio_Trabajadores', logger, multiple=False, if_exists=_metod)\n    logger.info(f'Tiempo Fact_Mercurio_Trabajadores --- {time.time() - parametica_time:.2f} seconds ---')\n\nMercurio_Trabajadores('replace')\n</code></pre> <pre><code>2024-10-25 22:22:14,291 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:22:14,293 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:22:14,808 - INFO - Almacenando tabla \u00fanica en DWH como Fact_Mercurio_Trabajadores\n2024-10-25 22:22:45,242 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:22:45,495 - INFO - ALMACENAMIENTO ---  --- 31.20 seconds ---\n2024-10-25 22:22:45,497 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:22:45,498 - INFO - Tiempo Fact_Mercurio_Trabajadores --- 239.26 seconds ---\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#errores_de_grabacion","title":"Errores_de_Grabaci\u00f3n","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-de-datos-desde-un-archivo-excel","title":"Carga de Datos desde un Archivo Excel","text":"<p>Este c\u00f3digo utiliza la funci\u00f3n <code>pd.read_excel()</code> de Pandas para cargar un archivo Excel llamado <code>Errores_de_Grabaci\u00f3n.xlsx</code> desde el directorio <code>../Parametricas</code>. El contenido del archivo se almacena en el DataFrame <code>df</code>, y luego se imprime para verificar que los datos han sido cargados correctamente. Esta operaci\u00f3n es \u00fatil para analizar o procesar los errores de grabaci\u00f3n registrados en el archivo Excel.</p> <pre><code>parametica_time = time.time()\ndf = pd.read_excel(\"../Parametricas/Errores_de_Grabaci\u00f3n.xlsx\")\ndf\n</code></pre> Muestra a revisar Error en escritura Afiliaci\u00f3n en el formulario sin anexos, ni grabado en sistema Error en grabaci\u00f3n para pago de subsidio periodo Grabador 0 0 0 0 0 202307 GRABADOR N\u00b0 1 1 142 11 0 2 202307 GRABADOR N\u00b0 2 2 0 0 0 0 202307 GRABADOR N\u00b0 3 3 146 15 0 0 202307 GRABADOR N\u00b0 4 4 146 13 0 1 202307 GRABADOR N\u00b0 5 5 140 10 0 1 202307 GRABADOR N\u00b0 6 6 136 12 0 0 202307 GRABADOR N\u00b0 7 7 0 0 0 0 202307 GRABADOR N\u00b0 8"},{"location":"seccion/1.9-Dim_Parametricas/#renombrado-de-columnas-y-almacenamiento-de-errores-en-data-warehouse","title":"Renombrado de Columnas y Almacenamiento de Errores en Data Warehouse","text":"<p>Este bloque de c\u00f3digo renombra una columna en el DataFrame <code>df</code> para usar un nombre m\u00e1s corto y claro, facilitando su manipulaci\u00f3n. El DataFrame resultante, <code>Errores_grabacion</code>, contiene los datos cargados desde un archivo Excel que registra errores de grabaci\u00f3n. La funci\u00f3n <code>Mercurio_Trabajadores</code> se utiliza para guardar estos datos en una tabla llamada <code>DB_Dim_Errores_grabacion</code> dentro del Data Warehouse. La funci\u00f3n permite especificar el m\u00e9todo de inserci\u00f3n de datos mediante el par\u00e1metro <code>_metod</code>, que puede ser 'replace' para agregar nuevos registros o 'replace' para reemplazar los existentes en la tabla.</p> <pre><code># Renombra las columnas a nombres m\u00e1s cortos\ndf.rename(columns={'Afiliaci\u00f3n en el formulario sin anexos, ni grabado en sistema ': 'Afiliaci\u00f3n sin anexos ni grabaci\u00f3n'}, inplace=True)\nErrores_grabacion  = df\n\ndef Mercurio_Trabajadores(_metod): \n    convertir_columnas_mayusculas(Errores_grabacion)\n    guardar_en_dwh(Errores_grabacion, 'BD_Dim_Errores_grabacion', logger, multiple=False, if_exists=_metod)\n    logger.info(f'Tiempo BD_Dim_Errores_grabacion --- {time.time() - parametica_time:.2f} seconds ---')\n\nMercurio_Trabajadores('replace')\nErrores_grabacion.columns.tolist()\n</code></pre> <pre><code>2024-10-25 22:23:34,619 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:23:34,621 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:23:35,144 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Errores_grabacion\n2024-10-25 22:23:36,055 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:23:36,180 - INFO - ALMACENAMIENTO ---  --- 1.56 seconds ---\n2024-10-25 22:23:36,181 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:23:36,183 - INFO - Tiempo BD_Dim_Errores_grabacion --- 7.77 seconds ---\n\n\n\n\n\n['MUESTRA A REVISAR',\n 'ERROR EN ESCRITURA',\n 'AFILIACI\u00d3N SIN ANEXOS NI GRABACI\u00d3N',\n 'ERROR EN GRABACI\u00d3N PARA PAGO DE SUBSIDIO',\n 'PERIODO',\n 'GRABADOR']\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#proyeccion_cumpl_aportes","title":"Proyeccion_cumpl_aportes","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-proyeccion-de-cumplimiento-de-aportes","title":"Carga y Almacenamiento de Proyecci\u00f3n de Cumplimiento de Aportes","text":"<p>Este c\u00f3digo carga un archivo Excel llamado <code>Proyeccion_cumpl_aportes.xlsx</code> en un DataFrame de Pandas llamado <code>df_proyeccion_cumpl_aportes</code>. Posteriormente, define una funci\u00f3n <code>guardar_proyeccion_cumpl_aportes</code> que utiliza la funci\u00f3n <code>guardar_en_dwh</code> para almacenar los datos en la tabla <code>DB_Dim_Proyeccion_cumpl_aportes</code> en el Data Warehouse. La opci\u00f3n <code>if_exists='replace'</code> asegura que los datos nuevos se agreguen a los registros existentes, sin sobrescribir la tabla.</p> <pre><code>parametica_time = time.time()\ndf_proyeccion_cumpl_aportes = pd.read_excel(\"../Parametricas/Proyeccion_cumpl_aportes.xlsx\")\n\ndef guardar_proyeccion_cumpl_aportes(if_exists='replace'):\n    convertir_columnas_mayusculas(df_proyeccion_cumpl_aportes)\n    guardar_en_dwh(df_proyeccion_cumpl_aportes, 'BD_Dim_Proyeccion_cumpl_aportes', logger, multiple=False, if_exists=if_exists)\n    logger.info(f'Tiempo BD_Dim_Proyeccion_cumpl_aportes --- {time.time() - parametica_time:.2f} seconds ---')\n\nguardar_proyeccion_cumpl_aportes()\ndf_proyeccion_cumpl_aportes\n</code></pre> <pre><code>2024-10-25 22:24:09,616 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:24:09,619 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:24:10,145 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Proyeccion_cumpl_aportes\n2024-10-25 22:24:11,139 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:24:11,261 - INFO - ALMACENAMIENTO ---  --- 1.65 seconds ---\n2024-10-25 22:24:11,263 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:24:11,264 - INFO - Tiempo BD_Dim_Proyeccion_cumpl_aportes --- 1.68 seconds ---\n</code></pre> A\u00d1O PERIODO PROYECCI\u00d3N 0 2023 Enero 1.011240e+10 1 2023 Febrero 1.095939e+10 2 2023 Marzo 1.341095e+10 3 2023 Abril 1.054549e+10 4 2023 Mayo 1.264176e+10 5 2023 Junio 1.259030e+10 6 2023 Julio 1.386005e+10 7 2023 Agosto 1.364761e+10 8 2023 Septiembre 1.185319e+10 9 2023 Octubre 1.350863e+10 10 2023 Noviembre 1.241680e+10 11 2023 Diciembre 1.686632e+10"},{"location":"seccion/1.9-Dim_Parametricas/#metricas","title":"Metricas","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-metricas-en-el-data-warehouse","title":"Carga y Almacenamiento de M\u00e9tricas en el Data Warehouse","text":"<p>Este bloque de c\u00f3digo carga un archivo Excel llamado <code>Metricas.xlsx</code> en el DataFrame <code>df_n</code>. Luego, define la funci\u00f3n <code>Metricas</code>, que guarda el contenido de este DataFrame en la tabla <code>DB_Dim_Metricas</code> del Data Warehouse utilizando la funci\u00f3n <code>guardar_en_dwh</code>. La opci\u00f3n <code>if_exists='replace'</code> asegura que los datos se a\u00f1adan a la tabla existente, sin sobrescribir registros previos.</p> <pre><code>parametica_time = time.time()\ndf_n = pd.read_excel(\"../Parametricas/Metricas.xlsx\")\n\ndef Metricas(if_exists='replace'): \n    convertir_columnas_mayusculas(df_n)\n    guardar_en_dwh(df_n, 'BD_Dim_Metricas', logger, multiple=False, if_exists=if_exists)\n    logger.info(f'Tiempo BD_Dim_Metricas --- {time.time() - parametica_time:.2f} seconds ---')\n\nMetricas(if_exists='replace')\ndf_n\n</code></pre> <pre><code>2024-10-25 22:24:54,149 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:24:54,150 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:24:54,676 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Metricas\n2024-10-25 22:24:55,542 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:24:55,685 - INFO - ALMACENAMIENTO ---  --- 1.54 seconds ---\n2024-10-25 22:24:55,687 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:24:55,689 - INFO - Tiempo BD_Dim_Metricas --- 1.60 seconds ---\n\n\n         METRICA ESPECIFICO                               NOMBRE  \\\n0       Num\u00e9rico     Conteo                       # Afiliaciones   \n1       Num\u00e9rico     Conteo                            # Retiros   \n2       Num\u00e9rico     Conteo                            # Activos   \n3    Indicadores        NaN        Cobertura Proyecci\u00f3n Empresas   \n4    Indicadores        NaN    Cobertura Proyecci\u00f3n Trabajadores   \n5    Indicadores        NaN      Afiliaciones Oportunas Empresas   \n6    Indicadores        NaN  Afiliaciones Oportunas Trabajadores   \n7   Satisfacci\u00f3n        NaN             # Satisfechos Afiliaci\u00f3n   \n8   Satisfacci\u00f3n        NaN           # Insatisfechos Afiliaci\u00f3n   \n9   Satisfacci\u00f3n        NaN     Promedio Satisfacci\u00f3n Afiliaci\u00f3n   \n10   Indicadores        NaN                  Incremento Ingresos   \n11   Indicadores        NaN                 Cumplimiento Aportes   \n12      Num\u00e9rico     Conteo                 # Aportes Entregados   \n13      Num\u00e9rico     Conteo   # Empresas Aportantes No Afiliadas   \n14      Num\u00e9rico     Conteo                   # Empresas En Mora   \n15      Num\u00e9rico  Monetario                      $ Aportes Total   \n16      Num\u00e9rico  Monetario                            $ En Mora   \n17      Num\u00e9rico  Monetario           $ Aportes Emp No Afiliadas   \n18      Num\u00e9rico  Monetario                         $ Recuperado   \n19      Num\u00e9rico     Conteo            # Beneficiarios Con Cuota   \n20      Num\u00e9rico     Conteo                              # Giros   \n21      Num\u00e9rico  Monetario                  $ Cuotas Entregadas   \n22      Num\u00e9rico  Monetario                            $ Recobro   \n23   Indicadores        NaN         Calidad Pago Cuota Monetaria   \n24  Satisfacci\u00f3n        NaN                  # Satisfechos Cuota   \n25  Satisfacci\u00f3n        NaN                # Insatisfechos Cuota   \n26  Satisfacci\u00f3n        NaN          Promedio Satisfacci\u00f3n Cuota   \n27      Num\u00e9rico  Monetario                   Subsidio Prescrito   \n28      Num\u00e9rico        NaN                          # Servicios   \n29   Indicadores        NaN                 Incremento Servicios\n\n            PROCESO  \n0        Afiliaci\u00f3n  \n1        Afiliaci\u00f3n  \n2        Afiliaci\u00f3n  \n3        Afiliaci\u00f3n  \n4        Afiliaci\u00f3n  \n5        Afiliaci\u00f3n  \n6        Afiliaci\u00f3n  \n7        Afiliaci\u00f3n  \n8        Afiliaci\u00f3n  \n9        Afiliaci\u00f3n  \n10          Aportes  \n11          Aportes  \n12          Aportes  \n13          Aportes  \n14          Aportes  \n15          Aportes  \n16          Aportes  \n17          Aportes  \n18          Aportes  \n19  Cuota Monetaria  \n20  Cuota Monetaria  \n21  Cuota Monetaria  \n22  Cuota Monetaria  \n23  Cuota Monetaria  \n24  Cuota Monetaria  \n25  Cuota Monetaria  \n26  Cuota Monetaria  \n27  Cuota Monetaria  \n28        Servicios  \n29        Servicios\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#parametricatipovinculacion","title":"ParametricaTipoVinculacion","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-la-parametrica-de-tipo-de-vinculacion","title":"Carga y Almacenamiento de la Param\u00e9trica de Tipo de Vinculaci\u00f3n","text":"<p>Este c\u00f3digo carga el archivo Excel <code>ParametricaTipoVinculacion.xlsx</code> en el DataFrame <code>df_3</code>. Posteriormente, define la funci\u00f3n <code>Proyeccion_cumpl_aportes</code>, que guarda los datos en la tabla <code>Dim_ParametricaTipoVinculacion</code> del Data Warehouse usando la funci\u00f3n <code>guardar_en_dwh</code>. La opci\u00f3n <code>if_exists='replace'</code> asegura que los nuevos registros se agreguen sin sobrescribir los datos existentes.</p> <pre><code>parametica_time = time.time()\ndf_3 = pd.read_excel(\"../Parametricas/ParametricaTipoVinculacion.xlsx\")\n\n\ndef Proyeccion_cumpl_aportes(): \n    guardar_en_dwh(df_3, 'Dim_ParametricaTipoVinculacion', logger, multiple=False, if_exists='replace')\n    logger.info(f'Tiempo Dim_ParametricaTipoVinculacion --- {time.time() - parametica_time:.2f} seconds ---')\n\nProyeccion_cumpl_aportes()\ndf_3\n</code></pre> <pre><code>2024-10-25 22:26:08,053 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:26:08,055 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:26:08,581 - INFO - Almacenando tabla \u00fanica en DWH como Dim_ParametricaTipoVinculacion\n2024-10-25 22:26:09,494 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:26:09,621 - INFO - ALMACENAMIENTO ---  --- 1.57 seconds ---\n2024-10-25 22:26:09,623 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:26:09,624 - INFO - Tiempo Dim_ParametricaTipoVinculacion --- 1.60 seconds ---\n</code></pre> Tipo Agrupador1 Agrupador2 0 C CONCEJALES CONCEJALES 1 E EMPRESA EMPRESA 2 S SERVICIO DOMESTICO SERVICIO DOMESTICO 3 M MADRE COMUNITARIA MADRE COMUNITARIA 4 F FACULTATIVO FACULTATIVO 5 O PENSIONADO 0.6% PENSIONADO 6 P PENSIONADO 2% PENSIONADO 7 Q PENSIONADO POR FIDELIDAD 25 A\u00d1OS PENSIONADO 8 R PENSIONADO 0% PENSIONADO 9 I INDEPENDIENTE 2% INDEPENDIENTE 10 Y INDEPENDIENTE 0.6% INDEPENDIENTE 11 ET EMPRESA DEPENDIENTE"},{"location":"seccion/1.9-Dim_Parametricas/#metas_indicadores","title":"Metas_indicadores","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-metas-e-indicadores-en-el-data-warehouse","title":"Carga y Almacenamiento de Metas e Indicadores en el Data Warehouse","text":"<p>Este c\u00f3digo carga el archivo Excel <code>Metas_indicadores.xlsx</code> en el DataFrame <code>df_4</code>. Luego, define la funci\u00f3n <code>Metas_indicadores</code>, que guarda los datos en la tabla <code>DB_Dim_Metas_indicadores</code> del Data Warehouse. El par\u00e1metro <code>_metod</code> permite especificar si los datos deben agregarse (<code>replace</code>) o reemplazarse (<code>replace</code>) en la tabla, proporcionando flexibilidad en la gesti\u00f3n del almacenamiento.</p> <pre><code>parametica_time = time.time()\ndf_4 = pd.read_excel(\"../Parametricas/Metas_indicadores.xlsx\")\n\n\ndef Metas_indicadores(_metod): \n    convertir_columnas_mayusculas(df_4)\n    guardar_en_dwh(df_4, 'BD_Dim_Metas_indicadores', logger, multiple=False, if_exists=_metod)\n    logger.info(f'Tiempo BD_Dim_Metas_indicadores --- {time.time() - parametica_time:.2f} seconds ---')\n\nMetas_indicadores('replace')\ndf_4\n</code></pre> <pre><code>2024-10-25 22:26:28,631 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:26:28,634 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:26:29,155 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Metas_indicadores\n2024-10-25 22:26:30,000 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:26:30,149 - INFO - ALMACENAMIENTO ---  --- 1.52 seconds ---\n2024-10-25 22:26:30,150 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:26:30,152 - INFO - Tiempo BD_Dim_Metas_indicadores --- 1.63 seconds ---\n</code></pre> INDICADOR A\u00d1O META 0 Afiliaciones oportunas empresas 2022 0.9000 1 Afiliaciones oportunas empresas 2023 0.9000 2 Afiliaciones oportunas empresas 2024 0.9000 3 Afiliaciones oportunas empresas 2025 0.9000 4 Afiliaciones oportunas empresas 2026 0.9000 5 Afiliaciones oportunas trabajadores 2022 0.7500 6 Afiliaciones oportunas trabajadores 2023 0.7500 7 Afiliaciones oportunas trabajadores 2024 0.8000 8 Afiliaciones oportunas trabajadores 2025 0.8000 9 Afiliaciones oportunas trabajadores 2026 0.8500 10 Incremento de ingresos por aportes 2022 0.0700 11 Incremento de ingresos por aportes 2023 0.0543 12 Incremento de ingresos por aportes 2024 0.0506 13 Incremento de ingresos por aportes 2025 0.0566 14 Incremento de ingresos por aportes 2026 0.0466 15 Cumplimiento de los aportes 2022 0.9500 16 Cumplimiento de los aportes 2023 0.9500 17 Cumplimiento de los aportes 2024 0.9500 18 Cumplimiento de los aportes 2025 0.9500 19 Cumplimiento de los aportes 2026 0.9500 20 Calidad pago cuota monetaria 2022 0.9000 21 Calidad pago cuota monetaria 2023 0.9000 22 Calidad pago cuota monetaria 2024 0.9000 23 Calidad pago cuota monetaria 2025 0.9500 24 Calidad pago cuota monetaria 2026 0.9500 25 Oportunidad pago cuota monetaria 2022 0.9000 26 Oportunidad pago cuota monetaria 2023 0.9000 27 Oportunidad pago cuota monetaria 2024 0.9000 28 Oportunidad pago cuota monetaria 2025 0.9500 29 Oportunidad pago cuota monetaria 2026 0.9500 30 Recuperaci\u00f3n cartera 30 d\u00edas 2022 0.0500 31 Recuperaci\u00f3n cartera 30 d\u00edas 2023 0.0500 32 Recuperaci\u00f3n cartera 30 d\u00edas 2024 0.0500 33 Recuperaci\u00f3n cartera 30 d\u00edas 2025 0.0800 34 Recuperaci\u00f3n cartera 30 d\u00edas 2026 0.0800 35 Recuperaci\u00f3n cartera 60 d\u00edas 2022 0.0300 36 Recuperaci\u00f3n cartera 60 d\u00edas 2023 0.0300 37 Recuperaci\u00f3n cartera 60 d\u00edas 2024 0.0300 38 Recuperaci\u00f3n cartera 60 d\u00edas 2025 0.0500 39 Recuperaci\u00f3n cartera 60 d\u00edas 2026 0.0500 40 Recuperaci\u00f3n cartera 90 d\u00edas 2022 0.0100 41 Recuperaci\u00f3n cartera 90 d\u00edas 2023 0.0100 42 Recuperaci\u00f3n cartera 90 d\u00edas 2024 0.0100 43 Recuperaci\u00f3n cartera 90 d\u00edas 2025 0.0300 44 Recuperaci\u00f3n cartera 90 d\u00edas 2026 0.0300"},{"location":"seccion/1.9-Dim_Parametricas/#metasencuestassuper","title":"MetasEncuestasSuper","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-metas-de-encuestas-en-el-data-warehouse","title":"Carga y Almacenamiento de Metas de Encuestas en el Data Warehouse","text":"<p>El c\u00f3digo carga el archivo Excel <code>MetasEncuestasSuper.xlsx</code> en el DataFrame <code>df_5</code> y luego define la funci\u00f3n <code>MetasEncuestasSuper</code>, que guarda los datos en la tabla <code>DB_Dim_MetasEncuestasSuper</code> del Data Warehouse. El par\u00e1metro <code>_metod</code> permite controlar si los datos se deben agregar a la tabla existente (<code>replace</code>) o reemplazar los datos anteriores (<code>replace</code>), garantizando la flexibilidad en la actualizaci\u00f3n de la informaci\u00f3n almacenada.</p> <pre><code>parametica_time = time.time()\ndf_5 = pd.read_excel(\"../Parametricas/MetasEncuestasSuper.xlsx\")\n\ndef MetasEncuestasSuper(_metod): \n    convertir_columnas_mayusculas(df_5)\n    guardar_en_dwh(df_5, 'BD_Dim_MetasEncuestasSuper', logger, multiple=False, if_exists=_metod)\n    logger.info(f'Tiempo BD_Dim_MetasEncuestasSuper --- {time.time() - parametica_time:.2f} seconds ---')\n\nMetasEncuestasSuper('replace')\ndf_5.columns.tolist()\n</code></pre> <pre><code>2024-10-25 22:26:57,657 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:26:57,659 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:26:58,187 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_MetasEncuestasSuper\n2024-10-25 22:26:59,328 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:26:59,482 - INFO - ALMACENAMIENTO ---  --- 1.82 seconds ---\n2024-10-25 22:26:59,501 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:26:59,502 - INFO - Tiempo BD_Dim_MetasEncuestasSuper --- 1.89 seconds ---\n\n\n\n\n\n['A\u00d1O',\n 'MIN BENEFICIARIOS',\n 'MAX BENEFICIARIOS',\n 'MIN ENCUESTAS',\n 'MAX ENCUESTAS']\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#metascuota","title":"MetasCuota","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-metas-de-cuota-en-el-data-warehouse","title":"Carga y Almacenamiento de Metas de Cuota en el Data Warehouse","text":"<p>Este c\u00f3digo carga el archivo Excel <code>MetasCuota.xlsx</code> en el DataFrame <code>df_6</code> y define la funci\u00f3n <code>MetasCuota</code>, que guarda los datos en la tabla <code>DB_Dim_MetasCuota</code> del Data Warehouse. El par\u00e1metro <code>_metod</code> permite especificar si los datos deben agregarse a la tabla existente (<code>replace</code>) o reemplazarse (<code>replace</code>), ofreciendo flexibilidad en la gesti\u00f3n de los datos almacenados.</p> <pre><code>parametica_time = time.time()\ndf_6 = pd.read_excel(\"../Parametricas/MetasCuota.xlsx\")\n\ndef MetasCuota(_metod): \n    convertir_columnas_mayusculas(df_6)\n    guardar_en_dwh(df_6, 'BD_Dim_MetasCuota', logger, multiple=False, if_exists=_metod)\n    logger.info(f'Tiempo BD_Dim_MetasCuota --- {time.time() - parametica_time:.2f} seconds ---')\n\nMetasCuota('replace')\ndf_6\n</code></pre> <pre><code>2024-10-25 22:27:30,714 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:27:30,716 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:27:31,239 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_MetasCuota\n2024-10-25 22:27:32,306 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:27:32,409 - INFO - ALMACENAMIENTO ---  --- 1.69 seconds ---\n2024-10-25 22:27:32,410 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:27:32,411 - INFO - Tiempo BD_Dim_MetasCuota --- 1.75 seconds ---\n</code></pre> A\u00d1O MES 1 2 3 4 UNNAMED: 6 5 UNNAMED: 8 6 ... 25 26 UNNAMED: 45 27 UNNAMED: 47 28 UNNAMED: 49 29 30 31 0 2023 ENERO NaN NaN NaN NaN NaN NaN NaN 1.0 ... NaN NaN NaN NaN NaN NaN NaN NaN 1.0 NaN 1 2023 FEBRERO NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 1.0 NaN NaN NaN NaN 2 2023 MARZO NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 1.0 NaN NaN NaN NaN 3 2023 ABRIL NaN NaN NaN NaN NaN 1.0 NaN NaN ... NaN NaN NaN NaN NaN 1.0 NaN NaN NaN NaN 4 2023 MAYO NaN NaN NaN NaN NaN 1.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 1.0 NaN 5 2023 JUNIO NaN NaN NaN NaN NaN NaN NaN NaN ... NaN 1.0 NaN NaN NaN NaN NaN NaN 1.0 NaN 6 2023 JULIO NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 7 2023 AGOSTO NaN NaN NaN 1.0 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN 1.0 NaN 8 2023 SEPTIEMBRE NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN 9 2023 OCTUBRE NaN NaN NaN NaN NaN NaN NaN 1.0 ... NaN NaN NaN NaN NaN NaN NaN NaN 1.0 NaN 10 2023 NOVIEMBRE NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN 11 2023 DICIEMBRE NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN <p>12 rows \u00d7 53 columns</p>"},{"location":"seccion/1.9-Dim_Parametricas/#coordenadas","title":"Coordenadas","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-coordenadas-en-el-data-warehouse","title":"Carga y Almacenamiento de Coordenadas en el Data Warehouse","text":"<p>Este c\u00f3digo carga el archivo Excel <code>Coordenadas.xlsx</code> en el DataFrame <code>df_7</code> y define la funci\u00f3n <code>Coordenadas</code>, la cual guarda los datos en la tabla <code>DB_Dim_Coordenadas</code> del Data Warehouse. El par\u00e1metro <code>if_exists</code> controla si los datos se deben agregar a los registros existentes (<code>replace</code>) o reemplazar la informaci\u00f3n almacenada anteriormente (<code>replace</code>), permitiendo la gesti\u00f3n eficiente de los datos de coordenadas.</p> <pre><code>parametica_time = time.time()\ndf_7 = pd.read_excel(\"../Parametricas/Coordenadas.xlsx\")\ncolumnas = {\n        'CODIGOZONA':'CODIGO_ZONA', \n        'NOMBREZONA':'NOMBRE_ZONA', \n        'ZONACOMPLETA':'ZONA_COMPLETA'\n        }\ndf = convertir_columnas_mayusculas(df_7,rename_dict=columnas)\ndef Coordenadas(if_exists): \n    guardar_en_dwh(df, 'BD_Dim_Coordenadas', logger, multiple=False, if_exists=if_exists)\n    logger.info(f'Tiempo BD_Dim_Coordenadas --- {time.time() - parametica_time:.2f} seconds ---')\n\nCoordenadas('replace')\ndf.columns.tolist()\n</code></pre> <pre><code>2024-10-25 22:29:16,096 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:29:16,098 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:29:16,616 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Coordenadas\n2024-10-25 22:29:17,547 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:29:17,701 - INFO - ALMACENAMIENTO ---  --- 1.61 seconds ---\n2024-10-25 22:29:17,753 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:29:17,755 - INFO - Tiempo BD_Dim_Coordenadas --- 1.70 seconds ---\n\n\n\n\n\n['CODIGO_ZONA', 'NOMBRE_ZONA', 'ZONACOMPLETA', 'LATITUDE', 'LONGITUDE']\n</code></pre>"},{"location":"seccion/1.9-Dim_Parametricas/#meta_cobertura_afiliacion","title":"Meta_cobertura_afiliacion","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-metas-de-proyeccion-de-cobertura-y-afiliacion-en-el-data-warehouse","title":"Carga y Almacenamiento de Metas de Proyecci\u00f3n de Cobertura y Afiliaci\u00f3n en el Data Warehouse","text":"<p>El c\u00f3digo carga el archivo Excel <code>Meta_proyeccion_cobertura_afiliacion.xlsx</code> en el DataFrame <code>df_8</code> y define la funci\u00f3n <code>Meta_proyeccion_cobertura_afiliacion</code>, que guarda los datos en la tabla <code>BD_Dim_Meta_cobertura_afiliacion</code> del Data Warehouse. El par\u00e1metro <code>if_exists</code> permite elegir entre agregar los datos a la tabla existente (<code>replace</code>) o reemplazar los datos actuales (<code>replace</code>), proporcionando flexibilidad en la actualizaci\u00f3n de la tabla.</p> <pre><code>parametica_time = time.time()\ndf_8 = pd.read_excel(\"../Parametricas/Meta_proyeccion_cobertura_afiliacion.xlsx\")\n\n\ndef Meta_proyeccion_cobertura_afiliacion(if_exists): \n    df = convertir_columnas_mayusculas(df_8)\n    guardar_en_dwh(df, 'BD_Dim_Meta_cobertura_afiliacion', logger, multiple=False, if_exists=if_exists)\n    logger.info(f'Tiempo BD_Dim_Meta_cobertura_afiliacion --- {time.time() - parametica_time:.2f} seconds ---')\n\nMeta_proyeccion_cobertura_afiliacion('replace')\ndf\n</code></pre> <pre><code>2024-10-25 22:29:46,031 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:29:46,033 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:29:46,560 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Meta_cobertura_afiliacion\n2024-10-25 22:29:47,500 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:29:47,620 - INFO - ALMACENAMIENTO ---  --- 1.59 seconds ---\n2024-10-25 22:29:47,622 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:29:47,623 - INFO - Tiempo BD_Dim_Meta_cobertura_afiliacion --- 1.68 seconds ---\n</code></pre> CODIGO_ZONA NOMBRE_ZONA ZONACOMPLETA LATITUDE LONGITUDE 0 47001 SANTA MARTA SANTA MARTA, COLOMBIA 11.240355 -74.211023 1 47288 FUNDACION FUNDACION, COLOMBIA 10.449279 -73.865429 2 47030 ALGARROBO ALGARROBO, COLOMBIA 10.100040 -74.266630 3 47053 ARACATACA ARACATACA, COLOMBIA 10.591925 -74.186103 4 47058 ARIGUANI ARIGUANI, COLOMBIA 9.844197 -74.228119 5 47161 CERRO DE SAN ANTONIO CERRO DE SAN ANTONIO, COLOMBIA 10.327091 -74.869002 6 47170 CHIVOLO CHIVOLO, COLOMBIA 10.027447 -74.623845 7 47189 CIENAGA CIENAGA, COLOMBIA 11.007984 -74.248926 8 47205 CONCORDIA CONCORDIA, COLOMBIA 6.072311 -75.923098 9 47245 EL BANCO EL BANCO, COLOMBIA 9.138144 -73.983003 10 47258 EL PINON EL PINON, COLOMBIA 10.404038 -74.823769 11 47268 EL RETEN EL RETEN, COLOMBIA 10.611793 -74.269383 12 47318 GUAMAL GUAMAL, COLOMBIA 3.880475 -73.769877 13 47460 NUEVA GRANADA NUEVA GRANADA, COLOMBIA 9.800020 -74.391264 14 47541 PEDRAZA PEDRAZA, COLOMBIA 10.188806 -74.915762 15 47545 PIJINO DEL CARMEN PIJINO DEL CARMEN, COLOMBIA 9.330239 -74.453295 16 47551 PIVIJAY PIVIJAY, COLOMBIA 10.462231 -74.616859 17 47555 PLATO PLATO, COLOMBIA 9.791337 -74.797525 18 47570 PUEBLOVIEJO PUEBLOVIEJO, COLOMBIA 10.992353 -74.284038 19 47605 REMOLINOS REMOLINOS, COLOMBIA 5.662310 -74.793830 20 47660 SABANAS DE SAN ANGEL SABANAS DE SAN ANGEL, COLOMBIA 10.029770 -74.215310 21 47675 SALAMINA SALAMINA, COLOMBIA 5.402354 -75.486468 22 47692 SAN SEBASTIAN BUENAVISTA SAN SEBASTIAN BUENAVISTA, COLOMBIA 8.995635 -74.585474 23 47703 SAN ZENON SAN ZENON, COLOMBIA 9.244884 -74.498450 24 47707 SANTA ANA SANTA ANA, COLOMBIA 9.322961 -74.570544 25 47720 SANTA BARBARA DE PINTO SANTA BARBARA DE PINTO, COLOMBIA 9.432340 -74.704930 26 47745 SITIONUEVO SITIONUEVO, COLOMBIA 10.776607 -74.720902 27 47798 TENERIFE TENERIFE, COLOMBIA 9.898413 -74.858425 28 47960 ZAPAYAN ZAPAYAN, COLOMBIA 4.570868 -74.297333 29 47980 ZONA BANANERA ZONA BANANERA, COLOMBIA 10.792245 -74.170970"},{"location":"seccion/1.9-Dim_Parametricas/#indicadores","title":"Indicadores","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-indicadores-en-el-data-warehouse","title":"Carga y Almacenamiento de Indicadores en el Data Warehouse","text":"<p>Este c\u00f3digo carga el archivo Excel <code>Indicadores.xlsx</code> en el DataFrame <code>df_11</code> y define la funci\u00f3n <code>Indicadores</code>, que guarda los datos en la tabla <code>DB_Dim_Indicadores</code> del Data Warehouse. La funci\u00f3n est\u00e1 configurada para agregar los datos a la tabla existente usando la opci\u00f3n <code>if_exists='replace'</code>, lo que garantiza que los nuevos registros se sumen a los ya almacenados sin sobrescribir la informaci\u00f3n previa.</p> <pre><code>parametica_time = time.time()\ndf_11 = pd.read_excel(\"../Parametricas/Indicadores.xlsx\")\n\ndef Indicadores(): \n    df = convertir_columnas_mayusculas(df_11)\n    guardar_en_dwh(df, 'BD_Dim_Indicadores', logger, multiple=False, if_exists='replace')\n    logger.info(f'Tiempo BD_Dim_Indicadores --- {time.time() - parametica_time:.2f} seconds ---')\n\nIndicadores()\ndf_11\n</code></pre> <pre><code>2024-10-25 22:30:50,072 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:30:50,073 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:30:50,596 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Indicadores\n2024-10-25 22:30:51,609 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:30:51,740 - INFO - ALMACENAMIENTO ---  --- 1.67 seconds ---\n2024-10-25 22:30:51,741 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:30:51,742 - INFO - Tiempo BD_Dim_Indicadores --- 1.72 seconds ---\n</code></pre> INDICADOR MOTIVO PERIODO CANTIDAD 0 Solicitudes de Afiliaci\u00f3n devueltas M1 202307 422.00 1 Solicitudes de Afiliaci\u00f3n devueltas M2 202307 60.00 2 Solicitudes de Afiliaci\u00f3n devueltas M3 202307 444.00 3 Solicitudes de Afiliaci\u00f3n devueltas M4 202307 463.00 4 Solicitudes de Afiliaci\u00f3n devueltas M5 202307 440.00 ... ... ... ... ... 190 Gesti\u00f3n PQRSF M1 202307 0.15 191 Gesti\u00f3n PQRSF M2 202307 NaN 192 Gesti\u00f3n PQRSF M3 202307 NaN 193 Gesti\u00f3n PQRSF M4 202307 NaN 194 Gesti\u00f3n PQRSF M5 202307 NaN <p>195 rows \u00d7 4 columns</p>"},{"location":"seccion/1.9-Dim_Parametricas/#satisfaccion_cartera","title":"Satisfacci\u00f3n_cartera","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-satisfaccion-de-cartera-en-el-data-warehouse","title":"Carga y Almacenamiento de Satisfacci\u00f3n de Cartera en el Data Warehouse","text":"<p>Este c\u00f3digo carga el archivo Excel <code>SatisfaccionCartera.xlsx</code> en el DataFrame <code>df_9</code> y define la funci\u00f3n <code>Satisfacci\u00f3n_cartera</code>, que guarda los datos en la tabla <code>DB_Dim_Satisfacci\u00f3n_cartera</code> del Data Warehouse. La funci\u00f3n est\u00e1 configurada para agregar los datos a los registros existentes utilizando <code>if_exists='replace'</code>, asegurando que la informaci\u00f3n previa se mantenga mientras se agregan nuevos registros.</p> <pre><code>parametica_time = time.time()\ndf_9 = pd.read_excel(\"../Parametricas/SatisfaccionCartera.xlsx\")\n\ndef Satisfacci\u00f3n_cartera(): \n    df = convertir_columnas_mayusculas(df_9)    \n    guardar_en_dwh(df, 'BD_Dim_Satisfacci\u00f3n_cartera', logger, multiple=False, if_exists='replace')\n    logger.info(f'Tiempo BD_Dim_Satisfacci\u00f3n_cartera --- {time.time() - parametica_time:.2f} seconds ---')\n\nSatisfacci\u00f3n_cartera()\ndf_9\n</code></pre> <pre><code>2024-10-25 22:31:18,544 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:31:18,546 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:31:19,071 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Satisfacci\u00f3n_cartera\n2024-10-25 22:31:20,083 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:31:20,212 - INFO - ALMACENAMIENTO ---  --- 1.67 seconds ---\n2024-10-25 22:31:20,213 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:31:20,214 - INFO - Tiempo BD_Dim_Satisfacci\u00f3n_cartera --- 1.72 seconds ---\n</code></pre> PREGUNTAS PERIODO CALIFICACI\u00d3N 0 \u00bfLa informaci\u00f3n entregada en la comunicaci\u00f3n p... 202207 5.0 1 \u00bfConsidera que le falta informaci\u00f3n para reali... 202207 4.2 2 \u00bfTuvo problemas de acceso a la plataforma del ... 202207 3.7 3 Si present\u00f3 alguna duda o inquietud, \u00bfle fue f... 202207 3.1 4 \u00bfLa informaci\u00f3n entregada en la comunicaci\u00f3n p... 202306 3.5 5 \u00bfConsidera que le falta informaci\u00f3n para reali... 202306 3.7 6 \u00bfTuvo problemas de acceso a la plataforma del ... 202306 4.6 7 Si present\u00f3 alguna duda o inquietud, \u00bfle fue f... 202306 4.9 8 \u00bfLa informaci\u00f3n entregada en la comunicaci\u00f3n p... 202307 4.9 9 \u00bfConsidera que le falta informaci\u00f3n para reali... 202307 4.8 10 \u00bfTuvo problemas de acceso a la plataforma del ... 202307 4.6 11 Si present\u00f3 alguna duda o inquietud, \u00bfle fue f... 202307 4.6"},{"location":"seccion/1.9-Dim_Parametricas/#detalle_preguntas_encuestas","title":"Detalle_preguntas_encuestas","text":""},{"location":"seccion/1.9-Dim_Parametricas/#carga-y-almacenamiento-de-detalle-de-preguntas-de-encuestas-en-el-data-warehouse","title":"Carga y Almacenamiento de Detalle de Preguntas de Encuestas en el Data Warehouse","text":"<p>El c\u00f3digo carga el archivo Excel <code>Detalle_preguntas_encuestas.xlsx</code> en el DataFrame <code>df_10</code> y define la funci\u00f3n <code>Detalle_preguntas_encuestas</code>, que guarda los datos en la tabla <code>DB_Dim_Detalle_preguntas_encuestas</code> del Data Warehouse. La opci\u00f3n <code>if_exists='replace'</code> asegura que los datos nuevos se agreguen a los registros existentes sin sobrescribir la informaci\u00f3n almacenada previamente.</p> <pre><code>parametica_time = time.time()\ndf_10 = pd.read_excel(\"../Parametricas/Detalle_preguntas_encuestas.xlsx\")\n\ndef Detalle_preguntas_encuestas(if_exists='replace'): \n    convertir_columnas_mayusculas(df_10)\n    guardar_en_dwh(df_10, 'BD_Dim_Detalle_preguntas_encuestas', logger, multiple=False, if_exists=if_exists)\n    logger.info(f'Tiempo BD_Dim_Detalle_preguntas_encuestas --- {time.time() - parametica_time:.2f} seconds ---')\n\nDetalle_preguntas_encuestas('replace')\ndf_10\n</code></pre> <pre><code>2024-10-25 22:32:01,932 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 22:32:01,934 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 22:32:02,449 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Detalle_preguntas_encuestas\n2024-10-25 22:32:03,308 - INFO - Tabla almacenada correctamente.\n2024-10-25 22:32:03,443 - INFO - ALMACENAMIENTO ---  --- 1.51 seconds ---\n2024-10-25 22:32:03,444 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-25 22:32:03,446 - INFO - Tiempo BD_Dim_Detalle_preguntas_encuestas --- 1.57 seconds ---\n</code></pre> FUENTE PREGUNTA DETALLE 0 Encuesta_afil_empleador_2022_PB Pregunta1 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 1 Encuesta_afil_empleador_2022_PB Pregunta2 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 2 Encuesta_afil_empleador_2022_PB Pregunta3 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 3 Encuesta_afil_empleador_2022_PB Pregunta4 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 4 Encuesta_afil_empleador_2023_I_sem_PB Pregunta1 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 5 Encuesta_afil_empleador_2023_I_sem_PB Pregunta2 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 6 Encuesta_afil_empleador_2023_I_sem_PB Pregunta3 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 7 Encuesta_afil_empleador_2023_I_sem_PB Pregunta4 \u00bfC\u00d3MO CALIFICA USTED EL SERVICIO DE AFILIACI\u00d3N... 8 Encuesta_afil_empleador_2023_II_sem_PB Pregunta1 \u00bfCOMO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 9 Encuesta_afil_empleador_2023_II_sem_PB Pregunta2 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 10 Encuesta_afil_empleador_2023_II_sem_PB Pregunta3 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 11 Encuesta_afil_empleador_2023_II_sem_PB Pregunta4 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 12 Encuesta_afil_trabajador_2022_PB Pregunta1 \u00bfCOMO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 13 Encuesta_afil_trabajador_2022_PB Pregunta2 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 14 Encuesta_afil_trabajador_2022_PB Pregunta3 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 15 Encuesta_afil_trabajador_2022_PB Pregunta4 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 16 Encuesta_afil_trabajador_2022_PB Pregunta5 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 17 Encuesta_afil_trabajador_2023_I_sem_PB Pregunta1 \u00bfCOMO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 18 Encuesta_afil_trabajador_2023_I_sem_PB Pregunta2 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 19 Encuesta_afil_trabajador_2023_I_sem_PB Pregunta3 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 20 Encuesta_afil_trabajador_2023_I_sem_PB Pregunta4 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 21 Encuesta_afil_trabajador_2023_I_sem_PB Pregunta5 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 22 Encuesta_afil_trabajador_2023_II_sem_PB Pregunta1 \u00bfCOMO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 23 Encuesta_afil_trabajador_2023_II_sem_PB Pregunta2 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 24 Encuesta_afil_trabajador_2023_II_sem_PB Pregunta3 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 25 Encuesta_afil_trabajador_2023_II_sem_PB Pregunta4 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 26 Encuesta_afil_trabajador_2023_II_sem_PB Pregunta5 \u00bfCOMO CALIFICA USTED EL SERVICIO DE AFILIACION... 27 Encuesta_cuota_monetaria_PB Pregunta1 OPORTUNIDAD [\u00bfCONSIDER\u00cdA USTED OPORTUNO EL TIE... 28 Encuesta_cuota_monetaria_PB Pregunta2 OPORTUNIDAD [\u00bfFUE OPORTUNA LA RESPUESTA A SU S... 29 Encuesta_cuota_monetaria_PB Pregunta3 OPORTUNIDAD [\u00bfRECIBE EL PAGO DE LA CUOTA MONET... 30 Encuesta_cuota_monetaria_PB Pregunta4 INFRAESTRUCTURA (FISICA Y VIRTUAL) [CALIFIQUE ... 31 Encuesta_cuota_monetaria_PB Pregunta5 INFRAESTRUCTURA (FISICA Y VIRTUAL) [EVAL\u00daE LA ... 32 Encuesta_cuota_monetaria_PB Pregunta6 INFRAESTRUCTURA (FISICA Y VIRTUAL) [CALIFIQUE ... 33 Encuesta_cuota_monetaria_PB Pregunta7 CALIDEZ [QUE TAN AMABLE FUE LA ATENCION PRESEN... 34 Encuesta_cuota_monetaria_PB Pregunta8 CALIDEZ [\u00bfCONSIDERA USTED QUE LA INFORMACI\u00d3N E... 35 Encuesta_cuota_monetaria_PB Pregunta9 CALIDEZ [\u00bfQUE TAN AMABLE FUE LA ATENCI\u00d3N TELEF... 36 Encuesta_cuota_monetaria_PB Pregunta10 EFECTIVIDAD [\u00bfCREE USTED QUE LA INFORMACI\u00d3N EN... 37 Encuesta_cuota_monetaria_PB Pregunta11 EFECTIVIDAD [\u00bfFUERON RESUELTAS TODAS SUS DUDAS... 38 Encuesta_cuota_monetaria_PB Pregunta12 EFECTIVIDAD [\u00bfEL VALOR DE LA CUOTA MONETARIA C..."},{"location":"seccion/2.1-FactAportesEmpAfiliadas/","title":"2.1 Fact Aportes Emp Afiliadas","text":""},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#intoduccion","title":"Intoducci\u00f3n","text":"<p>El proceso <code>FactAportesEmpAfiliadas</code> consiste en un pipeline ETL que extrae datos de aportes empresariales desde las tablas <code>xml4.xml4c085</code> y <code>subsidio.subsi11</code> en la base de datos <code>MINERVA</code>, transform\u00e1ndolos para integrar informaci\u00f3n sobre valores pagados, intereses, n\u00famero de trabajadores aportantes y otros datos clave. Los registros extra\u00eddos son procesados para eliminar duplicados, limpiar valores inconsistentes y crear claves \u00fanicas, asegurando una estructura consolidada y sin redundancias. Finalmente, los datos transformados se cargan en el data warehouse (DWH) en la tabla <code>BD_Fact_HistoricoAportesEmpAfiliadas</code> para su almacenamiento y an\u00e1lisis, documentando el proceso en un archivo de log para control y monitoreo de eventos.</p>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    autonumber\n    title Diagrama de Secuencia del Proceso ETL - FactAportesEmpAfiliadas\n    participant \ud83d\udc64 Usuario\n    participant ETL as Proceso ETL\n    participant MINERVA as Base de Datos MINERVA\n    participant DWH as Data Warehouse\n    Usuario-&gt;&gt;ETL: Inicia ETL\n    ETL-&gt;&gt;MINERVA: Conexi\u00f3n a Base de Datos MINERVA\n    MINERVA--&gt;&gt;ETL: Confirmaci\u00f3n de Conexi\u00f3n\n    ETL-&gt;&gt;MINERVA: Ejecuci\u00f3n de Consultas en `xml4.xml4c085` y `subsidio.subsi11`\n    MINERVA--&gt;&gt;ETL: Datos de Aportes\n    ETL-&gt;&gt;ETL: Transformaci\u00f3n y Limpieza de Datos\n    ETL-&gt;&gt;DWH: Conexi\u00f3n a DWH\n    DWH--&gt;&gt;ETL: Confirmaci\u00f3n de Conexi\u00f3n\n    ETL-&gt;&gt;DWH: Almacenamiento en `BD_Fact_HistoricoAportesEmpAfiliadas`\n    DWH--&gt;&gt;ETL: Confirmaci\u00f3n de Almacenamiento\n    ETL-&gt;&gt;Usuario: Finalizaci\u00f3n del Proceso ETL</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#etl","title":"ETL","text":""},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque inicializa el entorno de trabajo para manejar datos y conectarse a bases de datos utilizando SQLAlchemy y <code>pymysql</code>. Configura el <code>sys.path</code> para importar funciones personalizadas desde <code>Funciones.py</code>, como <code>guardar_en_dwh</code> para almacenar datos en el data warehouse, <code>StoreDuplicated</code> y <code>RemoveDuplicated</code> para el manejo de duplicados, <code>RemoveErrors</code> para limpiar registros err\u00f3neos, y las funciones de conexi\u00f3n <code>Conexion_Minerva</code> y <code>Conexion_dwh</code> que facilitan la creaci\u00f3n de motores de conexi\u00f3n a distintas bases de datos. La funci\u00f3n <code>testfunciones()</code> se ejecuta como una verificaci\u00f3n inicial, asegurando que todas las funciones necesarias est\u00e1n correctamente importadas y listas para su uso en procesos de carga y limpieza de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, Conexion_Minerva, Conexion_dwh, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 23-10-2024 15:21\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Calendario_Mensual.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='FactAportes.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-23 15:21:46,666 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#conexion-a-la-base-de-datos-minerva","title":"Conexi\u00f3n a la base de datos MINERVA","text":"<p>Se establece la conexi\u00f3n con la base de datos MINERVA utilizando <code>create_engine</code> y la cadena de conexi\u00f3n generada por <code>Conexion_Minerva()</code>. Se registra en el log la confirmaci\u00f3n de la conexi\u00f3n.</p> <pre><code>#Conexion a base Minerva\n# Crea un motor SQLAlchemy\ncadena_conexion = Conexion_Minerva()\nmotor = create_engine(cadena_conexion)\nlogger.info('CONEXION A BASE MINERVA')\n</code></pre> <pre><code>2024-10-23 15:24:14,057 - INFO - CONEXION A BASE MINERVA\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#definicion-de-consulta-sql-para-datos-de-aportes","title":"Definici\u00f3n de Consulta SQL para Datos de Aportes","text":"<p>En este c\u00f3digo, se define una consulta SQL dentro del diccionario <code>qr_structure</code> que extrae y organiza informaci\u00f3n sobre aportes en la tabla <code>aportes</code>. La consulta combina datos de las tablas <code>xml4.xml4c085</code> y <code>subsidio.subsi11</code> utilizando un <code>LEFT JOIN</code>, basado en la coincidencia de columnas <code>nit</code> y <code>periodo</code>. </p> <p>En la primera subconsulta (<code>p1</code>), se generan identificadores \u00fanicos (<code>id</code>) concatenando <code>tipide</code> y <code>nit</code>, y se filtran los registros dentro de los \u00faltimos 18 meses. Esta subconsulta agrupa los valores de aportes y de intereses (<code>valorpag</code> y <code>valorInt</code>) por <code>periodo</code>, <code>nit</code>, y <code>tipapo</code>. La segunda subconsulta (<code>p2</code>) agrega valores de aportes (<code>valorcon</code>), trabajadores aportantes (<code>trabaapo</code>), y clasifica cada registro como \"Empresa\" en la columna <code>Tipo</code>. </p> <p>El resultado de la consulta unificada (<code>b</code>) permite un an\u00e1lisis de los aportes de cada empresa o entidad, facilitando el seguimiento de pagos, periodos referenciados y valores de aportes e intereses. La estructura resultante se almacena en <code>df_structure</code> para uso posterior en an\u00e1lisis o almacenamiento en el sistema de datos.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"aportes\":'''\nselect b.id, b.nit, b.fecpag, b.tipapo, b.periodorefer, b.valorcon, b.valorpag, b.trabaapo, b.Tipo, b.valorInt from\n(select p1.*, p2.* from\n(select CONCAT(CAST(CASE WHEN tipide = 1 THEN 'CC' WHEN tipide = 3 THEN 'RC' WHEN tipide = 4 THEN 'CE' WHEN tipide = 6 THEN 'PA' WHEN tipide = 7 THEN 'NI'            \nELSE tipide END AS CHAR), nit) as id, nit, periodo as fecpag, tipapo, SUM(apomes)  as valorpag, SUM(valint)  as valorInt\nfrom xml4.xml4c085 where (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\nAND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR))\ngroup by periodo, nit, tipapo) as p1\n\nleft join\n\n(select (CONCAT(SUBSTRING(fecpag, 1, 4), SUBSTRING(fecpag, 6, 2))) COLLATE utf8mb4_general_ci as periodo,\n periodo as periodorefer, nit as nit_2, SUM(valcon) as valorcon, SUM(traapo) as trabaapo, 'Empresa' as Tipo\nfrom subsidio.subsi11 group by  (CONCAT(SUBSTRING(fecpag, 1, 4), SUBSTRING(fecpag, 6, 2))) COLLATE utf8mb4_general_ci, nit ) as p2\non p1.nit = p2.nit_2 and p1.fecpag =p2.periodo\n) as b\n;\n'''   \n\n               }\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-23 15:24:30,841 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#carga-de-tablas-desde-sql","title":"Carga de Tablas desde SQL","text":"<p>Este bloque de c\u00f3digo ejecuta la carga de datos desde una base de datos SQL para cada consulta en <code>qr_structure</code>, almacenando los resultados en <code>df_structure</code>. Para cada clave en <code>qr_structure</code>, se establece una conexi\u00f3n a la base de datos utilizando <code>motor.begin()</code> y se ejecuta la consulta SQL correspondiente. El resultado se carga en un DataFrame dentro de <code>df_structure</code>, donde el nombre de cada tabla coincide con la clave utilizada en <code>qr_structure</code>. </p> <p>Se utiliza <code>logger</code> para registrar el nombre de cada tabla cargada y el tiempo total de ejecuci\u00f3n (<code>cargue_time</code>). Este registro proporciona trazabilidad y facilita el monitoreo del proceso de carga. Finalmente, el tiempo total de carga se muestra en el <code>logger</code>, brindando una m\u00e9trica del rendimiento en la extracci\u00f3n de datos desde MySQL.</p> <pre><code># Cargue de tablas desde sql\n\ncargue_time = time.time()\nfor ky in list(qr_structure.keys()):\n    #print(ky)\n    with motor.begin() as conn:\n        df_structure[ky] = pd.read_sql_query(sa.text(qr_structure[ky]), conn)\n    logger.info('CARGUE TABLA: '+ ky)\nlogger.info(f'CARGUE TABLAS DESDE MYSQL --- {time.time() - cargue_time:.2f} seconds ---')# Cargue de tablas desde sql\ncargue_time = time.time()\nfor ky in list(qr_structure.keys()):\n    with motor.begin() as conn:\n        df_structure[ky] = pd.read_sql_query(sa.text(qr_structure[ky]), conn)\nlogger.info(f'CARGUE TABLAS DESDE MYSQL --- {time.time() - cargue_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:25:04,816 - INFO - CARGUE TABLA: aportes\n2024-10-23 15:25:04,819 - INFO - CARGUE TABLAS DESDE MYSQL --- 26.06 seconds ---\n2024-10-23 15:25:15,466 - INFO - CARGUE TABLAS DESDE MYSQL --- 10.65 seconds ---\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#validador-de-campos-repetidos-y-limpieza-de-duplicados","title":"Validador de Campos Repetidos y Limpieza de Duplicados","text":"<p>Este bloque de c\u00f3digo valida y elimina registros duplicados en cada DataFrame de <code>df_structure</code>. Primero, se definen las columnas a comparar (<code>ColumnsToCompare</code>), excluyendo la columna <code>id</code>, y se llama a la funci\u00f3n <code>StoreDuplicated</code> para almacenar cualquier registro duplicado en un archivo con el prefijo <code>trazaDuplicados_</code>. Luego, se eliminan los duplicados dentro de cada DataFrame usando <code>drop_duplicates()</code> para conservar solo las filas \u00fanicas. </p> <p>Cada validaci\u00f3n de tabla se registra en el <code>logger</code>, y al final, se muestra en el log el tiempo total de ejecuci\u00f3n del validador de duplicados, permitiendo monitorear la eficiencia del proceso de limpieza y la trazabilidad de los datos procesados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:25:21,825 - INFO - VALIDADOR TABLA: aportes\n2024-10-23 15:25:21,826 - INFO - VALIDADOR DUPLICADOS --- 0.23 seconds ---\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#transformacion-y-estandarizacion-de-datos-en-df_structure","title":"Transformaci\u00f3n y Estandarizaci\u00f3n de Datos en <code>df_structure</code>","text":"<p>Este bloque de c\u00f3digo aplica transformaciones en <code>df_structure</code> para estandarizar los datos. Primero, convierte todas las columnas de texto a may\u00fasculas, elimina espacios en blanco y reemplaza valores <code>\"NAN\"</code> y <code>\"NONE\"</code> con <code>NaN</code>. Luego, concatena todos los DataFrames en <code>df_structure</code> en uno solo y renombra la columna <code>fecpag</code> como <code>periodo</code>. </p> <p>Se agrega la columna <code>KeyHistoricos</code>, que combina <code>periodo</code>, <code>nit</code>, y los primeros tres caracteres de <code>Tipo</code> para crear una clave \u00fanica. Las columnas se renombran en may\u00fasculas, y algunos nombres espec\u00edficos como <code>ID</code>, <code>TIPAPO</code>, y <code>PERIODOREFER</code> se actualizan para una mayor claridad (por ejemplo, <code>ID</code> se convierte en <code>ID_AFILIADO</code>). Adem\u00e1s, se crea una columna <code>ID_REGISTRO</code> con valores secuenciales para identificar cada fila de forma \u00fanica. Finalmente, se reorganizan las columnas en el orden especificado en <code>columnas_ordenadas</code> para obtener un DataFrame estandarizado (<code>df_final</code>) listo para an\u00e1lisis o carga en un sistema de almacenamiento.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:25:27,856 - INFO - LIMPIEZA --- 1.16 seconds ---\n</code></pre> <pre><code>df_structure = pd.concat(df_structure.values(), ignore_index=True)\n</code></pre> <pre><code>print(df_structure.columns)\n</code></pre> <pre><code>Index(['id', 'nit', 'fecpag', 'tipapo', 'periodorefer', 'valorcon', 'valorpag',\n       'trabaapo', 'Tipo', 'valorInt'],\n      dtype='object')\n</code></pre> <pre><code># Renombrar la columna 'fecpag' a 'periodo'\ndf_structure = df_structure.rename(columns={'fecpag': 'periodo'})\n\n# Agregar la columna 'KeyHistoricos'\ndf_structure['KeyHistoricos'] = df_structure['periodo'] + df_structure['nit'] + df_structure['Tipo'].str[:3]\n</code></pre> <pre><code># Convertir todos los nombres de las columnas de df_final a may\u00fasculas\ndf_structure.columns = df_structure.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf_final = df_structure.rename(columns={    \n            'ID': 'ID_AFILIADO',\n            'TIPAPO': 'TIPO_APORTANTE',\n            'PERIODOREFER': 'PERIODO_REFERENCIA',\n            'VALORCON': 'VALOR_CONSIGNADO',\n            'VALORPAG': 'APORTE_MES',\n            'TRABAAPO': 'NUM_TRAB_POR_APORTE',\n            'VALORINT': 'VALOR_INTERES'\n        })\n</code></pre> <pre><code>print(df_final.columns)\n</code></pre> <pre><code>Index(['ID_AFILIADO', 'NIT', 'PERIODO', 'TIPO_APORTANTE', 'PERIODO_REFERENCIA',\n       'VALOR_CONSIGNADO', 'APORTE_MES', 'NUM_TRAB_POR_APORTE', 'TIPO',\n       'VALOR_INTERES', 'KEYHISTORICOS'],\n      dtype='object')\n</code></pre> <pre><code># Crear una nueva columna 'ID_REGISTRO' con valores secuenciales\ndf_final['ID_REGISTRO'] = range(1, len(df_final) + 1)\n\n# Reorganizar las columnas, colocando 'ID_REGISTRO' de primeras\ncolumnas_ordenadas = ['ID_REGISTRO', 'ID_AFILIADO', 'NIT', 'PERIODO', 'TIPO_APORTANTE', 'PERIODO_REFERENCIA', 'VALOR_CONSIGNADO', \n                      'APORTE_MES', 'NUM_TRAB_POR_APORTE', 'TIPO', 'VALOR_INTERES', 'KEYHISTORICOS']\n\n# Reorganizar el DataFrame\ndf_final = df_final[columnas_ordenadas]\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion2 = Conexion_dwh()\nmotor2 = create_engine(cadena_conexion2)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-23 15:27:43,846 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/2.1-FactAportesEmpAfiliadas/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en la base de datos DWH. Si la tabla ya existe en la base de datos, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoAportesEmpAfiliadas', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\n</code></pre> <pre><code>2024-10-23 15:29:10,074 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-23 15:29:10,074 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-23 15:29:10,326 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoAportesEmpAfiliadas\n2024-10-23 15:29:25,831 - INFO - Tabla almacenada correctamente.\n2024-10-23 15:29:26,137 - INFO - ALMACENAMIENTO ---  --- 16.06 seconds ---\n2024-10-23 15:29:26,140 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['ID_REGISTRO',\n 'ID_AFILIADO',\n 'NIT',\n 'PERIODO',\n 'TIPO_APORTANTE',\n 'PERIODO_REFERENCIA',\n 'VALOR_CONSIGNADO',\n 'APORTE_MES',\n 'NUM_TRAB_POR_APORTE',\n 'TIPO',\n 'VALOR_INTERES',\n 'KEYHISTORICOS']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:29:31,684 - INFO - FINAL ETL --- 466.96 seconds ---\n</code></pre>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/","title":"2.1 Fact Historico Aportes Totales","text":""},{"location":"seccion/2.1-FactHistoricoAportesTotales/#introduccion","title":"Introducci\u00f3n","text":"<p>Este proceso ETL extrae, transforma y carga datos relacionados con los aportes empresariales en la tabla <code>FactHistoricoAportesTotales</code>. La extracci\u00f3n se realiza desde las tablas <code>subsidio.subsi11</code> y <code>BD_Dim_Datos_Fijos</code>, que contienen informaci\u00f3n sobre fechas de pago (<code>fecpag</code>), confirmaci\u00f3n (<code>feccon</code>), referencia de periodo (<code>periodorefer</code>), identificaci\u00f3n de empresa (<code>nit</code>), valor del aporte (<code>valorpag</code>), valor consignado (<code>valorcon</code>), inter\u00e9s asociado (<code>valorInt</code>) y trabajadores afiliados (<code>trabaapo</code>). Durante la transformaci\u00f3n, se manipulan columnas, se agregan claves \u00fanicas y se realiza la limpieza de duplicados, estructurando los datos para integrarlos en el Data Warehouse (DWH).</p>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#diagrama-de-secuencia-para-el-proceso-etl","title":"Diagrama de secuencia para el proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de secuencia Proceso ETL - FactHistoricoAportesTotales\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL en subsidio.subsi11\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de aportes empresariales\n    ETL_Script -&gt;&gt; ETL_Script: Transforma datos y crea claves \u00fanicas\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Ejecuta consulta en BD_Dim_Datos_Fijos\n    DWH --&gt;&gt; ETL_Script: Retorna datos fijos de afiliados\n    ETL_Script -&gt;&gt; ETL_Script: Limpia duplicados y une datos\n    ETL_Script -&gt;&gt; DWH: Carga datos transformados en BD_Fact_HistoricoTotalAportesEmp\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#etl","title":"ETL","text":""},{"location":"seccion/2.1-FactHistoricoAportesTotales/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque de c\u00f3digo configura el entorno para la manipulaci\u00f3n de datos y la conexi\u00f3n a bases de datos. Se importan librer\u00edas esenciales como SQLAlchemy para gestionar conexiones y consultas SQL, Pandas para la manipulaci\u00f3n de datos, y <code>logging</code> para registrar eventos y errores. La configuraci\u00f3n del <code>sys.path</code> permite importar funciones personalizadas desde el archivo <code>Funciones.py</code>. Entre estas funciones, <code>guardar_en_dwh</code> facilita el almacenamiento de datos en un data warehouse, <code>cargar_tablas</code> permite la carga de m\u00faltiples tablas SQL, y <code>obtener_conexion</code> maneja las conexiones de base de datos. <code>setup_logger</code> configura el registro de eventos, mientras que <code>testfunciones()</code> verifica la disponibilidad de estas funciones para su uso inmediato. Esta configuraci\u00f3n garantiza un entorno listo para el procesamiento, carga y almacenamiento de datos en un flujo ETL (Extract, Transform, Load).</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoAportesTotales.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoAportesTotales.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-25 14:34:22,939 - INFO - Importacion de funciones correcta, 25-10-2024 14:34\n2024-10-25 14:34:22,940 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#definicion-de-consulta-sql-para-datos-de-aportes-empresariales","title":"Definici\u00f3n de Consulta SQL para Datos de Aportes Empresariales","text":"<p>Este c\u00f3digo define una consulta SQL en <code>qr_structure</code> que agrupa y resume informaci\u00f3n de pagos de aportes de la tabla <code>subsidio.subsi11</code>. La consulta selecciona los campos <code>fecpag</code> (fecha de pago), <code>feccon</code> (fecha de confirmaci\u00f3n), <code>periodorefer</code> (periodo de referencia) y <code>nit</code> (identificaci\u00f3n de la empresa), sumando valores de contribuciones (<code>valorcon</code>), pagos (<code>valorpag</code>), trabajadores aportantes (<code>trabaapo</code>), y valores de inter\u00e9s (<code>valorInt</code>). Los datos se filtran para incluir solo registros de los \u00faltimos 18 meses, desde la fecha actual hasta la fecha indicada por <code>DATE_SUB</code>. Adem\u00e1s, la consulta clasifica cada registro bajo el tipo <code>'Empresa'</code> para identificar su origen.</p> <p>El resultado se almacena en <code>df_structure</code> y queda listo para ser cargado en un DataFrame para su posterior procesamiento o almacenamiento, lo cual facilita el an\u00e1lisis de aportes empresariales en el periodo especificado.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\" SELECT fecpag, feccon, periodo as periodorefer, nit, \n        SUM(valcon) as valorcon, \n        SUM(valpag) as valorpag, \n        SUM(traapo) as trabaapo, \n        'Empresa' as Tipo, \n        SUM(valint) as valorInt \n    FROM subsidio.subsi11\n    WHERE fecpag &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR)\n    AND fecpag &lt;= CURRENT_DATE()\n    GROUP BY fecpag, nit;\n    \"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-25 14:34:22,951 - INFO - LECTURA DE QUERYS\n\n\n SELECT fecpag, feccon, periodo as periodorefer, nit, \n        SUM(valcon) as valorcon, \n        SUM(valpag) as valorpag, \n        SUM(traapo) as trabaapo, \n        'Empresa' as Tipo, \n        SUM(valint) as valorInt \n    FROM subsidio.subsi11\n    WHERE fecpag &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR)\n    AND fecpag &lt;= CURRENT_DATE()\n    GROUP BY fecpag, nit;\n</code></pre>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-25 14:34:23,008 - INFO - CONEXION A BASE MINERVA\n2024-10-25 14:34:23,315 - INFO - Cargando query \n2024-10-25 14:34:28,426 - INFO - Cargada query --- 5.11 seconds ---\n2024-10-25 14:34:28,744 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 5.74 seconds ---\n</code></pre> <pre><code>df_structure['query']\n</code></pre> fecpag feccon periodorefer nit valorcon valorpag trabaapo Tipo valorInt 0 2023-04-25 2023-04-25 202207 804000387 375600.0 375600.0 8.0 Empresa 0.0 1 2023-04-25 2023-04-25 202303 860404791 92800.0 92800.0 2.0 Empresa 0.0 2 2023-04-25 2023-04-25 202303 819005585 92800.0 92800.0 2.0 Empresa 200.0 3 2023-04-25 2023-04-25 202303 900342297 1218500.0 1218500.0 34.0 Empresa 0.0 4 2023-04-25 2023-04-25 202303 901299067 60000.0 60000.0 1.0 Empresa 500.0 ... ... ... ... ... ... ... ... ... ... 191968 2024-10-17 2024-10-17 202409 860022269 572300.0 572300.0 2.0 Empresa 500.0 191969 2024-10-17 2024-10-17 202409 900318219 53800.0 53800.0 2.0 Empresa 600.0 191970 2024-10-17 2024-10-17 202409 800125779 52000.0 52000.0 1.0 Empresa 0.0 191971 2024-10-17 2024-10-17 202409 29951245 52000.0 52000.0 1.0 Empresa 300.0 191972 2024-10-17 2024-10-17 202409 890916575 2565700.0 2565700.0 33.0 Empresa 0.0 <p>191973 rows \u00d7 9 columns</p>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#transformacion-de-datos-para-analisis-de-aportes-empresariales","title":"Transformaci\u00f3n de Datos para An\u00e1lisis de Aportes Empresariales","text":"<p>Este bloque de c\u00f3digo lleva a cabo una transformaci\u00f3n detallada de los datos de aportes empresariales almacenados en <code>df</code>. Despu\u00e9s de ejecutar una consulta SQL para cargar los datos de aportes (<code>subsidio.subsi11</code>), todas las columnas de <code>df</code> se convierten a may\u00fasculas y algunas se renombraron para mejorar la claridad (como <code>FECPAG</code> a <code>FECHA_PAGO_APORTE</code> y <code>VALORCON</code> a <code>VALOR_CONSIGNADO</code>). Tambi\u00e9n se crea una columna <code>ID_REGISTRO</code> que asigna un identificador secuencial a cada fila.</p> <p>Adicionalmente, una consulta en <code>qr_structure_fijos</code> extrae los datos fijos de la tabla <code>BD_Dim_Datos_Fijos</code> en el data warehouse. Este resultado se almacena en <code>df_fijos</code>, y luego de eliminar duplicados en el campo <code>NIT</code>, se realiza un <code>inner join</code> entre <code>df</code> y <code>df_fijos</code> para agregar los datos fijos relevantes bas\u00e1ndose en el campo <code>NIT</code>.</p> <p>Finalmente, el DataFrame resultante se reorganiza seg\u00fan el orden en <code>columnas_ordenadas</code> para obtener un conjunto de datos estructurado y listo para an\u00e1lisis de aportes, que permite evaluar la informaci\u00f3n de pago, contribuciones, y referencia temporal de los aportes empresariales.</p> <pre><code>df = df_structure['query']\n\n# Convertir las columnas 'fecpag' y 'feccon' a tipo datetime\ndf['fecpag'] = pd.to_datetime(df['fecpag'], format='%Y-%m-%d')\ndf['feccon'] = pd.to_datetime(df['feccon'], format='%Y-%m-%d')\n\n# Agregar la columna 'periodo' con el formato YYYYMM basado en 'fecpag'\ndf['periodo'] = df['fecpag'].dt.year * 100 + df['fecpag'].dt.month\n\n# Convertir la columna 'periodo' a string (equivalente a 'type text' en Power Query)\ndf['periodo'] = df['periodo'].astype(str)\n\n# Agregar la columna 'KeyHistoricos', concatenando 'periodo', 'nit' y los primeros 3 caracteres de 'Tipo'\ndf['KeyHistoricos'] = df['periodo'] + df['nit'].astype(str) + df['Tipo'].str[:3]\n\n# Agregar la columna 'periodoCon' con el formato YYYYMM basado en 'feccon'\ndf['periodoCon'] = df['feccon'].dt.year * 100 + df['feccon'].dt.month\n</code></pre> <pre><code># Convertir todos los nombres de las columnas de df a may\u00fasculas\ndf.columns = df.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf = df.rename(columns={\n    'FECPAG': 'FECHA_PAGO_APORTE',\n    'FECCON': 'FECHA_CONSIGNACION',\n    'PERIODOREFER': 'PERIODO_REFERENCIA',\n    'VALORCON':'VALOR_CONSIGNADO',\n    'VALORPAG':'APORTE_MES',\n    'TRABAAPO':'NUM_TRAB_POR_APORTE',\n    'VALORINT':'VALOR_INTERES',\n    'PERIODOCON':'PERIODO_CONSIGNACION'\n\n    })\n</code></pre> <pre><code>print(df.columns)\n</code></pre> <pre><code>Index(['FECHA_PAGO_APORTE', 'FECHA_CONSIGNACION', 'PERIODO_REFERENCIA', 'NIT',\n       'VALOR_CONSIGNADO', 'APORTE_MES', 'NUM_TRAB_POR_APORTE', 'TIPO',\n       'VALOR_INTERES', 'PERIODO', 'KEYHISTORICOS', 'PERIODO_CONSIGNACION'],\n      dtype='object')\n</code></pre> <pre><code># Crear una nueva columna 'ID_REGISTRO' con valores secuenciales\ndf['ID_REGISTRO'] = range(1, len(df) + 1)\n</code></pre> <pre><code>#Lista de querys\nqr_structure_fijos = {\n    \"query\": \"\"\"\n    SELECT ID_AFILIADO,DOCUMENTO FROM dwh.BD_Dim_Datos_Fijos;\n    \"\"\"\n}\ndf_structure_fijos = dict()\nlogger.info('LECTURA DE QUERYS')\n#print(qr_structure_apo['query'])\n</code></pre> <pre><code>2024-10-25 14:34:29,030 - INFO - LECTURA DE QUERYS\n</code></pre> <pre><code>#Conexion a base DWH\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor2, qr_structure_fijos, df_structure_fijos, logger)\n</code></pre> <pre><code>2024-10-25 14:34:29,036 - INFO - CONEXION A BASE DWH\n2024-10-25 14:34:29,336 - INFO - Cargando query \n2024-10-25 14:34:36,392 - INFO - Cargada query --- 7.06 seconds ---\n2024-10-25 14:34:36,461 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 7.42 seconds ---\n</code></pre> <pre><code>df_fijos = df_structure_fijos['query']\n\n# Cambiar el nombre de la columna 'DOCUMENTO' a 'NIT'\ndf_fijos.rename(columns={'DOCUMENTO': 'NIT'}, inplace=True)\n\n# Eliminar duplicados bas\u00e1ndose en 'NIT'\ndf_fijos = df_fijos.drop_duplicates(subset='NIT', keep='first')\n#df_fijos\n</code></pre> <pre><code># Realizar el left join por el campo 'KEYHISTORICOS'\ndf_merged = pd.merge(df, df_fijos, how='inner', on='NIT')\n\ndf = df_merged.copy()\n</code></pre> <pre><code># Reorganizar las columnas, colocando 'ID_REGISTRO' de primeras\ncolumnas_ordenadas = ['ID_REGISTRO', 'ID_AFILIADO','PERIODO','FECHA_PAGO_APORTE', 'FECHA_CONSIGNACION', 'PERIODO_REFERENCIA', 'NIT',\n       'VALOR_CONSIGNADO', 'APORTE_MES', 'NUM_TRAB_POR_APORTE', 'TIPO',\n       'VALOR_INTERES', 'KEYHISTORICOS', 'PERIODO_CONSIGNACION']\n\n# Reorganizar el DataFrame\ndf = df[columnas_ordenadas]\ndf\n</code></pre> ID_REGISTRO ID_AFILIADO PERIODO FECHA_PAGO_APORTE FECHA_CONSIGNACION PERIODO_REFERENCIA NIT VALOR_CONSIGNADO APORTE_MES NUM_TRAB_POR_APORTE TIPO VALOR_INTERES KEYHISTORICOS PERIODO_CONSIGNACION 0 1 NI804000387 202304 2023-04-25 2023-04-25 202207 804000387 375600.0 375600.0 8.0 Empresa 0.0 202304804000387Emp 202304 1 9225 NI804000387 202305 2023-05-17 2023-05-17 202304 804000387 320500.0 320500.0 6.0 Empresa 0.0 202305804000387Emp 202305 2 20634 NI804000387 202306 2023-06-22 2023-06-22 202305 804000387 331900.0 331900.0 6.0 Empresa 0.0 202306804000387Emp 202306 3 29598 NI804000387 202307 2023-07-17 2023-07-17 202306 804000387 349800.0 349800.0 7.0 Empresa 0.0 202307804000387Emp 202307 4 41738 NI804000387 202308 2023-08-22 2023-08-22 202307 804000387 333900.0 333900.0 6.0 Empresa 0.0 202308804000387Emp 202308 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 191870 191385 NI901122897 202410 2024-10-16 2024-10-16 202409 901122897 205400.0 205400.0 1.0 Empresa 0.0 202410901122897Emp 202410 191871 191407 NI901809233 202410 2024-10-16 2024-10-16 202408 901809233 27900.0 27900.0 3.0 Empresa 800.0 202410901809233Emp 202410 191872 191441 CC39003038 202410 2024-10-16 2024-10-16 202409 39003038 36600.0 36600.0 1.0 Empresa 200.0 20241039003038Emp 202410 191873 191704 CC36541363 202410 2024-10-17 2024-10-17 202409 36541363 52000.0 52000.0 1.0 Empresa 100.0 20241036541363Emp 202410 191874 191898 NI900371875 202410 2024-10-17 2024-10-17 202409 900371875 51600.0 51600.0 4.0 Empresa 0.0 202410900371875Emp 202410 <p>191875 rows \u00d7 14 columns</p>"},{"location":"seccion/2.1-FactHistoricoAportesTotales/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>df</code> en la tabla <code>BD_Fact_HistoricoTotaAportesEmp</code> de la base DWH. Si la tabla ya existe en la base de datos, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df, 'BD_Fact_HistoricoTotalAportesEmp', logger, multiple=False, if_exists='replace')\ndf.columns.to_list()\n</code></pre> <pre><code>2024-10-25 14:34:37,171 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-25 14:34:37,172 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-25 14:34:37,472 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoTotalAportesEmp\n2024-10-25 14:35:01,862 - INFO - Tabla almacenada correctamente.\n2024-10-25 14:35:02,053 - INFO - ALMACENAMIENTO ---  --- 24.88 seconds ---\n2024-10-25 14:35:02,056 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['ID_REGISTRO',\n 'ID_AFILIADO',\n 'PERIODO',\n 'FECHA_PAGO_APORTE',\n 'FECHA_CONSIGNACION',\n 'PERIODO_REFERENCIA',\n 'NIT',\n 'VALOR_CONSIGNADO',\n 'APORTE_MES',\n 'NUM_TRAB_POR_APORTE',\n 'TIPO',\n 'VALOR_INTERES',\n 'KEYHISTORICOS',\n 'PERIODO_CONSIGNACION']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-25 14:35:02,073 - INFO - FINAL ETL --- 39.14 seconds ---\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/","title":"2.10 Fact Igestion Mercurio","text":""},{"location":"seccion/2.10-FactIgestion_Mercurio/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_Igestion_Mercurio</code> permite consolidar datos de afiliaci\u00f3n provenientes de m\u00faltiples fuentes (SGI, Mercurio Beneficiarios, Empresas y Trabajadores) en un \u00fanico Data Warehouse (DWH). Este proceso extrae registros de radicados de afiliaci\u00f3n, transform\u00e1ndolos en un formato estandarizado, y genera claves \u00fanicas para cada registro, asegurando una integraci\u00f3n y an\u00e1lisis eficientes. Las transformaciones incluyen normalizaci\u00f3n de tipos de documentos, generaci\u00f3n de per\u00edodos de referencia y claves de historial. Finalmente, los datos se cargan en la tabla <code>BD_Fact_Igestion_Mercurio</code> del DWH y se registran los pasos en un log para asegurar trazabilidad y rendimiento.</p>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_Igestion_Mercurio\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Ejecuta consulta SQL para extraer datos de afiliaci\u00f3n\n    DWH --&gt;&gt; ETL_Script: Retorna datos de afiliaci\u00f3n\n    ETL_Script -&gt;&gt; ETL_Script: Procesa y transforma los datos de afiliaci\u00f3n\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_Igestion_Mercurio`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script establece el entorno inicial para un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) mediante el uso de SQLAlchemy y Pandas, facilitando la conexi\u00f3n a bases de datos y la manipulaci\u00f3n avanzada de datos. Se inicia un cron\u00f3metro con <code>time</code> para medir la duraci\u00f3n de los procesos, \u00fatil para evaluar el rendimiento. A trav\u00e9s de <code>sys.path</code>, se habilita la carga de funciones espec\u00edficas desde un m\u00f3dulo externo en la carpeta <code>funciones</code>, importando m\u00e9todos como <code>convertir_columnas_mayusculas</code>, <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, <code>obtener_conexion</code>, <code>testfunciones</code> y <code>setup_logger</code>. Estos m\u00e9todos proporcionan capacidades para transformar y guardar datos en el Data Warehouse, realizar conexiones de base de datos, ejecutar pruebas y configurar registros de eventos. As\u00ed, el entorno queda listo para ejecutar un flujo estructurado y monitoreado de integraci\u00f3n y transformaci\u00f3n de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Igestion_Mercurio.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Igestion_Mercurio.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 23:19:02,883 - INFO - Importacion de funciones correcta, 26-10-2024 23:19\n2024-10-26 23:19:02,886 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>El c\u00f3digo define una consulta SQL que combina datos de varias tablas en un \u00fanico resultado, utilizando una uni\u00f3n de tipo <code>UNION ALL</code> para consolidar registros de afiliaci\u00f3n desde diferentes fuentes de la base de datos <code>dwh</code>. La consulta extrae informaci\u00f3n relevante, incluyendo <code>Fecha</code>, <code>TIPDOC</code> (tipo de documento), <code>Documento</code>, <code>Radicado</code>, y una columna <code>Fuente</code> que indica la procedencia de cada registro (<code>Igestion_SGI</code>, <code>Mercurio_Beneficiarios</code>, <code>Mercurio_Empresas</code>, o <code>Mercurio_Trabajadores</code>). Esto permite unificar en una misma estructura registros provenientes de distintas tablas, facilitando as\u00ed el an\u00e1lisis y tratamiento de datos relacionados con procesos de afiliaci\u00f3n. Adem\u00e1s, se registra la ejecuci\u00f3n de la consulta en <code>logger.info</code> para control y trazabilidad.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    select FECHARADIC as Fecha, TIPDOC as TD, DOCUMENTO as Documento, RADICADO as Radicado, 'Igestion' as Fuente\n    from dwh.Igestion_SGI\n    where TIPOPQRS = 'AFILIACION'\n    union all\n    select fecsol as Fecha, tipdoc as TD, numdoc as Documento, radicado as Radicado, 'Mercurio_Beneficiarios' as Fuente\n    from dwh.Mercurio_Beneficiarios\n    union all\n    select fecsol as Fecha, tipdoc as TD, nit as Documento, radicado as Radicado, 'Mercurio_Empresas' as Fuente\n    from dwh.Mercurio_Empresas\n    union all\n    select fecsol as Fecha, tipdoc as TD, cedtra as Documento, radicado as Radicado, 'Mercurio_Trabajadores' as Fuente\n    from dwh.Mercurio_Trabajadores;\n\"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-26 23:19:02,898 - INFO - LECTURA DE QUERYS\n\n\n\n    select FECHARADIC as Fecha, TIPDOC as TD, DOCUMENTO as Documento, RADICADO as Radicado, 'Igestion' as Fuente\n    from dwh.Igestion_SGI\n    where TIPOPQRS = 'AFILIACION'\n    union all\n    select fecsol as Fecha, tipdoc as TD, numdoc as Documento, radicado as Radicado, 'Mercurio_Beneficiarios' as Fuente\n    from dwh.Mercurio_Beneficiarios\n    union all\n    select fecsol as Fecha, tipdoc as TD, nit as Documento, radicado as Radicado, 'Mercurio_Empresas' as Fuente\n    from dwh.Mercurio_Empresas\n    union all\n    select fecsol as Fecha, tipdoc as TD, cedtra as Documento, radicado as Radicado, 'Mercurio_Trabajadores' as Fuente\n    from dwh.Mercurio_Trabajadores;\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-26 23:19:03,001 - INFO - CONEXION A BASE DWH\n2024-10-26 23:19:03,557 - INFO - Cargando query \n2024-10-26 23:19:34,185 - INFO - Cargada query, 788,249 registros finales obtenidos. --- 30.63 seconds ---\n2024-10-26 23:19:34,316 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 31.31 seconds ---\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#transformacion-y-enriquecimiento-del-dataframe-para-analisis-de-afiliacion","title":"Transformaci\u00f3n y Enriquecimiento del DataFrame para An\u00e1lisis de Afiliaci\u00f3n","text":"<p>Este c\u00f3digo aplica una serie de transformaciones y enriquecimientos al DataFrame <code>df</code> con el fin de preparar los datos para un an\u00e1lisis estructurado. Primero, se convierte la columna <code>Fecha</code> al tipo <code>datetime</code> para manipulaci\u00f3n de fechas, y luego se crea la columna <code>Periodo</code> en el formato <code>YYYYMM</code>, derivando a\u00f1o y mes de la columna <code>Fecha</code>. Tambi\u00e9n se define la columna <code>tipide</code>, asignando c\u00f3digos num\u00e9ricos a valores espec\u00edficos en la columna <code>TD</code> mediante una funci\u00f3n <code>map_tipide</code>, lo que permite estandarizar los diferentes tipos de documentos.</p> <p>Tras esta normalizaci\u00f3n, <code>Periodo</code> y <code>tipide</code> se convierten a tipo <code>string</code> para alinearse con la salida deseada, y se genera la columna <code>KeyHistorico</code>, concatenando <code>Periodo</code>, <code>tipide</code> y <code>Documento</code> para formar una clave \u00fanica que identifica cada registro. Por \u00faltimo, la funci\u00f3n <code>convertir_columnas_mayusculas</code> estandariza los nombres de las columnas a may\u00fasculas, asegurando consistencia en el formato final del DataFrame <code>df_final</code>, ideal para futuras consultas o carga en un sistema de almacenamiento.</p> <pre><code>df = df_structure['query']\n\n# Cambiar el tipo de la columna 'Fecha' a datetime\ndf['Fecha'] = pd.to_datetime(df['Fecha'], format='%Y-%m-%d')\n\n# Agregar la columna 'Periodo' con el formato YYYYMM\ndf['Periodo'] = df['Fecha'].dt.year * 100 + df['Fecha'].dt.month\n\n# Definir la columna 'tipide' con la l\u00f3gica condicional\ndef map_tipide(td):\n    if td == \"CC\":\n        return 1\n    elif td == \"CD\":\n        return 8\n    elif td == \"CE\":\n        return 4\n    elif td in [\"NI\", \"NIT\"]:\n        return 7\n    elif td == \"NP\":\n        return 5\n    elif td == \"PA\":\n        return 6\n    elif td in [\"PE\", \"PEP\", \"PP\", \"TE\"]:\n        return 9\n    elif td in [\"PPT\", \"PT\"]:\n        return 15\n    elif td == \"RC\":\n        return 3\n    elif td == \"TI\":\n        return 2\n    else:\n        return td\n\ndf['tipide'] = df['TD'].apply(map_tipide)\n\n# Convertir la columna 'Periodo' y 'tipide' a string (similar a type text en Power Query)\ndf['Periodo'] = df['Periodo'].astype(str)\ndf['tipide'] = df['tipide'].astype(str)\n\n# Agregar la columna 'KeyHistorico'\ndf['KeyHistorico'] = df['Periodo'] + df['tipide'] + df['Documento'].astype(str)\n\ndf_final = convertir_columnas_mayusculas(df)\n</code></pre>"},{"location":"seccion/2.10-FactIgestion_Mercurio/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_Igestion_Mercurio</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_Igestion_Mercurio', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\n</code></pre> <pre><code>2024-10-26 23:19:35,305 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 23:19:35,308 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 23:19:36,130 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Igestion_Mercurio\n2024-10-26 23:20:45,615 - INFO - Tabla almacenada correctamente. 788,249 registros finales obtenidos.\n2024-10-26 23:20:46,012 - INFO - ALMACENAMIENTO ---  --- 1.18 minutes ---\n2024-10-26 23:20:46,013 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['FECHA',\n 'TD',\n 'DOCUMENTO',\n 'RADICADO',\n 'FUENTE',\n 'PERIODO',\n 'TIPIDE',\n 'KEYHISTORICO']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 23:20:46,027 - INFO - FINAL ETL --- 103.37 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/","title":"2.2 Fact Estado Giro Cuota","text":""},{"location":"seccion/2.2-FactEstadoGiroCuota/#introduccion","title":"Introducci\u00f3n","text":"<p>Este proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) tiene como objetivo centralizar y analizar el estado de los giros de cuotas en un sistema de subsidios. Primero, se configura el entorno de trabajo importando librer\u00edas esenciales y funciones personalizadas, estableciendo una conexi\u00f3n a la base de datos de Minerva para consultar el estado de los giros. La consulta SQL incluye datos del per\u00edodo de pago y la c\u00e9dula de los responsables, clasificando los giros como \"Exitosos\" o \"No exitosos\" mediante un <code>LEFT JOIN</code> con las tablas <code>subsi09</code> y <code>subsi146</code>. </p> <p>Los datos se procesan para generar una clave \u00fanica <code>KeyCuota</code> que combina el per\u00edodo de pago y la c\u00e9dula, y se organizan en un formato estandarizado antes de ser cargados en el data warehouse (DWH) en la tabla <code>BD_Fact_EstadoGiroCuota</code>. Esto permite una evaluaci\u00f3n integral del estado de los giros en un rango de 18 meses, optimizando la disponibilidad de los datos para an\u00e1lisis posteriores.</p>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#diagrama-de-secuencia-para-el-proceso-etl","title":"Diagrama de secuencia para el proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de secuencia para el proceso ETL - FactEstadoGiroCuota\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta en subsi09 y subsi146\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de estado de giros\n    ETL_Script -&gt;&gt; ETL_Script: Transforma datos y crea clave KeyCuota\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en BD_Fact_EstadoGiroCuota\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#etl","title":"ETL","text":""},{"location":"seccion/2.2-FactEstadoGiroCuota/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura un entorno inicial para ejecutar procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga), utilizando las librer\u00edas SQLAlchemy y Pandas para gestionar la conexi\u00f3n a bases de datos y manipular datos de manera eficiente. Al iniciar el cron\u00f3metro con <code>time</code>, el c\u00f3digo permite medir el tiempo de ejecuci\u00f3n de los procesos, lo cual es \u00fatil para optimizaci\u00f3n de rendimiento. Utilizando <code>sys.path</code>, se habilita la carga de funciones personalizadas desde un m\u00f3dulo externo ubicado en el directorio <code>funciones</code>, importando m\u00e9todos espec\u00edficos como <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, <code>obtener_conexion</code>, <code>testfunciones</code>, y <code>setup_logger</code>. Estas funciones proporcionan la capacidad de conectar con el Data Warehouse, cargar y transformar tablas, realizar pruebas funcionales y configurar el sistema de registro de eventos. De este modo, el script est\u00e1 preparado para ejecutar operaciones de integraci\u00f3n de datos de forma estructurada y monitoreada.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>EstadoGiroCuota.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='EstadoGiroCuota.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 23:10:55,025 - INFO - Importacion de funciones correcta, 26-10-2024 23:10\n2024-10-26 23:10:55,029 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#consulta-sql-para-verificacion-de-estado-de-giros-en-subsidios","title":"Consulta SQL para Verificaci\u00f3n de Estado de Giros en Subsidios","text":"<p>Este script define una consulta SQL en el diccionario <code>qr_structure</code> para verificar el estado de los giros en el sistema de subsidios, almacenando el resultado en un DataFrame. La consulta realiza una selecci\u00f3n de los campos <code>pergir</code>, <code>cedres</code>, y un indicador de estado (<code>estadoGiro</code>), el cual eval\u00faa si el giro fue exitoso o no en funci\u00f3n de si el registro aparece en la tabla <code>subsi146</code>. A trav\u00e9s de un <code>LEFT JOIN</code>, la consulta relaciona la tabla <code>subsi09</code> con la tabla <code>subsi146</code>, filtrando por <code>codcue</code> para excluir ciertos c\u00f3digos espec\u00edficos (<code>'16'</code>, <code>'61'</code>, <code>'98'</code>). Adem\u00e1s, se aplican restricciones de fecha que limitan los resultados a un rango de los \u00faltimos 18 meses, verificando que las fechas no sean mayores al d\u00eda actual. Esta estructura permite evaluar el estado de los giros y realizar un seguimiento efectivo de los mismos en el periodo definido, centralizando la informaci\u00f3n para su posterior an\u00e1lisis.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    SELECT \n        s9.pergir, \n        s9.cedres,\n        IF(s146.cedres IS NULL, 'Exitoso', 'No exitoso') as estadoGiro\n    FROM subsidio.subsi09 as s9\n    LEFT JOIN (\n        SELECT * \n        FROM subsidio.subsi146 \n        WHERE codcue NOT IN ('16', '61', '98') \n        GROUP BY cedres, pergirpag\n    ) as s146\n    ON s146.pergirpag = s9.pergir\n    AND s146.cedres = s9.cedres\n    WHERE STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &lt;= CURRENT_DATE()\n    AND STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR)\n    AND s9.codcue NOT IN ('16', '61', '98')\n    GROUP BY s9.pergir, s9.cedres;\n\"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-26 23:10:55,041 - INFO - LECTURA DE QUERYS\n\n\n\n    SELECT \n        s9.pergir, \n        s9.cedres,\n        IF(s146.cedres IS NULL, 'Exitoso', 'No exitoso') as estadoGiro\n    FROM subsidio.subsi09 as s9\n    LEFT JOIN (\n        SELECT * \n        FROM subsidio.subsi146 \n        WHERE codcue NOT IN ('16', '61', '98') \n        GROUP BY cedres, pergirpag\n    ) as s146\n    ON s146.pergirpag = s9.pergir\n    AND s146.cedres = s9.cedres\n    WHERE STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &lt;= CURRENT_DATE()\n    AND STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR)\n    AND s9.codcue NOT IN ('16', '61', '98')\n    GROUP BY s9.pergir, s9.cedres;\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-26 23:10:55,131 - INFO - CONEXION A BASE MINERVA\n2024-10-26 23:10:55,662 - INFO - Cargando query \n2024-10-26 23:13:27,411 - INFO - Cargada query, 968,556 registros finales obtenidos. --- 2.53 minutes ---\n2024-10-26 23:13:27,528 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 2.54 minutes ---\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#transformacion-y-reorganizacion-de-datos-en-el-dataframe","title":"Transformaci\u00f3n y Reorganizaci\u00f3n de Datos en el DataFrame","text":"<p>Este c\u00f3digo lleva a cabo varias transformaciones en el DataFrame <code>df</code> para mejorar la organizaci\u00f3n y facilitar su integraci\u00f3n en procesos anal\u00edticos. Primero, genera una columna clave, <code>KeyCuota</code>, que combina el per\u00edodo de pago (<code>pergir</code>) y la c\u00e9dula del responsable (<code>cedres</code>). A continuaci\u00f3n, convierte todos los nombres de las columnas a may\u00fasculas para asegurar uniformidad y luego renombra campos clave como <code>PERGIR</code> a <code>PERIODO_PAGO</code> y <code>CEDRES</code> a <code>CEDULA_RESPONSABLE_DEL_PAGO</code> para mayor claridad sem\u00e1ntica. </p> <p>Se crea una columna <code>ID_REGISTRO</code> con valores secuenciales que sirven como identificador \u00fanico de cada registro, lo cual facilita su referencia y control en bases de datos. Finalmente, se reorganizan las columnas del DataFrame para colocar <code>ID_REGISTRO</code> en la primera posici\u00f3n, seguido de los campos principales (<code>PERIODO_PAGO</code>, <code>CEDULA_RESPONSABLE_DEL_PAGO</code>, <code>ESTADO_GIRO</code> y <code>KEYCUOTA</code>), logrando una estructura de datos clara y ordenada.</p> <pre><code>df = df_structure['query']\n# Agregar la columna 'KeyCuota'\ndf['KeyCuota'] = df['pergir'].astype(str) + df['cedres'].astype(str)\n</code></pre> <pre><code># Convertir todos los nombres de las columnas de df a may\u00fasculas\ndf.columns = df.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf = df.rename(columns={\n    'PERGIR': 'PERIODO_PAGO',\n    'CEDRES': 'CEDULA_RESPONSABLE_DEL_PAGO',\n    'ESTADOGIRO': 'ESTADO_GIRO'\n\n    })\n</code></pre> <pre><code># Crear una nueva columna 'ID_REGISTRO' con valores secuenciales\ndf['ID_REGISTRO'] = range(1, len(df) + 1)\n\n# Reorganizar las columnas, colocando 'ID_REGISTRO' de primeras\ncolumnas_ordenadas = ['ID_REGISTRO', 'PERIODO_PAGO', 'CEDULA_RESPONSABLE_DEL_PAGO', 'ESTADO_GIRO','KEYCUOTA']\n\n# Reorganizar el DataFrame\ndf = df[columnas_ordenadas]\n</code></pre>"},{"location":"seccion/2.2-FactEstadoGiroCuota/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_EstadoGiroCuota</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df, 'BD_Fact_EstadoGiroCuota', logger, multiple=False, if_exists='replace')\ndf.columns.to_list()\n</code></pre> <pre><code>2024-10-26 23:13:28,046 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 23:13:28,047 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 23:13:28,646 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_EstadoGiroCuota\n2024-10-26 23:14:22,147 - INFO - Tabla almacenada correctamente. 968,556 registros finales obtenidos.\n2024-10-26 23:14:22,733 - INFO - ALMACENAMIENTO ---  --- 54.69 seconds ---\n2024-10-26 23:14:22,735 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['ID_REGISTRO',\n 'PERIODO_PAGO',\n 'CEDULA_RESPONSABLE_DEL_PAGO',\n 'ESTADO_GIRO',\n 'KEYCUOTA']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 23:14:22,750 - INFO - FINAL ETL --- 207.90 seconds ---\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/","title":"2.2 Fact Historico Cuota","text":""},{"location":"seccion/2.2-FactHistoricoCuota/#introduccion","title":"Introducci\u00f3n","text":"<p>Este proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) tiene como objetivo principal centralizar la informaci\u00f3n hist\u00f3rica de cuotas asociadas a giros de subsidios. Primero, se configura el entorno mediante la importaci\u00f3n de librer\u00edas y funciones personalizadas para establecer conexiones con las bases de datos y manejar el almacenamiento y la transformaci\u00f3n de datos. El script se conecta a la base de datos Minerva para obtener datos de giros y cuotas desde la tabla <code>subsi09</code>, a la cual se le aplica un <code>LEFT JOIN</code> con la tabla <code>subsi22</code> para integrar datos adicionales de beneficiarios. Luego, la informaci\u00f3n extra\u00edda se organiza y clasifica seg\u00fan el tipo de subsidio, la oportunidad de pago y otras variables relevantes.</p> <p>Posteriormente, los datos se enriquecen en el Data Warehouse (DWH) con claves \u00fanicas, identificadores secuenciales y etiquetas de clasificaci\u00f3n, como <code>TipoGiro</code>, <code>KeyHistoricos</code>, y <code>KeyEstadoGiroCuota</code>, lo cual facilita la administraci\u00f3n y an\u00e1lisis de la informaci\u00f3n. Al final, el DataFrame resultante se carga en la tabla <code>BD_Fact_HistoricoCuota</code> en el DWH, reemplazando los datos previos si existen, para mantener una fuente de datos consolidada y actualizada sobre el estado de giros y cuotas de subsidios.</p>"},{"location":"seccion/2.2-FactHistoricoCuota/#diagrama-de-secuencia-para-el-proceso-etl","title":"Diagrama de secuencia para el proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de secuencia para el proceso  - FactHistoricoCuota\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta en subsi09 y subsi22\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de cuotas y giros\n    ETL_Script -&gt;&gt; ETL_Script: Transforma datos y crea claves \u00fanicas (KeyHistoricos, KeyEstadoGiroCuota)\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en BD_Fact_HistoricoCuota\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#etl","title":"ETL","text":""},{"location":"seccion/2.2-FactHistoricoCuota/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script configura un entorno para la conexi\u00f3n y manipulaci\u00f3n de datos en una base de datos, empleando SQLAlchemy y Pandas junto a un conjunto de funciones personalizadas. Al inicio, establece un contador de tiempo usando <code>time</code>, y prepara el uso de las librer\u00edas <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, y <code>logging</code> para tareas de an\u00e1lisis y registro de eventos. A trav\u00e9s de la manipulaci\u00f3n de rutas en <code>sys.path</code>, permite la importaci\u00f3n de funciones desde el archivo <code>Funciones.py</code>, como <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, <code>obtener_conexion</code>, <code>testfunciones</code>, y <code>setup_logger</code>, que proporcionan m\u00e9todos espec\u00edficos para la conexi\u00f3n con la base de datos, la carga de datos y el manejo de eventos, estructurando el c\u00f3digo para optimizar las operaciones de almacenamiento y procesamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoCuota.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoCuota.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#conexion-y-procesamiento-de-datos-en-base-minerva","title":"Conexi\u00f3n y Procesamiento de Datos en Base Minerva","text":"<p>El c\u00f3digo se conecta a la base de datos Minerva mediante SQLAlchemy, usando un motor (<code>motor_minerva</code>) que obtiene su conexi\u00f3n a trav\u00e9s de la funci\u00f3n <code>obtener_conexion</code>. Una vez establecida la conexi\u00f3n, se ejecuta <code>cargar_tablas</code>, funci\u00f3n encargada de extraer los datos especificados en la consulta <code>qr_structure_s9</code> y almacenarlos en <code>df_structure_s9</code>. Posteriormente, los datos resultantes (<code>df_final_s9</code>) se almacenan en el Data Warehouse (DWH) en la tabla <code>subsi09</code>, reemplazando cualquier contenido previo si existe.</p> <p>El c\u00f3digo tambi\u00e9n define otra consulta SQL en <code>qr_structure</code> que, partiendo de la tabla <code>subsi09</code> en el DWH, clasifica el campo <code>tipsub</code> en \"Rural\", \"No definido\", o \"Urbano\" seg\u00fan condiciones espec\u00edficas. Adem\u00e1s, la consulta eval\u00faa el campo <code>Oportunidad</code> para etiquetar registros como \"Oportuno\" o \"No oportuno\" en funci\u00f3n de la comparaci\u00f3n entre <code>numgir</code> y <code>numenv</code> de la tabla <code>Oportunidad</code>, unida mediante un <code>LEFT JOIN</code>.</p> <pre><code>#Lista de querys\nqr_structure_s9 = {\n    \"query\": \"\"\"\nselect c.id, b.pergir, b.cedres, b.codben, b.numgir, b.numcuo, b.valor, b.tipgir, b.fecasi, b.fecent, b.codcue, b.tipsub\nfrom subsidio.subsi09 as b\nleft join (select CONCAT(coddoc, documento) as id, codben from subsidio.subsi22) as c\non b.codben = c.codben \n\nwhere STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &lt;= CURRENT_DATE()\nand STR_TO_DATE(CONCAT(pergir, '01'), '%Y%m%d') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR)\n\"\"\"\n}\ndf_structure_s9 = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure_s9['query'])\n</code></pre> <pre><code>#Conexion a base Minerva\nmotor_minerva = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor_minerva, qr_structure_s9, df_structure_s9, logger)\n</code></pre> <pre><code>df_final_s9 = df_structure_s9['query']\n</code></pre> <pre><code>guardar_en_dwh(df_final_s9, 'subsi09', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    select \n        s9.id, s9.pergir, s9.codben, s9.numgir, s9.numcuo, s9.valor, s9.tipgir, s9.fecasi, s9.fecent, s9.codcue, s9.cedres,\n        IF(s9.tipsub in ('A', 'G'), 'Rural', (IF (s9.tipsub IS NULL, 'No definido', 'Urbano' ))) as tipsub,\n        IF( s9.numgir &lt;= opo.numenv , 'Oportuno' , \n            IF( s9.numgir &gt; opo.numenv , 'No oportuno' , NULL ) ) AS Oportunidad\n    from dwh.subsi09 as s9\n    LEFT JOIN (select numenv, pergirpag, cedres, codben from dwh.Oportunidad) as opo\n    ON opo.pergirpag = s9.pergir\n    AND opo.cedres = s9.cedres\n    AND opo.codben = s9.codben;\n\"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#transformacion-de-datos-y-preparacion-de-columnas-en-df_final","title":"Transformaci\u00f3n de Datos y Preparaci\u00f3n de Columnas en <code>df_final</code>","text":"<p>Este c\u00f3digo aplica una serie de transformaciones en el <code>DataFrame</code> <code>df_final</code> para estructurar los datos de manera uniforme y estandarizada. Cambia el tipo de datos de la columna <code>codben</code> a texto, renombra <code>pergir</code> como <code>periodo</code>, y agrega nuevas columnas como <code>KeyHistoricos</code>, formada a partir de la concatenaci\u00f3n de <code>periodo</code>, <code>codben</code> y el sufijo <code>'Ben'</code>. Adem\u00e1s, genera <code>TipoGiro</code> para clasificar el tipo de giro en \"Adicional\" o \"Normal\" seg\u00fan el valor de <code>tipgir</code>, y <code>KeyEstadoGiroCuota</code> para crear identificadores \u00fanicos. Tras convertir los nombres de todas las columnas a may\u00fasculas, se renombran campos clave (por ejemplo, <code>ID</code> como <code>ID_AFILIADO</code>, <code>VALOR</code> como <code>VALOR_CUOTA</code>) y se agrega <code>ID_REGISTRO</code> como un identificador secuencial para cada fila. Finalmente, se reorganizan las columnas en un orden espec\u00edfico para una estructura consistente, con <code>ID_REGISTRO</code> al inicio, seguido de los campos principales y claves generadas. Este flujo asegura que el <code>DataFrame</code> est\u00e9 optimizado para cargas posteriores y an\u00e1lisis en bases de datos u otros sistemas.</p> <pre><code># Asumiendo que df_final = df_structure['query']\n# Crear la columna 'Validador' basado en la comparaci\u00f3n entre 'fecafi' y 'fecapr'\n\ndf_final = df_structure['query']\n# Cambiar el tipo de columna 'codben' a texto\ndf_final['codben'] = df_final['codben'].astype(str)\n\n# Renombrar la columna 'pergir' a 'periodo'\ndf_final = df_final.rename(columns={'pergir': 'periodo'})\n\n# Agregar la columna 'KeyHistoricos'\ndf_final['KeyHistoricos'] = df_final['periodo'] + df_final['codben'] + 'Ben'  # Equivalente a Text.Start('Beneficiario',3)\n\n# Agregar la columna 'TipoGiro'\ndf_final['TipoGiro'] = df_final['tipgir'].apply(lambda x: 'Adicional' if x == 'A' else 'Normal')\n\n# Agregar la columna 'KeyEstadoGiroCuota'\ndf_final['KeyEstadoGiroCuota'] = df_final['periodo'] + df_final['cedres']\n</code></pre> <pre><code># Convertir todos los nombres de las columnas de df_final a may\u00fasculas\ndf_final.columns = df_final.columns.str.upper()\n\n# Cambiar el nombre de dos campos espec\u00edficos\ndf_final = df_final.rename(columns={\n    'ID': 'ID_AFILIADO',\n    'PERIODO': 'PERIODO_PAGO',\n    'CODBEN': 'COD_BENEFICIARIO',\n    'NUMGIR':'CONSECUTIVO_GIRO',\n    'NUMCUO':'NUMERO_CUOTAS',\n    'VALOR':'VALOR_CUOTA',\n    'TIPGIR':'TIPO_GIRO',\n    'FECASI':'FECHA_ASIGNACION_CUOTA',\n    'FECENT':'FECHA_DISPOSICION_CUOTA',\n    'CODCUE':'CODIGO_CUENTA',\n    'CEDRES':'CEDULA_RESPONSABLE_DEL_PAGO',\n    'TIPSUB':'TIPO_SUBSIDIO',\n\n    })\n</code></pre> <pre><code># Crear una nueva columna 'ID_REGISTRO' con valores secuenciales\ndf_final['ID_REGISTRO'] = range(1, len(df_final) + 1)\n\n# Reorganizar las columnas, colocando 'ID_REGISTRO' de primeras\ncolumnas_ordenadas = ['ID_REGISTRO', 'ID_AFILIADO', 'PERIODO_PAGO', 'COD_BENEFICIARIO','CONSECUTIVO_GIRO', 'NUMERO_CUOTAS', 'VALOR_CUOTA', 'TIPO_GIRO', \n                      'FECHA_ASIGNACION_CUOTA', 'FECHA_DISPOSICION_CUOTA', 'CODIGO_CUENTA', 'CEDULA_RESPONSABLE_DEL_PAGO', 'TIPO_SUBSIDIO', \n                      'OPORTUNIDAD', 'KEYHISTORICOS', 'TIPOGIRO', 'KEYESTADOGIROCUOTA']\n\n# Reorganizar el DataFrame\ndf_final = df_final[columnas_ordenadas]\n</code></pre> <pre><code>df_final\n</code></pre>"},{"location":"seccion/2.2-FactHistoricoCuota/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_HistoricoCuota</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoCuota', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/","title":"2.3 Fact Subsidio Vivienda","text":""},{"location":"seccion/2.3-FactSubsidioVivienda/#introduccion","title":"Introducci\u00f3n","text":"<p>El objetivo de este proceso ETL es integrar y almacenar en un Data Warehouse (DWH) los datos de postulaciones para subsidios de vivienda. Se inicia configurando el entorno de trabajo y registrando los eventos mediante un <code>logger</code> que documenta cada fase del proceso. Con una serie de consultas SQL definidas en <code>qr_structure</code>, el proceso extrae datos de varias tablas relacionadas con subsidios en la base de datos Minerva, tales como <code>subvi02</code>, <code>subvi03</code>, <code>subvi04</code>, <code>subvi05</code>, y <code>subvi06</code>. Estas consultas incluyen filtros espec\u00edficos para seleccionar los registros de los \u00faltimos 18 meses, a trav\u00e9s de la columna <code>periodo</code>.</p> <p>Cada tabla es validada en b\u00fasqueda de registros duplicados antes de transformarse en un formato uniforme y limpio. Las columnas de texto son convertidas a may\u00fasculas y sus valores de <code>\"NAN\"</code> o <code>\"NONE\"</code> son reemplazados con valores nulos para facilitar el procesamiento. Finalmente, el proceso guarda los datos transformados en tablas espec\u00edficas del DWH, reemplazando los datos previos si existen. Las tablas se guardan con nombres definidos en <code>dim_names</code> para cada conjunto de datos: <code>BD_Dim_Subvi_Postulacion</code>, <code>BD_Dim_Subvi_Postulacion_Detalle</code>, <code>BD_Fact_Subvi_Beneficiario</code>, <code>BD_Fact_Subvi_Financiero</code>, y <code>BD_Fact_Subvi_Resultado_Solicitud</code>.</p>"},{"location":"seccion/2.3-FactSubsidioVivienda/#diagrama-de-secuencia-para-el-proceso-etl","title":"Diagrama de secuencia para el proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de secuencia para el proceso ETL - FactSubsidioVivienda\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consultas para cada tabla de `qr_structure`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de postulaciones y subsidios\n    ETL_Script -&gt;&gt; ETL_Script: Valida duplicados y limpia datos\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga cada tabla en DWH seg\u00fan `dim_names`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#etl","title":"ETL","text":""},{"location":"seccion/2.3-FactSubsidioVivienda/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque de c\u00f3digo configura el entorno para la conexi\u00f3n a bases de datos y el procesamiento de datos. Primero, se importan varias bibliotecas clave como SQLAlchemy, Pandas, y PyMySQL para manejar la conexi\u00f3n y la manipulaci\u00f3n de datos, y <code>logging</code> para registrar eventos. Luego, se define la ruta para importar funciones personalizadas desde un archivo <code>Funciones.py</code>, ubicado en un directorio superior. Entre estas funciones se encuentran <code>StoreDuplicated</code>, <code>guardar_en_dwh</code>, <code>obtener_conexion</code>, <code>cargar_tablas</code>, <code>testfunciones</code>, y <code>setup_logger</code>, que permiten gestionar registros, establecer conexiones, y ejecutar operaciones de carga y almacenamiento en un Data Warehouse (DWH). Al final, <code>testfunciones()</code> es llamado para verificar la conexi\u00f3n o funcionalidad del m\u00f3dulo importado, lo que permite asegurar la correcta configuraci\u00f3n antes de iniciar el procesamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 23-10-2024 15:06\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>SubsidioVivienda.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code># Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='SubsidioVivienda.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-23 15:06:01,473 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#definicion-de-consultas-sql-para-extraccion-de-datos-de-vivienda","title":"Definici\u00f3n de Consultas SQL para Extracci\u00f3n de Datos de Vivienda","text":"<p>Este c\u00f3digo define un conjunto de consultas SQL organizadas en el diccionario <code>qr_structure</code>, que extraen y estructuran datos de varias tablas relacionadas con informaci\u00f3n de postulaciones de vivienda. Cada consulta (<code>\"subvi02\"</code>, <code>\"subvi03\"</code>, <code>\"subvi04\"</code>, <code>\"subvi05\"</code>, <code>\"subvi06\"</code>) selecciona un conjunto de campos espec\u00edficos, normalizando el formato de las columnas para facilitar su integraci\u00f3n en el an\u00e1lisis de datos. </p> <p>Adem\u00e1s, las consultas comparten un filtro com\u00fan que limita los datos a los registros con un <code>periodo</code> dentro de los \u00faltimos 18 meses, usando <code>DATE_FORMAT</code> y <code>DATE_SUB</code> para establecer el rango de fechas en las condiciones <code>WHERE</code>. Varias de las subconsultas emplean <code>JOIN</code> para combinar registros de la tabla <code>subvi02</code> y asociar documentos de solicitud espec\u00edficos con otros datos de las tablas adicionales.</p> <p>Finalmente, el diccionario <code>dim_names</code> asocia cada clave de <code>qr_structure</code> con un nombre de tabla espec\u00edfico para facilitar su identificaci\u00f3n en el procesamiento posterior, siendo este dise\u00f1o ideal para organizar y cargar datos en un entorno de Data Warehouse.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"subvi02\":'''SELECT \n                CONCAT(coddoc, cedpos) as ID_AFILIADO, \n                periodo, \n                documento as NUMERO_SOLICITUD,\n                numrad as NUMERO_RADICADO,\n                salario,\n                porapo AS PORCENTAJE_APORTES,\n                tippob AS TIPO_POBLACION,\n                tippos AS TIPO_POSTULANTE,\n                discap AS DISCAPACIDAD,\n                cabhog AS CABEZA_HOGAR,\n                numasi AS NUMERO_ASIGNACIONES,\n                tipmad AS TIPO_MADRE,\n                prioridad,\n                estado,\n                fecest AS FECHA_ESTADO,\n                fecdig AS FECHA_DIGITACION,\n                fecmod AS FECHA_MODIFICACION,\n                fecact AS FECHA_ACTUALIZACION,\n                nota\n                FROM vivienda.subvi02\n                WHERE DATE_FORMAT(STR_TO_DATE(periodo, '%Y%m'), '%Y%m01') &gt; DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m01')              \n                ''',\n    \"subvi03\":'''SELECT \n                documento as NUMERO_SOLICITUD,\n                modsol AS MODALIDAD_SOLICITUD,\n                tippro AS TIPO_PROYECTO,\n                coddep AS COD_DEP,\n                codciu AS COD_CIU,\n                tipviv AS TIPO_VIVIENDA,\n                valviv AS VALOR_VIVIENDA,\n                valaho AS VALOR_AHORRO,\n                valces AS VALOR_CESANTIAS,\n                fecter AS FECHA_TERMINA_CONTRATO,\n                totapo AS TOTAL_APORTES,\n                fecini AS FECHA_INICIAL,\n                totsub AS TOTAL_SUBSIDIO,\n                totfin AS TOTAL_FINANCIADO\n                FROM vivienda.subvi03 s03\n                INNER JOIN (\n                SELECT \n                    periodo, \n                    documento as NUMERO_SOLICITUD\n                    FROM vivienda.subvi02\n                    WHERE DATE_FORMAT(STR_TO_DATE(periodo, '%Y%m'), '%Y%m01') &gt; DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m01')\n                    ) AS s02 ON s03.documento = s02.NUMERO_SOLICITUD''',\n    \"subvi04\":'''SELECT\n                documento as NUMERO_SOLICITUD,\n                CONCAT(coddoc, numdoc) as ID_AFILIADO,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                fecnac AS FECHA_NACIMIENTO,\n                numdoc AS NUMERO_DOCUMENTO,\n                coddoc AS CODDOC,\n                sexo,\n                parent AS PARENTESCO,\n                estciv AS ESTADO_CIVIL,\n                discap AS DISCAPACIDAD,\n                ingres AS INGRESOS_MENSUALES\n                FROM vivienda.subvi04 s04\n                INNER JOIN (\n                SELECT \n                    periodo, \n                    documento as NUMERO_SOLICITUD\n                    FROM vivienda.subvi02\n                    WHERE DATE_FORMAT(STR_TO_DATE(periodo, '%Y%m'), '%Y%m01') &gt; DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m01')\n                    ) AS s02 ON s04.documento = s02.NUMERO_SOLICITUD\n                ''',\n    \"subvi05\":'''select \n                documento as NUMERO_SOLICITUD,\n                credito AS ESTADO_CREDITO,\n                cueaho AS CUENTA_AHORROS,\n                codban COD_BANCO,\n                ciuban AS CIUDAD_BANCO,\n                fecaho AS FECHA_AHORROS,\n                fonces AS FONDO_CESANTIAS,\n                fecces AS FECHA_CESANTIAS,\n                fecini AS FECHA_INICIAL,\n                ciufon AS CIUDAD_CESANTIAS\n                FROM vivienda.subvi05 s05\n                INNER JOIN (\n                SELECT \n                    periodo, \n                    documento as NUMERO_SOLICITUD\n                    FROM vivienda.subvi02\n                    WHERE DATE_FORMAT(STR_TO_DATE(periodo, '%Y%m'), '%Y%m01') &gt; DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m01')\n                    ) AS s02 ON s05.documento = s02.NUMERO_SOLICITUD''',\n    \"subvi06\":'''select \n                documento as NUMERO_SOLICITUD,\n                s06.periodo ,\n                numasi AS NUMERO_ASIGNACIONES,\n                fecasi AS FECHA_ASIGNACION, \n                valaju AS VALOR_AJUSTE,\n                valsub AS VALOR_SUBSIDIO,\n                nota,\n                estado,\n                fecven AS FECHA_VENCIMIENTO,\n                fecpag AS FECHA_PAGO,\n                fecleg AS FECHA_LEGALIZACION,\n                ruaf,\n                ruasub,\n                feccom AS FECHA_COMPRA\n                FROM vivienda.subvi06 s06\n                INNER JOIN (\n                SELECT \n                    periodo, \n                    documento as NUMERO_SOLICITUD\n                    FROM vivienda.subvi02\n                    WHERE DATE_FORMAT(STR_TO_DATE(periodo, '%Y%m'), '%Y%m01') &gt; DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y%m01')\n                    ) AS s02 ON s06.documento = s02.NUMERO_SOLICITUD'''\n               }\ndim_names = {\n    \"subvi02\":'BD_Dim_Subvi_Postulacion',\n    \"subvi03\":'BD_Dim_Subvi_Postulacion_Detalle',\n    \"subvi04\":'BD_Fact_Subvi_Beneficiario',\n    \"subvi05\":'BD_Fact_Subvi_Financiero',\n    \"subvi06\":'BD_Fact_Subvi_Resultado_Solicitud'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-23 15:06:01,487 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-23 15:06:01,507 - INFO - CONEXION A BASE MINERVA\n2024-10-23 15:06:01,865 - INFO - Cargando subvi02 \n2024-10-23 15:06:02,063 - INFO - Cargada subvi02 --- 0.20 seconds ---\n2024-10-23 15:06:02,063 - INFO - Cargando subvi03 \n2024-10-23 15:06:02,205 - INFO - Cargada subvi03 --- 0.14 seconds ---\n2024-10-23 15:06:02,206 - INFO - Cargando subvi04 \n2024-10-23 15:06:02,352 - INFO - Cargada subvi04 --- 0.15 seconds ---\n2024-10-23 15:06:02,353 - INFO - Cargando subvi05 \n2024-10-23 15:06:02,403 - INFO - Cargada subvi05 --- 0.05 seconds ---\n2024-10-23 15:06:02,405 - INFO - Cargando subvi06 \n2024-10-23 15:06:02,453 - INFO - Cargada subvi06 --- 0.05 seconds ---\n2024-10-23 15:06:02,509 - INFO - CARGUE TABLAS DESDE MYSQL --- subvi06 --- 1.00 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la presencia de registros duplicados en todas las tablas del diccionario <code>df_structure</code>. Las columnas utilizadas para comparar duplicados excluyen la columna <code>id</code>. Los duplicados detectados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:06:02,542 - INFO - VALIDADOR TABLA: subvi02\n2024-10-23 15:06:02,551 - INFO - VALIDADOR TABLA: subvi03\n2024-10-23 15:06:02,561 - INFO - VALIDADOR TABLA: subvi04\n2024-10-23 15:06:02,568 - INFO - VALIDADOR TABLA: subvi05\n2024-10-23 15:06:02,576 - INFO - VALIDADOR TABLA: subvi06\n2024-10-23 15:06:02,577 - INFO - VALIDADOR DUPLICADOS --- 0.06 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla dentro de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total de la operaci\u00f3n se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:06:02,656 - INFO - LIMPIEZA --- 0.07 seconds ---\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-23 15:06:02,662 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/2.3-FactSubsidioVivienda/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. Si las tablas ya existen en la base de datos, se reemplazan con los nuevos datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-23 15:06:02,668 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-23 15:06:02,670 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-23 15:06:02,962 - INFO - Almacenando tabla subvi02 en DWH como BD_Dim_Subvi_Postulacion\n2024-10-23 15:06:04,064 - INFO - Tabla subvi02 almacenada correctamente como BD_Dim_Subvi_Postulacion.\n2024-10-23 15:06:04,066 - INFO - Almacenando tabla subvi03 en DWH como BD_Dim_Subvi_Postulacion_Detalle\n2024-10-23 15:06:04,954 - INFO - Tabla subvi03 almacenada correctamente como BD_Dim_Subvi_Postulacion_Detalle.\n2024-10-23 15:06:04,955 - INFO - Almacenando tabla subvi04 en DWH como BD_Fact_Subvi_Beneficiario\n2024-10-23 15:06:05,906 - INFO - Tabla subvi04 almacenada correctamente como BD_Fact_Subvi_Beneficiario.\n2024-10-23 15:06:05,907 - INFO - Almacenando tabla subvi05 en DWH como BD_Fact_Subvi_Financiero\n2024-10-23 15:06:06,578 - INFO - Tabla subvi05 almacenada correctamente como BD_Fact_Subvi_Financiero.\n2024-10-23 15:06:06,579 - INFO - Almacenando tabla subvi06 en DWH como BD_Fact_Subvi_Resultado_Solicitud\n2024-10-23 15:06:07,249 - INFO - Tabla subvi06 almacenada correctamente como BD_Fact_Subvi_Resultado_Solicitud.\n2024-10-23 15:06:07,313 - INFO - ALMACENAMIENTO ---  --- 4.65 seconds ---\n2024-10-23 15:06:07,314 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 15:06:07,324 - INFO - FINAL ETL --- 5.86 seconds ---\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/2.4-FactFosfec/","title":"2.4 Fact Fos fec","text":""},{"location":"seccion/2.4-FactFosfec/#introduccion","title":"Introducci\u00f3n","text":"<p>Este proceso ETL para la tabla <code>FactFosfec</code> permite la extracci\u00f3n, transformaci\u00f3n y carga de datos de la base de datos Minerva a un Data Warehouse (DWH) en la tabla <code>BD_Fact_Fosfec</code>. Inicia con la importaci\u00f3n de bibliotecas y funciones personalizadas para el manejo de datos, configurando el <code>logger</code> para registrar eventos en un archivo log (<code>FactFosFec.log</code>). Luego, se realiza la extracci\u00f3n de datos desde m\u00faltiples tablas en la base de datos <code>fosfec</code> con una consulta SQL en <code>qr_structure</code>, que obtiene informaci\u00f3n clave relacionada con fechas, documentos y salarios de los afiliados. Posteriormente, se validan y eliminan duplicados, limpiando errores antes de realizar la carga final en el DWH. Al finalizar, el tiempo de ejecuci\u00f3n de cada etapa y del proceso completo se registra en el log para facilitar el monitoreo y optimizaci\u00f3n futura.</p>"},{"location":"seccion/2.4-FactFosfec/#diagrama-de-secuencia-para-el-proceso-etl","title":"Diagrama de Secuencia para el Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia para el Proceso ETL - FactFosfec\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer datos de `fosfec`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de `fact_foscec`\n    ETL_Script -&gt;&gt; ETL_Script: Valida y elimina duplicados en los datos\n    ETL_Script -&gt;&gt; ETL_Script: Limpia errores en los datos\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en la tabla `BD_Fact_Fosfec`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.4-FactFosfec/#importacion-de-bibliotecas-y-configuracion-inicial","title":"Importaci\u00f3n de bibliotecas y configuraci\u00f3n inicial","text":"<p>Este bloque configura el entorno de trabajo para ejecutar un proceso ETL, importando varias bibliotecas necesarias como <code>SQLAlchemy</code> para la conexi\u00f3n a bases de datos, <code>Pandas</code> para la manipulaci\u00f3n de datos, y <code>logging</code> para el registro de eventos en un archivo de log (<code>FactFosFec.log</code>). A continuaci\u00f3n, el c\u00f3digo importa un conjunto de funciones personalizadas desde el m\u00f3dulo <code>Funciones.py</code>, ubicado en un directorio superior, agregando su ruta al <code>sys.path</code>. Entre las funciones clave importadas est\u00e1n <code>guardar_en_dwh</code>, <code>StoreDuplicated</code>, <code>RemoveDuplicated</code>, <code>RemoveErrors</code>, y <code>cargar_tablas</code>, que permiten manipular los datos, eliminar duplicados y errores, y almacenar los resultados en un Data Warehouse.</p> <p>Al configurar el <code>logger</code> con <code>setup_logger</code>, se asegura que el proceso registre eventos importantes, comenzando con un mensaje inicial (\"COMIENZO ETL\") para trazar el flujo del proceso y facilitar el monitoreo del desempe\u00f1o y la identificaci\u00f3n de errores durante la ejecuci\u00f3n del ETL.</p> <pre><code># Importar bibliotecas necesarias\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport logging\n\n\nstart_time = time.time()\n#---------------------------------------------\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, RemoveDuplicated, RemoveErrors, cargar_tablas, obtener_conexion, testfunciones, setup_logger\nprint(testfunciones())\n\n# Configuraci\u00f3n inicial\nlogger = setup_logger(log_filename='FactFosFec.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-23 14:50:24,325 - INFO - COMIENZO ETL\n\n\nImportacion de funciones correcta, 23-10-2024 14:50\n</code></pre>"},{"location":"seccion/2.4-FactFosfec/#definicion-de-consultas-sql","title":"Definici\u00f3n de consultas SQL","text":"<p>Se define una consulta SQL en el diccionario <code>qr_structure</code> que extrae datos de las tablas <code>fosfec160</code>, <code>fosfec07</code>, <code>fosfec09</code>, y <code>fosfec21</code> de la base de datos <code>fosfec</code>, utilizando varias combinaciones de <code>INNER JOIN</code> y <code>LEFT JOIN</code>. La consulta selecciona columnas clave relacionadas con fechas, documentos y salarios. El proceso de lectura de consultas se registra en el log.</p> <pre><code># Consultas SQL\n\nqr_structure = {\n    \"fact_foscec\": '''\n    SELECT\n        fosfec160.id as REGISTRO_WEB,\n        CONCAT(fosfec160.tipdoc,fosfec160.cedtra) AS ID_AFILIADO,\n        DATE_FORMAT(fecsis, '%Y%m') AS PERIODO,\n        fosfec160.tipdoc AS TIPO_DOCUMENTO,\n        fosfec160.cedtra AS NUMERO_DOCUMENTO,\n        fosfec160.fecexp AS FECHA_EXPEDICION_DOC,\n        fosfec160.fecsis AS FECHA_SISTEMA,\n        fosfec07.fecasi AS FECHA_ASIGNACION,\n        fosfec07.fecest AS FECHA_ESTADO,\n        fosfec09.fecnac AS FECHA_NACIMIENTO,\n        fosfec160.fecfin AS FECHA_FINAL_BENEFICIO,\n        fosfec09.fecdig AS FECHA_DIGITACION,\n        fosfec09.fecsal AS FECHA_SALARIO,\n        fosfec160.fecenv AS FECHA_ENVIO,\n        fosfec160.fecterm AS FECHA_TERMINA_CONTRATO,\n        fosfec160.salario AS ULTIMO_SALARIO,\n        fosfec160.bonopen AS BONO_PENSIONAL,\n        fosfec07.recsub AS RECIBE_SUBSIDIO,\n        fosfec07.cuosub AS CUOTA_SUBSIDIO,\n        fosfec160.numdaviplata AS NUMERO_DAVIPLATA,\n        fosfec07.documento AS NUMERO_FORMULARIO,\n        fosfec160.numres AS NUMERO_RESOLUCION,\n        fosfec160.codcat AS CATEGORIA,\n        fosfec160.codpen AS FONDO_PENSIONES,\n        fosfec07.codces AS FONDO_CESANTIAS,\n        fosfec160.codcaj AS CAJA_COMPENSACION\n    FROM fosfec.fosfec160\n    right JOIN fosfec.fosfec07 \n        ON  fosfec.fosfec160.id = fosfec.fosfec07.id_web\n    INNER JOIN fosfec.fosfec09 \n        ON fosfec.fosfec07.cedtra = fosfec.fosfec09.cedtra\n    WHERE DATE_FORMAT(fosfec160.fecsis, '%Y%m%01') &gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH)\n    '''\n}\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-23 14:50:24,330 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.4-FactFosfec/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-23 14:50:24,387 - INFO - CONEXION A BASE MINERVA\n2024-10-23 14:50:24,696 - INFO - Cargando fact_foscec \n2024-10-23 14:50:26,609 - INFO - Cargada fact_foscec --- 1.91 seconds ---\n2024-10-23 14:50:26,680 - INFO - CARGUE TABLAS DESDE MYSQL --- fact_foscec --- 2.29 seconds ---\n</code></pre>"},{"location":"seccion/2.4-FactFosfec/#definicion-de-funciones-de-transformacion-y-limpieza","title":"Definici\u00f3n de funciones de transformaci\u00f3n y limpieza","text":"<p>Se define el identificador \u00fanico <code>columId</code> como <code>'id'</code>, y se crea una lista llamada <code>ColumnsToCompare</code> que contiene las columnas clave del dataframe que se utilizar\u00e1n en los procesos de transformaci\u00f3n y limpieza. Estas columnas incluyen datos sobre fechas, salarios, documentos, y c\u00f3digos. Esta lista es esencial para las operaciones posteriores, como la identificaci\u00f3n de duplicados y la validaci\u00f3n de datos.</p> <pre><code># Definici\u00f3n de funciones de transformaci\u00f3n y limpieza\ncolumId = 'REGISTRO_WEB'  # Ajusta seg\u00fan tu identificador \u00fanico\n\n# Lista actualizada de columnas que cumplen con las restricciones\nColumnsToCompare = [\n    'REGISTRO_WEB', 'ID_AFILIADO', 'periodo', 'TIPO_DOCUMENTO',\n       'NUMERO_DOCUMENTO', 'FECHA_EXPEDICION_DOC', 'FECHA_SISTEMA',\n       'FECHA_ASIGNACION', 'FECHA_ESTADO', 'FECHA_NACIMIENTO',\n       'FECHA_FINAL_BENEFICIO', 'FECHA_DIGITACION', 'FECHA_SALARIO',\n       'FECHA_ENVIO', 'FECHA_TERMINA_CONTRATO', 'ULTIMO_SALARIO',\n       'BONO_PENSIONAL', 'RECIBE_SUBSIDIO', 'CUOTA_SUBSIDIO',\n       'NUMERO_DAVIPLATA', 'NUMERO_FORMULARIO', 'NUMERO_RESOLUCION',\n       'CATEGORIA', 'FONDO_PENSIONES', 'FONDO_CESANTIAS', 'CAJA_COMPENSACION'\n]\n</code></pre>"},{"location":"seccion/2.4-FactFosfec/#proceso-de-limpieza-y-validacion-de-duplicados","title":"Proceso de limpieza y validaci\u00f3n de duplicados","text":"<p>Se itera sobre los dataframes en <code>df_structure</code>, guardando los duplicados mediante la funci\u00f3n <code>StoreDuplicated</code> usando las columnas especificadas en <code>ColumnsToCompare</code>. Si la columna <code>f_disp_benef</code> est\u00e1 presente, se eliminan duplicados en funci\u00f3n de esa fecha; en caso contrario, se omite esa operaci\u00f3n y se registra una advertencia en el log. Posteriormente, se limpia el dataframe de errores utilizando la funci\u00f3n <code>RemoveErrors</code>. Cada tabla procesada se registra en el log al completar la validaci\u00f3n y limpieza de duplicados. </p> <pre><code># Iterar sobre los DataFrames para realizar transformaciones\nfor key, df in df_structure.items():\n    print(f\"Procesando tabla: {key}\")\n\n    # Guardar duplicados\n    StoreDuplicated('N/A', ColumnsToCompare, df, 'duplicados_fact_fosfec_' + key)\n\n    # Eliminar duplicados por fecha (aseg\u00farate de tener una columna de fecha adecuada en tu DataFrame)\n    if 'FECHA_SISTEMA' in df.columns:\n        df_cleaned = RemoveDuplicated(columId, 'FECHA_SISTEMA', df)\n    else:\n        logger.warning(f'Columna de fecha \"FECHA_SISTEMA\" no encontrada en {key}. Se omite la eliminaci\u00f3n de duplicados por fecha.')\n        df_cleaned = df\n\n    # Limpiar errores (si fuera aplicable a tu contexto)\n    df_final = RemoveErrors(df_cleaned, 'errores_fact_foscec_' + key)\n\n    logger.info('Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: ' + key)\n</code></pre> <pre><code>2024-10-23 14:50:26,776 - INFO - Proceso de limpieza y validaci\u00f3n de duplicados completado para la tabla: fact_foscec\n\n\nProcesando tabla: fact_foscec\n</code></pre> <pre><code>df_structure['fact_foscec']\n</code></pre> REGISTRO_WEB ID_AFILIADO PERIODO TIPO_DOCUMENTO NUMERO_DOCUMENTO FECHA_EXPEDICION_DOC FECHA_SISTEMA FECHA_ASIGNACION FECHA_ESTADO FECHA_NACIMIENTO ... BONO_PENSIONAL RECIBE_SUBSIDIO CUOTA_SUBSIDIO NUMERO_DAVIPLATA NUMERO_FORMULARIO NUMERO_RESOLUCION CATEGORIA FONDO_PENSIONES FONDO_CESANTIAS CAJA_COMPENSACION 0 27364 CC1082953354 202305 CC 1082953354 2010-08-02 2023-05-15 None 2023-05-31 1992-01-14 ... N N 0 3002387992 0006409 1 A 230301 None None 1 27549 CC85472991 202306 CC 85472991 1994-02-07 2023-06-06 2021-10-21 2023-07-17 1975-08-08 ... N N 0 3107270027 0015707 1 B 25-14 SINAFP None 2 30246 CC57466051 202401 CC 57466051 2003-12-10 2024-01-09 2022-08-24 2024-01-23 1985-06-20 ... N S 1 3016669725 0015772 1 A 25-14 None None 3 33874 CC57463242 202409 CC 57463242 2003-03-25 2024-09-10 2022-08-24 2024-09-16 1981-12-29 ... N S 1 3145915729 0015831 1 A 230301 None None 4 28163 CC1082936727 202307 CC 1082936727 2009-08-12 2023-07-17 2022-08-24 2022-09-16 1991-06-27 ... N S 2 3005083755 0015847 1 A 230301 None None ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6985 34167 CC57296404 202410 CC 57296404 2001-10-26 2024-10-03 None 2024-10-18 1982-08-20 ... N N 0 3008079311 0026121 1 A 25-14 None None 6986 34168 CC1079934467 202410 CC 1079934467 2010-06-17 2024-10-04 None 2024-10-18 1992-05-24 ... N N 0 3235958060 0026122 1 A 230301 None None 6987 34218 CC39049304 202410 CC 39049304 1998-03-19 2024-10-01 None 2024-10-18 1979-08-29 ... N N 0 3012678558 0026123 1 A 230301 None None 6988 34219 CC1221981251 202410 CC 1221981251 2017-07-04 2024-10-04 None 2024-10-18 1999-04-25 ... N S 1 3042234766 0026124 1 A 230301 None None 6989 34259 CC85260916 202410 CC 85260916 2001-01-08 2024-10-08 None 2024-10-18 1982-09-04 ... N N 0 3234251547 0026125 1 A 230201 None None <p>6990 rows \u00d7 26 columns</p>"},{"location":"seccion/2.4-FactFosfec/#guardar-en-la-base-de-datos-dwh","title":"Guardar en la base de datos DWH","text":"<p>Se guarda el dataframe <code>fact_foscec</code> en la tabla <code>BD_FactFosfec</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza con los nuevos datos. El tiempo total de almacenamiento se registra en el log, seguido del registro de finalizaci\u00f3n del proceso ETL, incluyendo el tiempo total desde el inicio del proceso.</p> <pre><code>guardar_en_dwh(df_structure['fact_foscec'], 'BD_Fact_Fosfec', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-23 14:50:26,814 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-23 14:50:26,816 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-23 14:50:27,120 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Fosfec\n2024-10-23 14:50:29,773 - INFO - Tabla almacenada correctamente.\n2024-10-23 14:50:29,852 - INFO - ALMACENAMIENTO ---  --- 3.04 seconds ---\n2024-10-23 14:50:29,853 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code># Tiempo total de ejecuci\u00f3n\nlogger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 14:50:29,863 - INFO - FINAL ETL --- 5.54 seconds ---\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/","title":"2.5 Fact Historico Mora Emp","text":""},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_HistoricoMoraEmp</code> permite la extracci\u00f3n, transformaci\u00f3n y carga de datos de morosidad de empresas desde la base de datos Minerva hacia un Data Warehouse (DWH). Este flujo inicia con la configuraci\u00f3n del entorno, incluyendo bibliotecas y funciones necesarias para el manejo y transformaci\u00f3n de datos. Luego, se realiza una consulta en la base Minerva para obtener datos de la tabla <code>xml4.xml4c070</code>, que registra valores en mora, periodos en mora y estados de notificaci\u00f3n para cada empresa (<code>nit</code>). La consulta incluye un <code>LEFT JOIN</code> con la tabla <code>subsidio.subsi173</code> para verificar el estado de notificaci\u00f3n de cada empresa en los \u00faltimos 18 meses. Posteriormente, se transforman los datos, estandarizando nombres de columnas, creando claves \u00fanicas, y clasificando los periodos de mora en rangos. Finalmente, el resultado se almacena en la tabla <code>BD_Fact_HistoricoMoraEmp</code> del DWH, sobrescribiendo cualquier dato previo, y el proceso completo se registra en el log para monitoreo y an\u00e1lisis de rendimiento.</p>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_HistoricoMoraEmp\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer datos de morosidad\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de morosidad\n    ETL_Script -&gt;&gt; ETL_Script: Transformaci\u00f3n y normalizaci\u00f3n de datos\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en la tabla `BD_Fact_HistoricoMoraEmp`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#etl","title":"ETL","text":""},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para manejar conexiones a bases de datos y realizar manipulaci\u00f3n de datos. Se importa <code>sqlalchemy</code> para conectarse a las bases de datos y <code>pandas</code> para el manejo de datos tabulares. Las funciones personalizadas de <code>Funciones.py</code> incluyen <code>guardar_en_dwh</code> para almacenar datos en el Data Warehouse, <code>cargar_tablas</code> para cargar datos, <code>obtener_conexion</code> para establecer conexiones, <code>setup_logger</code> para configurar el registro de eventos, y <code>convertir_columnas_mayusculas</code> para transformar nombres de columnas a may\u00fasculas, facilitando la uniformidad en el procesamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger,convertir_columnas_mayusculas\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoMoraEmp.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoMoraEmp.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-28 18:07:16,655 - INFO - Importacion de funciones correcta, 28-10-2024 18:07\n2024-10-28 18:07:16,656 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#consulta-sql-para-analisis-de-morosidad-y-notificaciones-de-empresas","title":"Consulta SQL para An\u00e1lisis de Morosidad y Notificaciones de Empresas","text":"<p>Este c\u00f3digo define una consulta SQL en el diccionario <code>qr_structure</code>, que extrae datos relacionados con la morosidad de empresas desde la tabla <code>xml4.xml4c070</code>. La consulta calcula el valor total en mora (<code>salmor</code>) y el n\u00famero de periodos en mora (<code>permor</code>) por cada empresa (<code>nit</code>). Adem\u00e1s, agrega una columna <code>Notificacion</code> para indicar si la empresa fue notificada, comparando con registros de la tabla <code>subsidio.subsi173</code>. La consulta filtra datos entre los \u00faltimos 18 periodos, y agrupa los resultados por periodo y empresa. Esto permite evaluar tanto el valor en mora como el estado de notificaci\u00f3n de cada empresa en los periodos seleccionados.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n        select \n            m1.periodo as periodo_actualizacion, \n            m1.nit, \n            m1.coddoc,\n            SUM(m1.salmor) as valormor, \n            m1.permor as NumPerMora, \n            'Empresa' as Tipo,  \n            if(m2.nit is null, 0, 1) as Notificacion, \n            m1.razsoc \n        from \n            xml4.xml4c070 as m1 \n        LEFT JOIN (\n            select \n                perfin as periodo, \n                nit \n            from \n                subsidio.subsi173 \n            group by \n                periodo, nit \n            order by \n                nit\n        ) as m2 \n        ON \n            m1.nit = m2.nit \n            AND m1.periodo = m2.periodo \n        where \n            STR_TO_DATE(CONCAT(SUBSTRING(m1.periodo, 1, 4), '-', SUBSTRING(m1.periodo, 5, 2), '-01'), '%Y-%m-%d')\n            BETWEEN DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n            AND DATE_FORMAT(CURDATE(), '%Y-%m-01')\n        group by \n            m1.periodo, m1.nit;    \n    \"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-28 18:07:16,673 - INFO - LECTURA DE QUERYS\n\n\n\n        select \n            m1.periodo as periodo_actualizacion, \n            m1.nit, \n            m1.coddoc,\n            SUM(m1.salmor) as valormor, \n            m1.permor as NumPerMora, \n            'Empresa' as Tipo,  \n            if(m2.nit is null, 0, 1) as Notificacion, \n            m1.razsoc \n        from \n            xml4.xml4c070 as m1 \n        LEFT JOIN (\n            select \n                perfin as periodo, \n                nit \n            from \n                subsidio.subsi173 \n            group by \n                periodo, nit \n            order by \n                nit\n        ) as m2 \n        ON \n            m1.nit = m2.nit \n            AND m1.periodo = m2.periodo \n        where \n            STR_TO_DATE(CONCAT(SUBSTRING(m1.periodo, 1, 4), '-', SUBSTRING(m1.periodo, 5, 2), '-01'), '%Y-%m-%d')\n            BETWEEN DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n            AND DATE_FORMAT(CURDATE(), '%Y-%m-01')\n        group by \n            m1.periodo, m1.nit;\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-28 18:07:16,686 - INFO - CONEXION A BASE MINERVA\n2024-10-28 18:07:41,682 - INFO - Cargando query \n2024-10-28 18:07:46,519 - INFO - Cargada query, 14,767 registros finales obtenidos. --- 4.84 seconds ---\n2024-10-28 18:07:46,610 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 29.92 seconds ---\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#consulta-y-carga-de-codigo-de-documentos-desde-el-data-warehouse","title":"Consulta y Carga de C\u00f3digo de Documentos desde el Data Warehouse","text":"<p>Este c\u00f3digo define una consulta SQL en el diccionario <code>qr_structure2</code> que selecciona todos los registros de la tabla <code>BD_Dim_Codigo_Documento</code> en el Data Warehouse. La consulta se ejecuta y los datos resultantes se almacenan en el diccionario <code>df_codigos</code>. A trav\u00e9s de la funci\u00f3n <code>cargar_tablas</code>, los datos se cargan desde la base <code>dwh</code>, y la conexi\u00f3n se gestiona usando <code>obtener_conexion</code>, permitiendo el acceso a los c\u00f3digos de documentos para su uso en an\u00e1lisis o referencias en otros procesos.</p> <pre><code>#Lista de querys\nqr_structure2 = {\n    \"query\": \"\"\"\nSELECT * FROM dwh.BD_Dim_Codigo_Documento;\n    \"\"\"\n}\ndf_codigos = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure2['query'])\n\n\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor2, qr_structure2, df_codigos, logger)\n</code></pre> <pre><code>2024-10-28 18:07:46,623 - INFO - LECTURA DE QUERYS\n2024-10-28 18:07:46,625 - INFO - CONEXION A BASE DWH\n\n\n\nSELECT * FROM dwh.BD_Dim_Codigo_Documento;\n\n\n\n2024-10-28 18:07:47,189 - INFO - Cargando query \n2024-10-28 18:07:47,237 - INFO - Cargada query, 10 registros finales obtenidos. --- 0.05 seconds ---\n2024-10-28 18:07:47,331 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 0.70 seconds ---\n</code></pre>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#conversion-y-union-de-dataframes-para-documentos-de-morosidad","title":"Conversi\u00f3n y Uni\u00f3n de DataFrames para Documentos de Morosidad","text":"<p>Este bloque de c\u00f3digo realiza una serie de transformaciones y uniones sobre los DataFrames <code>df</code> y <code>df_cod</code> para estandarizar y combinar informaci\u00f3n de documentos. Primero, convierte las columnas <code>coddoc</code> en <code>df</code> y <code>COD_SUPERSUBSIDIO</code> en <code>df_cod</code> a tipo string y las normaliza a may\u00fasculas para asegurar coincidencias consistentes. Posteriormente, utiliza un <code>merge</code> con una uni\u00f3n <code>left</code> sobre <code>coddoc</code> y <code>COD_SUPERSUBSIDIO</code>, creando un nuevo DataFrame <code>df_merged</code> que incorpora informaci\u00f3n de ambos conjuntos de datos. Este proceso permite consolidar los datos de morosidad con los c\u00f3digos de documentos y verificar la correcta combinaci\u00f3n de datos.</p> <pre><code>df_cod = df_codigos['query']\n\ndf = df_structure['query']\n\n# Convertir 'coddoc' y 'CODDOC' a tipo string y a may\u00fasculas\ndf['coddoc'] = df['coddoc'].astype(str).str.upper()\ndf_cod['COD_SUPERSUBSIDIO'] = df_cod['COD_SUPERSUBSIDIO'].astype(str).str.upper()\n\n# Realizar el merge usando 'CODDOC' de df_cod\ndf_merged = df.merge(df_cod[['COD_SUPERSUBSIDIO', 'CODDOC']], left_on='coddoc', right_on='COD_SUPERSUBSIDIO', how='left')\n\n# Mostrar las primeras filas para verificar el resultado\ndf_merged.head()\n</code></pre> periodo_actualizacion nit coddoc valormor NumPerMora Tipo Notificacion razsoc COD_SUPERSUBSIDIO CODDOC 0 202304 1004345473 1 92800.0 2 Empresa 1 NAVARRETE ARIAS JULIANA GABRIELA 1 CC 1 202304 1004373978 1 148200.0 3 Empresa 0 BARROS MARTINEZ LISBETH KARIME 1 CC 2 202304 10090333 1 798000.0 3 Empresa 0 PUELLO MONTOYA GUSTAVO ADOLFO 1 CC 3 202304 10136103 1 162000.0 3 Empresa 0 ZAPATA RODRIGUEZ JHON JAIBER 1 CC 4 202304 1022360808 1 130200.0 2 Empresa 0 MORALES GARCIA HEIDY XIMENA 1 CC"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#transformaciones-del-dataframe-para-analisis-de-morosidad","title":"Transformaciones del DataFrame para An\u00e1lisis de Morosidad","text":"<p>Este bloque de c\u00f3digo realiza varias transformaciones en el DataFrame <code>df_merged</code>, que se renombra como <code>df</code>. Primero, se genera la columna <code>ID_AFILIADO</code>, que concatena los valores de <code>CODDOC</code> y <code>nit</code> para crear un identificador \u00fanico de afiliado. Luego, se crean las columnas <code>KeyHistoricos</code>, <code>PeriodosMoraRango</code>, y <code>KeyConvenio</code>, que proporcionan claves \u00fanicas y rangos de periodos en mora. La columna <code>per_act_fecha</code> formatea la fecha de actualizaci\u00f3n a <code>01/MM/AAAA</code> y la convierte a tipo fecha para facilitar an\u00e1lisis temporales. Finalmente, se eliminan columnas innecesarias y se reorganiza el DataFrame en un orden espec\u00edfico, dejando s\u00f3lo las columnas clave para el an\u00e1lisis.</p> <pre><code>df = df_merged\n# Crear la columna 'ID_AFILIADO' usando 'COD_GIASS' y 'nit'\ndf['ID_AFILIADO'] = df['CODDOC'].astype(str) + df['nit'].astype(str)\n\n# Agregar la columna 'KeyHistoricos'\ndf['KeyHistoricos'] = df['periodo_actualizacion'].astype(str) + df['nit'].astype(str) + df['Tipo'].str[:3]\n\n# Agregar la columna 'PeriodosMoraRango'\ndf['PeriodosMoraRango'] = df['NumPerMora'].apply(\n    lambda x: \"1-3 periodos\" if x &lt; 4 else \"4-10 periodos\" if 4 &lt;= x &lt; 11 else \"&gt;10 periodos\"\n)\n\n# Agregar la columna 'KeyConvenio'\ndf['KeyConvenio'] = df['periodo_actualizacion'].astype(str) + df['nit'].astype(str)\n\n# Crear la columna 'per_act_fecha' con formato \"01/MM/AAAA\"\ndf['per_act_fecha'] = \"01/\" + df['periodo_actualizacion'].str[-2:] + \"/\" + df['periodo_actualizacion'].str[:4]\n\n# Convertir 'per_act_fecha' a tipo fecha\ndf['per_act_fecha'] = pd.to_datetime(df['per_act_fecha'], format=\"%d/%m/%Y\")\ndf = df.drop(columns=['coddoc'])\n\ndf = df[['ID_AFILIADO',\n    'periodo_actualizacion',         \n    'nit',\n    'CODDOC',\n    'valormor',\n    'NumPerMora',\n    'Tipo',\n    'Notificacion',\n    'razsoc',\n    'COD_SUPERSUBSIDIO',\n    'KeyHistoricos',\n    'PeriodosMoraRango',\n    'KeyConvenio',\n    'per_act_fecha']]\n\ndf\n</code></pre> ID_AFILIADO periodo_actualizacion nit CODDOC valormor NumPerMora Tipo Notificacion razsoc COD_SUPERSUBSIDIO KeyHistoricos PeriodosMoraRango KeyConvenio per_act_fecha 0 CC1004345473 202304 1004345473 CC 92800.0 2 Empresa 1 NAVARRETE ARIAS JULIANA GABRIELA 1 2023041004345473Emp 1-3 periodos 2023041004345473 2023-04-01 1 CC1004373978 202304 1004373978 CC 148200.0 3 Empresa 0 BARROS MARTINEZ LISBETH KARIME 1 2023041004373978Emp 1-3 periodos 2023041004373978 2023-04-01 2 CC10090333 202304 10090333 CC 798000.0 3 Empresa 0 PUELLO MONTOYA GUSTAVO ADOLFO 1 20230410090333Emp 1-3 periodos 20230410090333 2023-04-01 3 CC10136103 202304 10136103 CC 162000.0 3 Empresa 0 ZAPATA RODRIGUEZ JHON JAIBER 1 20230410136103Emp 1-3 periodos 20230410136103 2023-04-01 4 CC1022360808 202304 1022360808 CC 130200.0 2 Empresa 0 MORALES GARCIA HEIDY XIMENA 1 2023041022360808Emp 1-3 periodos 2023041022360808 2023-04-01 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14762 NI901826054 202409 901826054 NI 208000.0 4 Empresa 0 APOYO EMPRESARIAL DEL CARIBE S A S 7 202409901826054Emp 4-10 periodos 202409901826054 2024-09-01 14763 NI901828239 202409 901828239 NI 36000.0 2 Empresa 0 CONSTRUCCIONES DEL CARIBE OITE SAS 7 202409901828239Emp 1-3 periodos 202409901828239 2024-09-01 14764 NI901828497 202409 901828497 NI 264600.0 2 Empresa 0 INDUSTRIAS &amp; FIBRA DEL MAGDALENA SAS 7 202409901828497Emp 1-3 periodos 202409901828497 2024-09-01 14765 NI901837744 202409 901837744 NI 156000.0 3 Empresa 0 EMPLOYER TALENT S A S 7 202409901837744Emp 1-3 periodos 202409901837744 2024-09-01 14766 NI901846652 202409 901846652 NI 1460000.0 2 Empresa 0 CONSORCIO INTER APURE 7 202409901846652Emp 1-3 periodos 202409901846652 2024-09-01 <p>14767 rows \u00d7 14 columns</p>"},{"location":"seccion/2.5-Fact_HistoricoMoraEmp/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Este bloque utiliza la funci\u00f3n <code>convertir_columnas_mayusculas</code> para renombrar las columnas clave del DataFrame <code>df</code> con un nuevo formato. Columnas como <code>valormor</code>, <code>NumPerMora</code>, <code>razsoc</code> y <code>PeriodosMoraRango</code> se transforman a <code>VALOR_MORA</code>, <code>NUMERO_PERSONAS_MORA</code>, <code>RAZON_SOCIAL</code> y <code>PERIODO_MODA_RANGO</code>, respectivamente, asegurando consistencia en el formato. Luego, el DataFrame <code>_df</code> se almacena en la tabla <code>BD_Fact_HistoricoMoraEmp</code> del Data Warehouse utilizando <code>guardar_en_dwh</code>, con la opci\u00f3n <code>if_exists='replace'</code> para sobrescribir los datos existentes, dejando s\u00f3lo las columnas renombradas y actualizadas para facilitar futuras consultas.</p> <pre><code>_df = convertir_columnas_mayusculas(df, {\n    'valormor':'VALOR_MORA',\n    'NumPerMora':'NUMERO_PERSONAS_MORA',\n    'razsoc':'RAZON_SOCIAL',\n    'PeriodosMoraRango':'PERIODO_MODA_RANGO'\n})\n\nguardar_en_dwh(_df, 'BD_Fact_HistoricoMoraEmp', logger, multiple=False, if_exists='replace')\n_df.columns.to_list()\n</code></pre> <pre><code>2024-10-28 18:07:47,517 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-28 18:07:47,519 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n\n\n2024-10-28 18:07:48,074 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoMoraEmp\n2024-10-28 18:08:02,979 - INFO - Tabla almacenada correctamente. 14,767 registros finales obtenidos.\n2024-10-28 18:08:04,924 - INFO - ALMACENAMIENTO ---  --- 17.41 seconds ---\n2024-10-28 18:08:04,926 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['ID_AFILIADO',\n 'PERIODO_ACTUALIZACION',\n 'NIT',\n 'CODDOC',\n 'VALOR_MORA',\n 'NUMERO_PERSONAS_MORA',\n 'TIPO',\n 'NOTIFICACION',\n 'RAZON_SOCIAL',\n 'COD_SUPERSUBSIDIO',\n 'KEYHISTORICOS',\n 'PERIODO_MODA_RANGO',\n 'KEYCONVENIO',\n 'PER_ACT_FECHA']\n</code></pre> <pre><code>_df['PERIODO_ACTUALIZACION'].unique()\n</code></pre> <pre><code>array(['202304', '202305', '202306', '202307', '202308', '202309',\n       '202310', '202311', '202312', '202401', '202402', '202403',\n       '202404', '202405', '202406', '202407', '202408', '202409'],\n      dtype=object)\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-28 18:08:05,026 - INFO - FINAL ETL --- 48.40 seconds ---\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/","title":"2.6 Fact Historico ANS","text":""},{"location":"seccion/2.6-FactHistoricoANS/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_HistoricoANS</code> se utiliza para extraer, transformar y cargar datos de tres fuentes de datos (<code>subsi02</code>, <code>subsi15</code>, y <code>subsi23</code>) en la base de datos <code>subsidio</code>, consolidando los registros relacionados con los historiales de afiliaci\u00f3n de empresas, trabajadores y beneficiarios en el Data Warehouse (DWH). La extracci\u00f3n se realiza mediante una consulta <code>UNION</code> para combinar datos de las tres tablas, asignando una fuente espec\u00edfica a cada entidad seg\u00fan su tabla de origen y limitando los resultados ultimos 18 meses. Posteriormente, se aplican transformaciones, como la generaci\u00f3n de una clave \u00fanica de historial y la estandarizaci\u00f3n de nombres de columnas, para facilitar su an\u00e1lisis y almacenamiento. Finalmente, el DataFrame <code>df_final</code> se almacena en la tabla <code>BD_Fact_HistoricoANS</code> del DWH, con registros en el log para monitorear el rendimiento y la precisi\u00f3n del proceso.</p>"},{"location":"seccion/2.6-FactHistoricoANS/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_HistoricoANS\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer historiales\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos hist\u00f3ricos\n    ETL_Script -&gt;&gt; ETL_Script: Procesa datos y genera claves \u00fanicas\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_HistoricoANS`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#etl","title":"ETL","text":""},{"location":"seccion/2.6-FactHistoricoANS/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script establece un entorno para la conexi\u00f3n a bases de datos y la manipulaci\u00f3n de datos utilizando SQLAlchemy y Pandas, optimizado para operaciones de carga y transformaci\u00f3n de datos. Al principio, el c\u00f3digo registra el tiempo de inicio de ejecuci\u00f3n con <code>time</code> y carga las librer\u00edas <code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>logging</code>, y <code>os</code>, necesarias para la manipulaci\u00f3n de datos y la creaci\u00f3n de conexiones a bases de datos. La configuraci\u00f3n del m\u00f3dulo personalizado ubicado en el directorio <code>../funciones</code> permite la importaci\u00f3n de funciones espec\u00edficas desde <code>Funciones.py</code>, incluyendo <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, <code>obtener_conexion</code>, <code>testfunciones</code>, y <code>setup_logger</code>. Estas funciones proporcionan m\u00e9todos para manejar conexiones, cargar y guardar datos en un Data Warehouse, y realizar pruebas o configuraciones de registros, creando una estructura modular y eficiente para el procesamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoANS.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoANS.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-28 18:27:02,075 - INFO - Importacion de funciones correcta, 28-10-2024 18:27\n2024-10-28 18:27:02,086 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#consulta-de-datos-y-union-de-tablas","title":"Consulta de Datos y Uni\u00f3n de Tablas","text":"<p>En este c\u00f3digo, se define una consulta SQL que combina datos de tres tablas (<code>subsi02</code>, <code>subsi15</code> y <code>subsi23</code>) de la base de datos <code>subsidio</code> mediante la operaci\u00f3n <code>UNION</code>. La consulta unifica registros de diferentes tipos de entidades (<code>Empresa</code>, <code>Trabajador</code>, y <code>Beneficiario</code>), asign\u00e1ndoles una fuente espec\u00edfica seg\u00fan la tabla de origen. Cada selecci\u00f3n dentro de la <code>UNION</code> crea un campo <code>periodo</code> basado en la fecha (<code>fecafi</code> o <code>fecpre</code>), que concatena el a\u00f1o y el mes. Tambi\u00e9n incluye los campos <code>coddoc</code>, <code>Identificacion</code>, <code>fecafi</code>, <code>fecapr</code>, <code>Tipo</code> y <code>Fuente</code>. En el caso de <code>subsi23</code>, se emplea una <code>INNER JOIN</code> con la tabla <code>subsi22</code> para obtener informaci\u00f3n adicional de identificaci\u00f3n, basada en el campo com\u00fan <code>codben</code>. La condici\u00f3n <code>WHERE</code> restringe los resultados a los a\u00f1os 2022 y 2023 para todas las tablas. </p> <p>La consulta est\u00e1 dise\u00f1ada para recuperar \u00fanicamente los registros de los \u00faltimos 18 periodos (meses) desde la fecha actual. Para lograrlo, En cada WHERE, se filtra el campo periodo utilizando la funci\u00f3n DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH), que calcula la fecha l\u00edmite de hace 18 meses. Esta condici\u00f3n es din\u00e1mica y siempre asegura que solo se devuelvan registros dentro de este rango de 18 meses.</p> <p>En el caso de la tabla subsi23, se emplea una INNER JOIN con subsi22 para obtener informaci\u00f3n adicional de identificaci\u00f3n, basada en el campo com\u00fan codben.</p> <p>Finalmente, el logger registra un mensaje indicando la lectura de la consulta, lo que ayuda a monitorear la ejecuci\u00f3n del proceso en tiempo real. La estructura generada en <code>qr_structure</code> estar\u00e1 lista para ejecutarse y almacenar los resultados en <code>df_structure</code>, permitiendo realizar an\u00e1lisis y procesamiento posteriores.</p> <pre><code>qr_structure = {\n    \"query\": \"\"\"\n                select \n                    CONCAT(YEAR(fecafi), LPAD(MONTH(fecafi), 2, '0')) AS periodo, \n                    coddoc, \n                    nit as Identifiacion, \n                    fecafi, \n                    fecapr, \n                    'Empresa' as Tipo, \n                    'subsi02' Fuente\n                from subsidio.subsi02\n                where CONCAT(YEAR(fecafi), LPAD(MONTH(fecafi), 2, '0')) &gt;= DATE_FORMAT(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH), '%Y%m')\n\n                UNION\n\n                select \n                    CONCAT(YEAR(s15.fecpre), LPAD(MONTH(s15.fecpre), 2, '0')) AS periodo, \n                    s15.coddoc, \n                    s15.cedtra as Identifiacion, \n                    s15.fecpre AS fecafi, \n                    IF(s15.fecpre &gt; s15.fecafi, s15.fecsis, s15.fecafi) AS fecapr, \n                    'Trabajador' as Tipo, \n                    'subsi15' Fuente\n                from subsidio.subsi15 as s15\n                where CONCAT(YEAR(s15.fecpre), LPAD(MONTH(s15.fecpre), 2, '0')) &gt;= DATE_FORMAT(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH), '%Y%m')\n\n                UNION\n\n                select \n                    CONCAT(YEAR(fecpre), LPAD(MONTH(fecpre), 2, '0')) AS periodo, \n                    k2.coddoc,  \n                    documento as Identificacion, \n                    fecpre AS fecafi,\n                    IF(fecpre &gt; fecafi, fecsis, fecafi) AS fecapr, \n                    'Beneficiario' as Tipo, \n                    'subsi23' Fuente\n                from subsidio.subsi23 as k1\n                INNER JOIN (select documento, coddoc, codben from subsidio.subsi22) as k2\n                ON k1.codben = k2.codben\n                where CONCAT(YEAR(fecpre), LPAD(MONTH(fecpre), 2, '0')) &gt;= DATE_FORMAT(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH), '%Y%m');\n\"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-28 18:27:02,097 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-28 18:27:02,178 - INFO - CONEXION A BASE MINERVA\n2024-10-28 18:27:02,731 - INFO - Cargando query \n2024-10-28 18:28:05,522 - INFO - Cargada query, 137,314 registros finales obtenidos. --- 1.05 minutes ---\n2024-10-28 18:28:05,624 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 1.06 minutes ---\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#procesamiento-de-datos-y-transformacion-en-df","title":"Procesamiento de Datos y Transformaci\u00f3n en <code>df</code>","text":"<p>En este script, partiendo de <code>df = df_structure['query']</code>, se realiza un procesamiento de datos detallado que incluye la creaci\u00f3n de nuevas columnas y la transformaci\u00f3n de tipos de datos para estructurar la informaci\u00f3n en <code>df_filtered</code>. Primero, se crea una columna <code>Validador</code> que eval\u00faa si <code>fecafi</code> es mayor que <code>fecapr</code>, asignando un valor de <code>0</code> o <code>1</code> seg\u00fan la comparaci\u00f3n. Posteriormente, se introduce una columna <code>tipide</code> utilizando <code>map</code> para convertir los c\u00f3digos en <code>coddoc</code> en identificadores num\u00e9ricos espec\u00edficos mediante <code>coddoc_mapping</code>, y se maneja cualquier valor no coincidente conservando su valor original.</p> <p>La columna <code>KeyHistoricos</code> se genera concatenando <code>periodo</code>, <code>tipide</code>, <code>Identifiacion</code> y las tres primeras letras de <code>Tipo</code>, formando una clave \u00fanica para cada registro. Luego, se filtran las filas donde <code>Validador</code> es igual a <code>1</code>, eliminando esta columna del <code>DataFrame</code> resultante <code>df_filtered</code>. Para mejorar la precisi\u00f3n en la manipulaci\u00f3n de fechas, <code>fecafi</code> y <code>fecapr</code> se convierten al tipo <code>datetime</code>.</p> <p>El proceso de ETL se registra en el <code>logger</code> tanto en t\u00e9rminos de la cantidad de registros finales obtenidos como del tiempo total de ejecuci\u00f3n. El <code>DataFrame</code> final <code>df_filtered</code> se muestra para validar las transformaciones realizadas, y se guarda opcionalmente en el Data Warehouse para futuras consultas y an\u00e1lisis.</p> <pre><code># Asumiendo que df = df_structure['query']\n# Crear la columna 'Validador' basado en la comparaci\u00f3n entre 'fecafi' y 'fecapr'\n\ndf = df_structure['query']\ndf['Validador'] = df.apply(lambda row: 0 if row['fecafi'] &gt; row['fecapr'] else 1, axis=1)\n\n# Agregar columna 'tipide' usando map en lugar de apply\ncoddoc_mapping = {\n    'CC': 1, 'CD': 8, 'CE': 4, 'NI': 7, 'NP': 5,\n    'PA': 6, 'PE': 9, 'PT': 15, 'RC': 3, 'TI': 2\n}\n\n# Mapear directamente el valor de 'coddoc' a 'tipide'\ndf['tipide'] = df['coddoc'].map(coddoc_mapping).fillna(df['coddoc'])\n\n# Agregar columna 'KeyHistoricos'\ndf['KeyHistoricos'] = df[['periodo', 'tipide', 'Identifiacion', 'Tipo']].agg(\n    lambda x: f\"{x['periodo']}{str(x['tipide'])}{x['Identifiacion']}{x['Tipo'][:3]}\", axis=1\n)\n\n# Filtrar filas donde Validador es 1 y hacer una copia expl\u00edcita\ndf_filtered = df[df['Validador'] == 1].copy()\n\n# Eliminar columna 'Validador'\ndf_filtered.drop(columns=['Validador'], inplace=True)\n\n# Cambiar el tipo de las columnas 'fecafi' y 'fecapr' a datetime\ndf_filtered['fecafi'] = pd.to_datetime(df_filtered['fecafi'], errors='coerce')\ndf_filtered['fecapr'] = pd.to_datetime(df_filtered['fecapr'], errors='coerce')\n\n# Registrar el n\u00famero de registros obtenidos\nlogger.info(f\"Proceso ETL completado, {len(df_filtered)} registros finales obtenidos.\")\n\n# Guardar en base de datos DWH (opcional)\n# df_filtered.to_sql('nombre_tabla_dwh', con=engine, index=False, if_exists='replace')\n\n# Loguear tiempo final de ejecuci\u00f3n\nlogger.info(f\"Finalizado ETL en {time.time() - start_time:.2f} segundos.\")\n\n# Mostrar el DataFrame final para ver los resultados\ndf_filtered\n</code></pre> <pre><code>2024-10-28 18:28:08,348 - INFO - Proceso ETL completado, 65582 registros finales obtenidos.\n2024-10-28 18:28:08,350 - INFO - Finalizado ETL en 66.31 segundos.\n</code></pre> periodo coddoc Identifiacion fecafi fecapr Tipo Fuente tipide KeyHistoricos 0 202310 CC 1000063770 2023-10-18 2023-10-18 Empresa subsi02 1 20231011000063770Emp 1 202309 CC 1000521027 2023-09-04 2023-09-04 Empresa subsi02 1 20230911000521027Emp 2 202407 CC 1000697509 2024-07-05 2024-07-05 Empresa subsi02 1 20240711000697509Emp 3 202408 CC 1001832447 2024-08-15 2024-08-15 Empresa subsi02 1 20240811001832447Emp 4 202310 CC 1002130415 2023-10-13 2023-10-13 Empresa subsi02 1 20231011002130415Emp ... ... ... ... ... ... ... ... ... ... 137309 202306 TI 1083053889 2023-06-27 2023-06-29 Beneficiario subsi23 2 20230621083053889Ben 137310 202306 TI 1083053907 2023-06-27 2023-06-29 Beneficiario subsi23 2 20230621083053907Ben 137311 202310 RC 1083055299 2023-10-18 2023-10-19 Beneficiario subsi23 3 20231031083055299Ben 137312 202311 PT 5200619 2023-11-08 2023-11-15 Beneficiario subsi23 15 202311155200619Ben 137313 202406 RC 1083069197 2024-06-01 2024-06-01 Beneficiario subsi23 3 20240631083069197Ben <p>65582 rows \u00d7 9 columns</p>"},{"location":"seccion/2.6-FactHistoricoANS/#generacion-de-id-unico-y-seleccion-de-columnas-en-df_final","title":"Generaci\u00f3n de ID \u00danico y Selecci\u00f3n de Columnas en <code>df_final</code>","text":"<p>En este c\u00f3digo, se crea la columna <code>ID AFILIADO</code> en <code>df_filtered</code> mediante la concatenaci\u00f3n de los valores en <code>coddoc</code> y <code>Identifiacion</code>, ambos convertidos a texto, para formar un identificador \u00fanico de afiliado. Seguidamente, el diccionario <code>_names</code> mapea los nombres originales de columnas a nombres m\u00e1s descriptivos y en may\u00fasculas, facilitando la estandarizaci\u00f3n y consistencia. La funci\u00f3n <code>convertir_columnas_mayusculas</code> aplica estos cambios a <code>df_filtered</code>.</p> <p>Finalmente, se genera <code>df_final</code> seleccionando \u00fanicamente las columnas clave, incluyendo <code>ID AFILIADO</code>, <code>PERIODO</code>, <code>CODIGO_DOCUMENTO</code>, <code>ID</code>, <code>FECHA_AFILIACION</code>, <code>FECHA_APROBACION</code>, <code>TIPO</code>, <code>FUENTE</code>, <code>TIPIDE</code> y <code>KEYHISTORICOS</code>, lo que asegura que el <code>DataFrame</code> final est\u00e9 estructurado de acuerdo con los requisitos para an\u00e1lisis o almacenamiento en la base de datos.</p> <pre><code>df_filtered['ID AFILIADO'] = df_filtered['coddoc'].astype(str) + df_filtered['Identifiacion'].astype(str)\n\n_names = {\n    'coddoc': 'CODIGO_DOCUMENTO',\n    'Identifiacion': 'ID',\n    'fecafi': 'FECHA_AFILIACION',\n    'fecapr': 'FECHA_APROBACION',\n    }\n\ndf = convertir_columnas_mayusculas(df_filtered, _names)\ndf_final = df[[\n    'ID AFILIADO',\n    'PERIODO',\n    'CODIGO_DOCUMENTO',\n    'ID',\n    'FECHA_AFILIACION',\n    'FECHA_APROBACION',\n    'TIPO',\n    'FUENTE',\n    'TIPIDE',\n    'KEYHISTORICOS'\n    ]]\n</code></pre>"},{"location":"seccion/2.6-FactHistoricoANS/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_HistoricoANS</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoANS', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\n</code></pre> <pre><code>2024-10-28 18:28:08,439 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-28 18:28:08,477 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-28 18:28:09,320 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoANS\n2024-10-28 18:28:19,527 - INFO - Tabla almacenada correctamente. 65,582 registros finales obtenidos.\n2024-10-28 18:28:19,659 - INFO - ALMACENAMIENTO ---  --- 11.22 seconds ---\n2024-10-28 18:28:19,661 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['ID AFILIADO',\n 'PERIODO',\n 'CODIGO_DOCUMENTO',\n 'ID',\n 'FECHA_AFILIACION',\n 'FECHA_APROBACION',\n 'TIPO',\n 'FUENTE',\n 'TIPIDE',\n 'KEYHISTORICOS']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-28 18:28:19,675 - INFO - FINAL ETL --- 77.63 seconds ---\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/","title":"2.7 Fact Historico Sub Prescrito","text":""},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_HistoricoSubPrescrito</code> permite extraer, transformar y cargar datos relacionados con la prescripci\u00f3n de subsidios de beneficiarios, consolidando la informaci\u00f3n en el Data Warehouse (DWH). Este flujo de trabajo se centra en los registros de prescripci\u00f3n de los \u00faltimos 18 meses, proporcionando detalles sobre beneficiarios, afiliaci\u00f3n, y tipo de subsidio, y clasificando los subsidios como \u201cRural\u201d o \u201cNo Rural\u201d seg\u00fan corresponda. A lo largo del proceso, se generan identificadores \u00fanicos y se estandarizan nombres de columnas para facilitar an\u00e1lisis y reportes.</p>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_HistoricoSubPrescrito\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para prescripci\u00f3n de subsidios\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de prescripci\u00f3n\n    ETL_Script -&gt;&gt; ETL_Script: Procesa y transforma datos\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_HistoricoSubPrescrito`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#etl","title":"ETL","text":""},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para manipular y cargar datos en un Data Warehouse, importando librer\u00edas clave (<code>pandas</code>, <code>numpy</code>, <code>sqlalchemy</code>) y configurando el m\u00f3dulo <code>Funciones.py</code> para ejecutar funciones personalizadas. Estas funciones permiten la normalizaci\u00f3n de datos, carga en DWH, ejecuci\u00f3n de consultas SQL, y manejo de registros de actividad, asegurando as\u00ed un flujo de trabajo completo para procesos ETL.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoSubPrescrito.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoSubPrescrito.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-27 08:42:19,568 - INFO - Importacion de funciones correcta, 27-10-2024 08:42\n2024-10-27 08:42:19,570 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#consulta-de-prescripcion-de-subsidios-por-beneficiario","title":"Consulta de Prescripci\u00f3n de Subsidios por Beneficiario","text":"<p>Esta consulta SQL obtiene informaci\u00f3n sobre la prescripci\u00f3n de subsidios en un per\u00edodo de 18 meses, agrupando datos de beneficiarios y sus detalles de afiliaci\u00f3n. Se accede a la tabla principal <code>subsi146</code> (representada como <code>k1</code>) y se realiza una uni\u00f3n interna con <code>subsi22</code> (<code>k2</code>) para obtener datos de identificaci\u00f3n como el sexo y un identificador \u00fanico de afiliado (<code>ID_AFILIADO</code>). Tambi\u00e9n se agrega una uni\u00f3n externa con <code>subsi09</code> (<code>k3</code>) para clasificar el tipo de subsidio (<code>tipsub</code>) como \"Rural\" o \"No Rural\" basado en el campo <code>tipsub</code>.</p> <p>Se filtran registros donde <code>estado</code> es \"P\" y <code>fecifz</code> est\u00e1 dentro del rango de 18 meses desde la fecha actual. La consulta agrupa los resultados por <code>periodo</code>, <code>codben</code>, <code>tipsub</code> y <code>ID_AFILIADO</code>, proporcionando el valor total de prescripci\u00f3n por cada beneficiario en el rango de tiempo especificado.</p> <pre><code># Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    select \n        fecifz as periodo, \n        SUM(valor) as valor_prescripcion, \n        k1.codben, \n        k2.sexo,\n        CONCAT(k2.coddoc, k2.documento) as ID_AFILIADO,\n        IF(k3.tipsub in ('A', 'G'), 'Rural', 'No Rural') as tipsub\n    from subsidio.subsi146 as k1\n    INNER JOIN (\n        select codben, sexo, coddoc, documento \n        from subsidio.subsi22\n    ) as k2\n    ON k1.codben = k2.codben\n    LEFT JOIN (\n        select pergir, cedres, codben, tipsub \n        from subsidio.subsi09\n    ) as k3\n    ON k1.pergirpag = k3.pergir\n    AND k1.cedres = k3.cedres\n    AND k1.codben = k3.codben\n    where estado = 'P' \n    AND fecifz BETWEEN DATE_SUB(CURDATE(), INTERVAL 18 MONTH) AND CURDATE()\n    group by fecifz, k1.codben, tipsub, CONCAT(k2.coddoc, k2.documento);\n    \"\"\"\n}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-27 08:42:19,582 - INFO - LECTURA DE QUERYS\n\n\n\n    select \n        fecifz as periodo, \n        SUM(valor) as valor_prescripcion, \n        k1.codben, \n        k2.sexo,\n        CONCAT(k2.coddoc, k2.documento) as ID_AFILIADO,\n        IF(k3.tipsub in ('A', 'G'), 'Rural', 'No Rural') as tipsub\n    from subsidio.subsi146 as k1\n    INNER JOIN (\n        select codben, sexo, coddoc, documento \n        from subsidio.subsi22\n    ) as k2\n    ON k1.codben = k2.codben\n    LEFT JOIN (\n        select pergir, cedres, codben, tipsub \n        from subsidio.subsi09\n    ) as k3\n    ON k1.pergirpag = k3.pergir\n    AND k1.cedres = k3.cedres\n    AND k1.codben = k3.codben\n    where estado = 'P' \n    AND fecifz BETWEEN DATE_SUB(CURDATE(), INTERVAL 18 MONTH) AND CURDATE()\n    group by fecifz, k1.codben, tipsub, CONCAT(k2.coddoc, k2.documento);\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 08:42:19,595 - INFO - CONEXION A BASE MINERVA\n2024-10-27 08:42:20,135 - INFO - Cargando query \n2024-10-27 08:42:22,715 - INFO - Cargada query, 1,594 registros finales obtenidos. --- 2.58 seconds ---\n2024-10-27 08:42:22,806 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 3.21 seconds ---\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#transformacion-de-columnas-y-creacion-de-campos-de-fecha-personalizados","title":"Transformaci\u00f3n de Columnas y Creaci\u00f3n de Campos de Fecha Personalizados","text":"<p>Este c\u00f3digo realiza una serie de transformaciones en el DataFrame <code>df</code>, principalmente para estandarizar formatos y crear campos derivados basados en fechas. Primero, cambia el tipo de la columna <code>codben</code> a texto y convierte <code>periodo</code> en un objeto de fecha (<code>datetime</code>). Luego, agrega las columnas <code>A\u00f1o</code> y <code>Mes</code>, derivadas del a\u00f1o y el mes de <code>periodo</code>, y crea una columna <code>Mes_personalizado</code> en formato de dos d\u00edgitos para asegurar la uniformidad.</p> <p>A partir de <code>A\u00f1o</code> y <code>Mes_personalizado</code>, se crea una columna <code>Periodo_2</code> en formato \"A\u00f1oMes\" y se elimina el campo <code>periodo</code> original, renombrando <code>Periodo_2</code> a <code>Periodo</code> para estandarizar el nombre. La columna <code>KeyPeriodos</code> tambi\u00e9n se agrega como un duplicado de <code>Periodo</code>, posiblemente para facilitar b\u00fasquedas o uniones en etapas posteriores. Finalmente, la funci\u00f3n <code>convertir_columnas_mayusculas</code> aplica el diccionario <code>_names</code>, renombrando <code>codben</code> a <code>CODIGO_BENEFICIARIO</code> y asegurando que los nombres de columnas est\u00e9n en un formato consistente.</p> <pre><code>df = df_structure['query']\n\n# Cambiar el tipo de columna 'codben' a texto\ndf['codben'] = df['codben'].astype(str)\n\n# Cambiar el tipo de columna 'periodo' a fecha\ndf['periodo'] = pd.to_datetime(df['periodo'])\n\n# Agregar columnas A\u00f1o y Mes\ndf['A\u00f1o'] = df['periodo'].dt.year.astype(str)\ndf['Mes'] = df['periodo'].dt.month.astype(str)\n\n# Agregar la columna Mes_personalizado (mes con formato de 2 d\u00edgitos)\ndf['Mes_personalizado'] = df['Mes'].apply(lambda x: x.zfill(2))\n\n# Crear la columna Periodo_2 combinando A\u00f1o y Mes_personalizado\ndf['Periodo_2'] = df['A\u00f1o'] + df['Mes_personalizado']\n\n# Eliminar la columna 'periodo' original\ndf = df.drop(columns=['periodo'])\n\n# Renombrar la columna Periodo_2 a Periodo\ndf = df.rename(columns={'Periodo_2': 'Periodo'})\n\n# Agregar la columna KeyPeriodos\ndf['KeyPeriodos'] = df['Periodo']\n\n\n_names = {\n    'codben': 'CODIGO_BENEFICIArIO',\n}\n\ndf_final =  convertir_columnas_mayusculas(df,_names)\n</code></pre>"},{"location":"seccion/2.7-FactHistoricoSubPrescrito/#guardado-de-datos-en-dwh-y-verificacion-de-valores-unicos-del-campo-periodo","title":"Guardado de Datos en DWH y Verificaci\u00f3n de Valores \u00danicos del Campo 'PERIODO'","text":"<p>Este bloque de c\u00f3digo guarda el DataFrame <code>df_final</code> en el Data Warehouse (DWH) dentro de la tabla <code>BD_Fact_HistoricoSubPrescrito</code> usando la funci\u00f3n <code>guardar_en_dwh</code>. El par\u00e1metro <code>if_exists='replace'</code> asegura que los datos existentes en la tabla ser\u00e1n reemplazados, y <code>multiple=False</code> indica que la carga se har\u00e1 en una sola operaci\u00f3n.</p> <p>Finalmente, se obtienen y muestran los valores \u00fanicos de la columna <code>PERIODO</code> en <code>df_final</code>, permitiendo validar que el campo contiene los valores esperados antes de continuar con el an\u00e1lisis o procesamiento adicional. Esto ayuda a confirmar la correcta generaci\u00f3n y transformaci\u00f3n de los datos en <code>PERIODO</code>.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoSubPrescrito', logger, multiple=False, if_exists='replace')\n# Obtener e imprimir los valores \u00fanicos del campo 'PERIODO'\ndf_final['PERIODO'].unique()\n</code></pre> <pre><code>2024-10-27 08:44:13,152 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 08:44:13,154 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 08:44:13,694 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoSubPrescrito\n2024-10-27 08:44:15,152 - INFO - Tabla almacenada correctamente. 1,594 registros finales obtenidos.\n2024-10-27 08:44:15,260 - INFO - ALMACENAMIENTO ---  --- 2.11 seconds ---\n2024-10-27 08:44:15,261 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\narray(['202309', '202308', '202409', '202406', '202304', '202305',\n       '202311', '202403', '202404', '202307', '202310', '202312',\n       '202405', '202401', '202408', '202306', '202407', '202402'],\n      dtype=object)\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 08:42:24,855 - INFO - FINAL ETL --- 5.31 seconds ---\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/","title":"2.8 Fact Historico Recobro","text":""},{"location":"seccion/2.8-FactHistoricoRecobro/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_HistoricoRecobro</code> se enfoca en extraer, transformar y cargar datos hist\u00f3ricos de subsidios y beneficiarios desde la base de datos <code>subsidio</code> a un Data Warehouse (DWH). Utilizando consultas SQL, se combinan las tablas <code>subsi219</code> y <code>subsi220</code> para reunir informaci\u00f3n relevante sobre los subsidios otorgados y los detalles de los beneficiarios en los \u00faltimos 18 meses. La transformaci\u00f3n incluye el procesamiento de datos num\u00e9ricos y la generaci\u00f3n de claves \u00fanicas para identificar a los beneficiarios y su historial. Al final, se carga el DataFrame resultante en la tabla <code>BD_Fact_HistoricoRecobro</code> del DWH, registrando cada paso del proceso en el log para garantizar un flujo de trabajo supervisado y eficiente.</p>"},{"location":"seccion/2.8-FactHistoricoRecobro/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_HistoricoRecobro\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer datos de subsidios\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de subsidios y beneficiarios\n    ETL_Script -&gt;&gt; ETL_Script: Procesa y normaliza los datos, generando claves \u00fanicas\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_HistoricoRecobro`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#etl","title":"ETL","text":""},{"location":"seccion/2.8-FactHistoricoRecobro/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para manipular y cargar datos en un Data Warehouse, importando librer\u00edas clave (<code>pandas</code>, <code>numpy</code>, <code>sqlalchemy</code>) y configurando el m\u00f3dulo <code>Funciones.py</code> para ejecutar funciones personalizadas. Estas funciones permiten la normalizaci\u00f3n de datos, carga en DWH, ejecuci\u00f3n de consultas SQL, y manejo de registros de actividad, asegurando as\u00ed un flujo de trabajo completo para procesos ETL.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoRecobro.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoRecobro.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-27 08:55:51,367 - INFO - Importacion de funciones correcta, 27-10-2024 08:55\n2024-10-27 08:55:51,374 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#consulta-de-subsidios-y-beneficiarios-con-filtrado-de-periodo","title":"Consulta de Subsidios y Beneficiarios con Filtrado de Per\u00edodo","text":"<p>Esta consulta SQL extrae datos de beneficiarios y subsidios desde las tablas <code>subsi219</code> y <code>subsi220</code> en una base de datos de subsidios. La tabla <code>subsi219</code> (<code>t1</code>) contiene informaci\u00f3n del subsidio, mientras que <code>subsi220</code> (<code>t2</code>) proporciona detalles adicionales del beneficiario, como <code>valabo</code> y <code>periodo</code>. Se realiza una uni\u00f3n <code>INNER JOIN</code> entre ambas tablas en el campo <code>idereg</code> de <code>t1</code> y <code>ide178</code> de <code>t2</code>, vinculando los subsidios con los beneficiarios correspondientes.</p> <p>Se filtran los registros donde <code>t1.tipsub</code> es igual a <code>'S'</code>, y <code>periodo</code> se convierte en una fecha en formato <code>'%Y-%m-%d'</code>. Solo se incluyen los registros en un rango de 18 meses desde la fecha actual. La consulta tambi\u00e9n genera una columna <code>ID_AFILIADO</code>, creada a partir de la concatenaci\u00f3n de <code>coddoc</code> y <code>cedtra</code> para identificar de forma \u00fanica a cada beneficiario.</p> <pre><code># Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    select \n        t1.tipsub,\n        t1.cedtra, \n        t1.valdes, \n        t2.valabo, \n        t1.estado, \n        t1.idereg, \n        t1.categoria, \n        t2.ide178, \n        t2.cedtra as cedtra2, \n        t2.codben, \n        t2.periodo,\n        CONCAT(t1.coddoc, t1.cedtra) as ID_AFILIADO\n    from subsidio.subsi219 as t1\n    INNER JOIN subsidio.subsi220 as t2\n    ON t1.idereg = t2.ide178\n    where t1.tipsub = 'S'\n    AND STR_TO_DATE(CONCAT(SUBSTRING(t2.periodo, 1, 4), '-', SUBSTRING(t2.periodo, 5, 2), '-01'), '%Y-%m-%d')\n    BETWEEN DATE_SUB(CURDATE(), INTERVAL 18 MONTH) AND CURDATE();\n    \"\"\"\n}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-27 08:55:51,385 - INFO - LECTURA DE QUERYS\n\n\n\n    select \n        t1.tipsub,\n        t1.cedtra, \n        t1.valdes, \n        t2.valabo, \n        t1.estado, \n        t1.idereg, \n        t1.categoria, \n        t2.ide178, \n        t2.cedtra as cedtra2, \n        t2.codben, \n        t2.periodo,\n        CONCAT(t1.coddoc, t1.cedtra) as ID_AFILIADO\n    from subsidio.subsi219 as t1\n    INNER JOIN subsidio.subsi220 as t2\n    ON t1.idereg = t2.ide178\n    where t1.tipsub = 'S'\n    AND STR_TO_DATE(CONCAT(SUBSTRING(t2.periodo, 1, 4), '-', SUBSTRING(t2.periodo, 5, 2), '-01'), '%Y-%m-%d')\n    BETWEEN DATE_SUB(CURDATE(), INTERVAL 18 MONTH) AND CURDATE();\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 08:55:51,399 - INFO - CONEXION A BASE MINERVA\n2024-10-27 08:55:51,951 - INFO - Cargando query \n2024-10-27 08:55:53,728 - INFO - Cargada query, 10,619 registros finales obtenidos. --- 1.78 seconds ---\n2024-10-27 08:55:53,822 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 2.42 seconds ---\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#transformacion-de-datos-y-normalizacion-de-columnas-en-dataframe-de-subsidios","title":"Transformaci\u00f3n de Datos y Normalizaci\u00f3n de Columnas en DataFrame de Subsidios","text":"<p>Este c\u00f3digo aplica varias transformaciones y validaciones en el DataFrame <code>df</code> para asegurar que los datos de subsidios y beneficiarios est\u00e9n en el formato correcto y se integren con facilidad. Se convierte <code>valabo</code> y <code>valdes</code> a tipo <code>float</code> para manejar valores monetarios, y <code>codben</code> se convierte a texto para evitar errores de formato en operaciones posteriores. </p> <p>Una nueva columna <code>KeyHistorico</code> se crea como identificador \u00fanico, combinando <code>periodo</code>, <code>codben</code>, y el sufijo <code>\"Ben\"</code>. La columna <code>Comparacion</code> compara <code>cedtra</code> y <code>cedtra2</code> y, si todos los valores coinciden (<code>True</code>), se eliminan las columnas <code>cedtra2</code> y <code>Comparacion</code>, limpiando el DataFrame de datos redundantes.</p> <p>Finalmente, se estandarizan los nombres de las columnas con el diccionario <code>_names</code> mediante la funci\u00f3n <code>convertir_columnas_mayusculas</code>, renombrando <code>codben</code> a <code>CODIGO_BENEFICIARIO</code> y <code>cedtra</code> a <code>CEDULA_TRABAJADOR</code>. El m\u00e9todo <code>columns.tolist()</code> permite ver el listado final de nombres de columnas en <code>df_final</code>.</p> <pre><code>df = df_structure['query']\n\n\n# Cambiar los tipos de las columnas: 'valabo' y 'valdes' a formato monetario (float) y 'codben' a texto\ndf['valabo'] = pd.to_numeric(df['valabo'], errors='coerce')\ndf['valdes'] = pd.to_numeric(df['valdes'], errors='coerce')\ndf['codben'] = df['codben'].astype(str)\n\n# Agregar la columna 'KeyHistorico'\ndf['KeyHistorico'] = df['periodo'] + df['codben'] + \"Ben\"\n# Comparar si 'cedtra' es igual a 'cedtrad' y crear una nueva columna 'Comparacion'\ndf['Comparacion'] = df['cedtra'] == df['cedtra2']\n\n# Verificar si todos los valores de 'Comparacion' son True\nif df['Comparacion'].all():\n    # Si todos los valores son True, eliminar la columna 'cedtrad'\n    df = df.drop(columns=['cedtra2','Comparacion'])\n\ndf\n\n_names = {\n    'codben': 'CODIGO_BENEFICIArIO',\n    'cedtra': 'CEDULA TrABAJADOr',\n}\n\ndf_final =  convertir_columnas_mayusculas(df,_names)\ndf_final.columns.tolist()\n</code></pre> <pre><code>['TIPSUB',\n 'CEDULA TRABAJADOR',\n 'VALDES',\n 'VALABO',\n 'ESTADO',\n 'IDEREG',\n 'CATEGORIA',\n 'IDE178',\n 'CODIGO_BENEFICIARIO',\n 'PERIODO',\n 'ID_AFILIADO',\n 'KEYHISTORICO']\n</code></pre>"},{"location":"seccion/2.8-FactHistoricoRecobro/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_HistoricoRecobro</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoRecobro', logger, multiple=False, if_exists='replace')\ndf_final['PERIODO'].unique()\n</code></pre> <pre><code>2024-10-27 08:55:53,869 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 08:55:53,870 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 08:55:54,430 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoRecobro\n2024-10-27 08:55:57,738 - INFO - Tabla almacenada correctamente. 10,619 registros finales obtenidos.\n2024-10-27 08:55:57,852 - INFO - ALMACENAMIENTO ---  --- 3.98 seconds ---\n2024-10-27 08:55:57,854 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\narray(['202401', '202309', '202310', '202402', '202305', '202306',\n       '202307', '202308', '202311', '202312', '202405', '202407',\n       '202404', '202406', '202408', '202403', '202410', '202409'],\n      dtype=object)\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 08:55:57,866 - INFO - FINAL ETL --- 6.51 seconds ---\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/","title":"2.9 Fact Historico Recuperado","text":""},{"location":"seccion/2.9-FactHistoricoRecuperado/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>Fact_HistoricoRecuperado</code> se utiliza para extraer, transformar y cargar datos hist\u00f3ricos de convenios en mora desde una base de datos de subsidios a un Data Warehouse (DWH). Este proceso recopila informaci\u00f3n sobre convenios y su estado de pago, clasificando la antig\u00fcedad de la deuda en categor\u00edas de d\u00edas y generando identificadores \u00fanicos para cada registro. La transformaci\u00f3n del DataFrame incluye la limpieza, normalizaci\u00f3n y creaci\u00f3n de claves \u00fanicas, finalizando con la carga de datos en la tabla <code>BD_Fact_HistoricoRecuperado</code> en el DWH. El proceso registra cada paso en un archivo de log, asegurando trazabilidad y eficiencia en el flujo de trabajo ETL.</p>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - Fact_HistoricoRecuperado\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer datos de convenios en mora\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de convenios\n    ETL_Script -&gt;&gt; ETL_Script: Procesa y clasifica los datos en mora\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_HistoricoRecuperado`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#etl","title":"ETL","text":""},{"location":"seccion/2.9-FactHistoricoRecuperado/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script configura un entorno para la carga, transformaci\u00f3n y almacenamiento de datos en una base de datos, utilizando SQLAlchemy para manejar las conexiones y Pandas para manipular datos tabulares. Tras iniciar un contador de tiempo con <code>time</code>, el c\u00f3digo incorpora el m\u00f3dulo <code>logging</code> para registrar eventos y errores en el flujo de trabajo. A trav\u00e9s de <code>sys.path</code>, el script carga funciones espec\u00edficas desde <code>Funciones.py</code>, incluyendo <code>convertir_columnas_mayusculas</code>, <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, <code>obtener_conexion</code>, <code>testfunciones</code> y <code>setup_logger</code>, que proporcionan m\u00e9todos para manipular columnas, almacenar datos en el data warehouse, cargar tablas y configurar los logs. De esta manera, el flujo est\u00e1 optimizado para realizar operaciones ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) de forma eficiente y modular.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>HistoricoCuota.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='HistoricoCuota.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 23:01:36,722 - INFO - Importacion de funciones correcta, 26-10-2024 23:01\n2024-10-26 23:01:36,723 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#consulta-sql-para-extraccion-y-transformacion-de-datos-de-subsidio","title":"Consulta SQL para Extracci\u00f3n y Transformaci\u00f3n de Datos de Subsidio","text":"<p>El script define una consulta SQL compleja dentro de un diccionario <code>qr_structure</code>, la cual se utiliza para extraer datos combinados de varias tablas en la base de datos <code>subsidio</code>. La consulta extrae informaci\u00f3n relevante como <code>nit</code>, <code>valmoraTotal</code>, <code>estadoConv</code>, <code>perpag</code>, <code>valpag</code>, <code>periodorefer</code> y <code>perini</code>. Estos datos se obtienen mediante la uni\u00f3n de tres subconsultas (<code>subsi113</code>, <code>subsi115</code> y <code>subsi11</code>), que agrupan y relacionan informaci\u00f3n financiera, periodos y estados de convenios, utilizando criterios espec\u00edficos de coincidencia de marcas y documentos. Esto permite preparar una estructura tabular consolidada y lista para an\u00e1lisis posterior. Finalmente, <code>df_structure</code> se inicializa como un diccionario, preparado para almacenar el resultado de la consulta una vez ejecutada, mientras que <code>logger.info</code> se encarga de registrar el inicio de la ejecuci\u00f3n de las consultas para su trazabilidad.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n    select \n        nit, \n        valmoraTotal, \n        estadoConv, \n        perpag, \n        valpag, \n        periodorefer, \n        perini \n    from (\n        select \n            u1.nit, \n            valcon as valmoraTotal, \n            perini, \n            perfin,\n            u1.estado as estadoConv, \n            marapo, \n            docapo, \n            marca113, \n            documento113  \n        from (\n            SELECT \n                nit, \n                valcon, \n                perini, \n                perfin, \n                estado, \n                marca as marca113,\n                documento as documento113 \n            FROM subsidio.subsi113\n        ) as u1\n        JOIN (\n            SELECT * \n            FROM subsidio.subsi115\n        ) as u2\n        ON u1.marca113 = u2.marca\n        AND u1.documento113 = u2.documento\n    ) as b1\n    left join (\n        SELECT \n            marca, \n            documento, \n            periodo as periodorefer, \n            valpag, \n            CONCAT( SUBSTRING(fecpag,1,4) , SUBSTRING(fecpag,6,2) ) as perpag \n        FROM subsidio.subsi11\n    ) as b2\n    ON b1.marapo = b2.marca\n    AND b1.docapo = b2.documento;\n\"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-26 23:01:36,736 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-26 23:01:36,750 - INFO - CONEXION A BASE MINERVA\n2024-10-26 23:01:37,334 - INFO - Cargando query \n2024-10-26 23:01:37,832 - INFO - Cargada query, 4,474 registros finales obtenidos. --- 0.50 seconds ---\n2024-10-26 23:01:37,919 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 1.17 seconds ---\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#procesamiento-de-datos-con-pandas-para-analisis-de-mora-en-convenios","title":"Procesamiento de Datos con Pandas para An\u00e1lisis de Mora en Convenios","text":"<p>Este c\u00f3digo realiza una serie de transformaciones en el DataFrame <code>df</code> a partir de los resultados obtenidos de una consulta SQL, enriqueciendo y filtrando los datos para un an\u00e1lisis m\u00e1s profundo del estado de mora en convenios. En primer lugar, se crea una clave \u00fanica <code>HistoricoKey</code> concatenando <code>perpag</code>, <code>nit</code> y un identificador constante. Luego, se filtran las filas para incluir solo aquellas en las que <code>valpag</code> no es nulo. Se genera la columna <code>perinifecha</code> con el primer d\u00eda del mes y a\u00f1o correspondiente en <code>perini</code>, convirti\u00e9ndola al tipo fecha. Adem\u00e1s, se a\u00f1ade la fecha actual y se establece un periodo de an\u00e1lisis, restando dos meses al mes actual.</p> <p>Se calcula el n\u00famero de d\u00edas entre <code>PeriodoAnalisis</code> y <code>perinifecha</code> exclusivamente para registros cuyo <code>estadoConv</code> es \"A\", y se clasifica la antig\u00fcedad de la deuda en categor\u00edas (<code>Edad</code>). Finalmente, se agrega una columna <code>KeyXML4</code> como un identificador adicional, se eliminan columnas intermedias no necesarias y se renombran campos con el uso de la funci\u00f3n <code>convertir_columnas_mayusculas</code>, empleando un mapeo proporcionado en <code>_name</code>, lo que asegura consistencia en los nombres para un an\u00e1lisis posterior y facilita su integraci\u00f3n en reportes o procesos ETL.</p> <pre><code>from datetime import datetime\ndf = df_structure['query']\n\n# Agregar la columna 'HistoricoKey'\ndf.loc[:, 'HistoricoKey'] = df['perpag'].astype(str) + df['nit'].astype(str) + \"Emp\"\n\n# Filtrar filas donde 'valpag' no es nulo (aseguramos que df sea una copia del DataFrame original)\ndf = df[df['valpag'].notnull()].copy()\n\n# Crear columna 'perinifecha' con formato \"01/MM/AAAA\"\ndf.loc[:, 'perinifecha'] = \"01/\" + df['perini'].str[-2:] + \"/\" + df['perini'].str[:4]\n\n# Convertir 'perinifecha' a tipo fecha\ndf.loc[:, 'perinifecha'] = pd.to_datetime(df['perinifecha'], format=\"%d/%m/%Y\")\n\n# Agregar la columna 'FechaActual' con la fecha actual\ndf.loc[:, 'FechaActual'] = pd.to_datetime(datetime.now().date())\n\n# Agregar la columna 'PeriodoAnalisis' restando 2 meses al final del mes actual\ndf.loc[:, 'PeriodoAnalisis'] = df['FechaActual'] - pd.offsets.MonthEnd(2)\n\n# Calcular los d\u00edas entre 'PeriodoAnalisis' y 'perinifecha' solo si 'estadoConv' es 'A'\ndf.loc[:, 'DiasEntre'] = df.apply(\n    lambda row: (row['PeriodoAnalisis'] - row['perinifecha']).days if row['estadoConv'] == 'A' else None, axis=1\n)\n\n# Crear la columna 'Edad' basada en los valores de 'DiasEntre'\ndef calcular_edad(dias):\n    if pd.isnull(dias):\n        return None\n    elif 30 &lt;= dias &lt;= 60:\n        return \"30-60 d\u00edas\"\n    elif 61 &lt;= dias &lt;= 90:\n        return \"61-90 d\u00edas\"\n    elif dias &gt; 90:\n        return \"&gt; 91 d\u00edas\"\n    else:\n        return None\n\ndf.loc[:, 'Edad'] = df['DiasEntre'].apply(calcular_edad)\n\n# Crear la columna 'KeyXML4'\ndf.loc[:, 'KeyXML4'] = df['periodorefer'].astype(str) + df['nit'].astype(str)\n\n# Eliminar las columnas innecesarias\ndf = df.drop(columns=['perinifecha', 'PeriodoAnalisis', 'FechaActual'])\n\n\n_name = {\n    'valmoraTotal':'VALOR_MORA_TOTAL',\n    'estadoConv':'ESTADO',\n    'perpag':'Periodo_pagado',\n    'valpag':'VALOr_PAGADO',\n    'periodorefer':'PErIODO_rEFErENCIA',\n    'perini':'PErIODO_INICIAL'\n}\n\ndf_final = convertir_columnas_mayusculas(df,_name)\ndf_final\n</code></pre>"},{"location":"seccion/2.9-FactHistoricoRecuperado/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_HistoricoRecuperado</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_HistoricoRecuperado', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\n</code></pre> <pre><code>2024-10-26 23:01:38,004 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 23:01:38,006 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 23:01:38,561 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_HistoricoRecuperado\n2024-10-26 23:01:40,262 - INFO - Tabla almacenada correctamente. 3,512 registros finales obtenidos.\n2024-10-26 23:01:40,383 - INFO - ALMACENAMIENTO ---  --- 2.38 seconds ---\n2024-10-26 23:01:40,384 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['NIT',\n 'VALOR_MORA_TOTAL',\n 'ESTADO',\n 'PERIODO_PAGADO',\n 'VALOR_PAGADO',\n 'PERIODO_REFERENCIA',\n 'PERIODO_INICIAL',\n 'HISTORICOKEY',\n 'DIASENTRE',\n 'EDAD',\n 'KEYXML4']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 23:01:40,398 - INFO - FINAL ETL --- 3.70 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/","title":"3.1 Fact Transacciones Ventas","text":""},{"location":"seccion/3.1-FactTransaccionesVentas/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>FactTransaccionesVentas</code> unifica y transforma datos de ventas de servicios provenientes de m\u00faltiples tablas (<code>servi21</code>, <code>servi20</code> y <code>servi03</code>) de la base de datos <code>servicios</code>, carg\u00e1ndolos en una \u00fanica tabla de ventas en el Data Warehouse (DWH). La extracci\u00f3n de registros de venta est\u00e1 limitada a los \u00faltimos 18 meses, asegurando que solo se procesen transacciones recientes. Se realizan transformaciones de datos, como la normalizaci\u00f3n de valores faltantes y creaci\u00f3n de claves \u00fanicas de afiliados, que optimizan la estructura de los datos para an\u00e1lisis posteriores. Finalmente, los datos se cargan en la tabla <code>BD_Fact_Ventas_Servicios</code> en el DWH, registrando cada etapa del proceso en un log para asegurar trazabilidad y control de calidad.</p>"},{"location":"seccion/3.1-FactTransaccionesVentas/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactTransaccionesVentas\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Servicios_DB as Base de Datos Servicios\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Servicios_DB: Conecta a BD Servicios\n    Servicios_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Servicios_DB: Ejecuta consulta SQL para extraer ventas de servicios\n    Servicios_DB --&gt;&gt; ETL_Script: Retorna datos de ventas de servicios\n    ETL_Script -&gt;&gt; ETL_Script: Limpia, transforma y valida datos de ventas\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en `BD_Fact_Ventas_Servicios`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#etl","title":"ETL","text":""},{"location":"seccion/3.1-FactTransaccionesVentas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para manipular y cargar datos en un Data Warehouse, importando librer\u00edas clave (<code>pandas</code>, <code>numpy</code>, <code>sqlalchemy</code>) y configurando el m\u00f3dulo <code>Funciones.py</code> para ejecutar funciones personalizadas. Estas funciones permiten la normalizaci\u00f3n de datos, carga en DWH, ejecuci\u00f3n de consultas SQL, y manejo de registros de actividad, asegurando as\u00ed un flujo de trabajo completo para procesos ETL.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger, guardar_en_dwh\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 23-10-2024 17:47\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>TransaccionesVentas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='TransaccionesVentas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-23 17:47:33,971 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#consulta-de-ventas-de-servicios-y-detalles-de-beneficiarios","title":"Consulta de Ventas de Servicios y Detalles de Beneficiarios","text":"<p>Esta consulta SQL extrae informaci\u00f3n de ventas de servicios de la tabla <code>servi21</code> en combinaci\u00f3n con datos adicionales de las tablas <code>servi20</code> y <code>servi03</code>. La uni\u00f3n se realiza entre <code>servi21</code> y <code>servi20</code> usando los campos <code>marca</code> y <code>documento</code>, y se limitan los resultados a las ventas en un per\u00edodo de 18 meses desde la fecha actual.</p> <p>El resultado incluye detalles de cada venta, como <code>DOCUMENTO_VENTA</code>, <code>MARCA_VENTA</code>, <code>PERIODO</code>, <code>FECHA_VENTA</code>, <code>TIPO_BENEFICIARIO</code>, <code>CATEGORIA_VENTA</code>, y otros campos descriptivos de los servicios y su costo (<code>VALOR_SERVICIO</code>, <code>VALOR_DESCONTADO</code>, <code>VALOR_COSTO</code>, <code>VALOR_SUBSIDIO</code>). La tabla <code>servi03</code> se usa para obtener descripciones de categor\u00edas, mientras que <code>servi20</code> aporta detalles de estado y fechas.</p> <p>Este c\u00f3digo asegura la configuraci\u00f3n para ejecutar la consulta y cargar los datos en <code>df_structure</code> para su posterior procesamiento y an\u00e1lisis.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"BD_Fact_Ventas_Servicios\":'''SELECT \n        s21.documento DOCUMENTO_VENTA,\n        s21.marca AS MARCA_VENTA,\n        DATE_FORMAT(s20.fecha, '%Y%m') AS PERIODO,\n        s20.fecha FECHA_VENTA,\n        s21.sec as CONSECUTIVO_VENTA,\n        s20.estado as ESTADO_VENTA,\n        s20.fecest as FECHA_ESTADO,\n        s21.tipben AS TIPO_BENEFICIARIO,\n        s21.codcat,\n    s3.detalle as CATEGORIA_VENTA,\n    s21.codben AS CODBEN,\n    s21.nombre,\n    s21.edad,\n    s21.sexo,\n    s21.codser AS COD_SERVICIO,\n    s21.numero AS NUMERO_APERTURA,\n    s21.codinf AS COD_INFRAESTRUCTURA,\n    s21.perate AS PERSONAS_ATENDIDAS,\n    s21.numpar AS NUMERO_PARTICIPANTES,\n    s21.usos AS NUMERO_USOS,\n    s21.cantidad AS CANTIDAD_UNIDADES,\n    s21.valser AS VALOR_SERVICIO,\n    s21.valdes AS VALOR_DESCONTADO,\n    s21.valcos AS VALOR_COSTO,\n    s21.valsub AS VALOR_SUBSIDIO, \n    s21.ruaf,\n    s21.numrua,\n    s21.estafi AS ESTADO_AFILIACION\n    FROM servicios.servi21 s21\n\n    right join (select \n                marca,documento,fecha, estado, fecest\n                from servicios.servi20\n    WHERE DATE_FORMAT(servi20.fecha, '%Y%m%01')&gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and servi20.fecha &lt;=CURDATE()\n     and servi20.documento is not null\n    ) as s20 on s21.marca = s20.marca and s21.documento = s20.documento\n    left join servicios.servi03 s3 on s21.codcat = s3.codcat\n    '''\n}\ndim_names = {\n    \"BD_Fact_Ventas_Servicios\":'BD_Fact_Ventas_Servicios'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-23 17:47:33,985 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y se cargan los resultados en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-23 17:47:34,036 - INFO - CONEXION A BASE MINERVA\n2024-10-23 17:47:34,339 - INFO - Cargando BD_Fact_Ventas_Servicios \n2024-10-23 17:48:22,047 - INFO - Cargada BD_Fact_Ventas_Servicios --- 47.71 seconds ---\n2024-10-23 17:48:22,261 - INFO - CARGUE TABLAS DESDE MYSQL --- BD_Fact_Ventas_Servicios --- 48.22 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#validacion-y-limpieza-de-datos-en-tablas-de-ventas-y-dimension-de-datos-fijos","title":"Validaci\u00f3n y Limpieza de Datos en Tablas de Ventas y Dimensi\u00f3n de Datos Fijos","text":"<p>Este bloque de c\u00f3digo realiza validaciones, transformaciones y limpieza de datos en el diccionario <code>df_structure</code>, que contiene los resultados de consultas sobre ventas de servicios. En la primera etapa, se verifica si existen duplicados en las tablas usando <code>StoreDuplicated</code>, que genera un archivo de trazabilidad de duplicados si se encuentran registros repetidos. Luego, se eliminan duplicados directamente de las tablas para asegurar la integridad de los datos.</p> <p>En la segunda etapa, se normalizan y limpian los datos en cada tabla de <code>df_structure</code> transformando los nombres de columnas y los valores de las columnas de tipo texto a may\u00fasculas. Adem\u00e1s, se eliminan los espacios en los extremos y se reemplazan las cadenas <code>\"NAN\"</code> o <code>\"NONE\"</code> por valores <code>NaN</code> de NumPy, asegurando consistencia en los valores faltantes.</p> <p>La \u00faltima parte define una consulta <code>qr_structure_datos_fijos</code> para la tabla <code>BD_Dim_Datos_Fijos</code>, que se usa para obtener datos fijos, como c\u00f3digos y documentos, que complementan el an\u00e1lisis de ventas. El resultado de esta consulta se cargar\u00e1 en <code>df_structure_datos_fijos</code> para su procesamiento posterior.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 17:48:29,273 - INFO - VALIDADOR TABLA: BD_Fact_Ventas_Servicios\n2024-10-23 17:48:29,274 - INFO - VALIDADOR DUPLICADOS --- 7.00 seconds ---\n</code></pre> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 17:48:52,981 - INFO - LIMPIEZA --- 23.70 seconds ---\n</code></pre> <pre><code>#Lista de querys\nqr_structure_datos_fijos = {\n    \"BD_Dim_Datos_Fijos\":'''SELECT CODDOC, DOCUMENTO FROM dwh.BD_Dim_Datos_Fijos \n    '''\n}\ndim_names_datos_fijos = {\n    \"BD_Fact_Ventas_Servicios\":'BD_Fact_Ventas_Servicios'\n               }\ndf_structure_datos_fijos = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-23 17:48:52,986 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#conexion-a-la-base-de-datos-dwh-y-carga-de-tabla-de-datos-fijos","title":"Conexi\u00f3n a la Base de Datos DWH y Carga de Tabla de Datos Fijos","text":"<p>Este c\u00f3digo establece una conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>obtener_conexion</code>, que proporciona los detalles de conexi\u00f3n para el motor <code>motor_dwh</code>. La conexi\u00f3n es utilizada para ejecutar consultas en la base de datos, en este caso, cargando las consultas definidas en <code>qr_structure_datos_fijos</code> y almacenando los resultados en <code>df_structure_datos_fijos</code>.</p> <p>La funci\u00f3n <code>cargar_tablas</code> toma como par\u00e1metros el motor de conexi\u00f3n, el diccionario de consultas, el diccionario donde se almacenar\u00e1n los datos (<code>df_structure_datos_fijos</code>), y el logger para registrar eventos. Esta configuraci\u00f3n permite realizar la carga de datos de la tabla <code>BD_Dim_Datos_Fijos</code>, que incluye informaci\u00f3n como c\u00f3digos y documentos, asegurando su disponibilidad para futuros an\u00e1lisis y validaciones.</p> <pre><code>#Conexion a base Minerva\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_datos_fijos, df_structure_datos_fijos, logger)\n</code></pre> <pre><code>2024-10-23 17:48:53,001 - INFO - CONEXION A BASE DWH\n2024-10-23 17:48:53,297 - INFO - Cargando BD_Dim_Datos_Fijos \n2024-10-23 17:48:58,690 - INFO - Cargada BD_Dim_Datos_Fijos --- 5.39 seconds ---\n2024-10-23 17:48:58,768 - INFO - CARGUE TABLAS DESDE MYSQL --- BD_Dim_Datos_Fijos --- 5.77 seconds ---\n</code></pre>"},{"location":"seccion/3.1-FactTransaccionesVentas/#eliminacion-de-duplicados-y-normalizacion-de-datos-en-ventas-de-servicios","title":"Eliminaci\u00f3n de Duplicados y Normalizaci\u00f3n de Datos en Ventas de Servicios","text":"<p>Este bloque de c\u00f3digo realiza uniones y transformaciones en los datos de ventas de servicios y documentos en <code>df_structure</code>. Primero, elimina duplicados en <code>df_structure_datos_fijos['BD_Dim_Datos_Fijos']</code> para conservar \u00fanicamente el \u00faltimo registro de cada documento, garantizando que solo se mantenga la informaci\u00f3n m\u00e1s reciente de cada documento.</p> <p>A continuaci\u00f3n, <code>df_ventas</code> y <code>df_datos_fijos</code> se combinan mediante un <code>join</code> en <code>CODBEN</code> y <code>DOCUMENTO</code>, proporcionando el tipo de documento al conjunto de datos de ventas. Luego, se crea un identificador \u00fanico <code>ID_AFILIADO</code> concatenando <code>CODDOC</code> y <code>CODBEN</code>, y se reorganizan las columnas para que <code>ID_AFILIADO</code> se ubique en la quinta posici\u00f3n, mejorando la legibilidad de los datos.</p> <p>Para asegurar consistencia en los tipos de datos, se rellenan valores faltantes en <code>CONSECUTIVO_VENTA</code>, <code>EDAD</code>, y <code>NUMERO_APERTURA</code> con ceros. Posteriormente, estos campos se convierten a tipo <code>int</code> para mantener la integridad de los datos. Finalmente, <code>df_structure['BD_Fact_Ventas_Servicios']</code> se actualiza con el DataFrame <code>df_joined</code>, listo para el an\u00e1lisis o carga en la base de datos.</p> <pre><code># Eliminar duplicados y quedarse con el \u00faltimo registro\ndf_structure_datos_fijos['BD_Dim_Datos_Fijos'] = df_structure_datos_fijos['BD_Dim_Datos_Fijos'].drop_duplicates(subset=['DOCUMENTO'], keep='last')\n\n# Supongamos que los DataFrames son df_ventas y df_datos_fijos\ndf_ventas = df_structure['BD_Fact_Ventas_Servicios']\ndf_datos_fijos = df_structure_datos_fijos['BD_Dim_Datos_Fijos']\n\n# Realizar el join para traer el tipo de documento\ndf_joined = pd.merge(df_ventas, df_datos_fijos, left_on='CODBEN', right_on='DOCUMENTO', how='inner')\n\n# Crear la columna 'ID_AFILIADO' concatenando 'CODDOC' y 'CODBEN'\ndf_joined['ID_AFILIADO'] = df_joined['CODDOC'] + df_joined['CODBEN']\n\n# Reorganizar las columnas para que 'ID_AFILIADO' est\u00e9 en la quinta posici\u00f3n\ncolumnas_ordenadas = df_joined.columns.tolist()\ncolumnas_ordenadas.insert(5, columnas_ordenadas.pop(columnas_ordenadas.index('ID_AFILIADO')))\ndf_joined = df_joined[columnas_ordenadas]\n\n# Ajustar tipos de dato enteros \n# Reemplazar valores NaN por 0\ndf_joined['CONSECUTIVO_VENTA'] = df_joined['CONSECUTIVO_VENTA'].fillna(0)\ndf_joined['EDAD'] = df_joined['EDAD'].fillna(0)\ndf_joined['NUMERO_APERTURA'] = df_joined['NUMERO_APERTURA'].fillna(0)\n\n# Convertir a float y luego a entero\ndf_joined['CONSECUTIVO_VENTA'] = df_joined['CONSECUTIVO_VENTA'].astype(float).astype(int)\ndf_joined['EDAD'] = df_joined['EDAD'].astype(float).astype(int)\ndf_joined['NUMERO_APERTURA'] = df_joined['NUMERO_APERTURA'].astype(float).astype(int)\n\ndf_structure['BD_Fact_Ventas_Servicios'] = df_joined.copy()\ndf_structure['BD_Fact_Ventas_Servicios']\n</code></pre> DOCUMENTO_VENTA MARCA_VENTA PERIODO FECHA_VENTA CONSECUTIVO_VENTA ID_AFILIADO ESTADO_VENTA FECHA_ESTADO TIPO_BENEFICIARIO CODCAT ... CANTIDAD_UNIDADES VALOR_SERVICIO VALOR_DESCONTADO VALOR_COSTO VALOR_SUBSIDIO RUAF NUMRUA ESTADO_AFILIACION CODDOC DOCUMENTO 0 0048360 01 202305 2023-05-02 1 CC1082875116 A 2023-05-02 B 3 ... 1.0 4600.0 0.0 4600.0 0.0 N 0.0 A CC 1082875116 1 0048362 01 202305 2023-05-02 1 CC1082995375 A 2023-05-02 C 2 ... 1.0 300800.0 0.0 470029.0 169229.0 N 0.0 A CC 1082995375 2 0048362 01 202305 2023-05-02 2 CC1082995375 A 2023-05-02 C 2 ... 1.0 300800.0 0.0 470029.0 169229.0 N 0.0 A CC 1082995375 3 0048362 01 202305 2023-05-02 3 CC1082995375 A 2023-05-02 C 2 ... 1.0 300800.0 0.0 470029.0 169229.0 N 0.0 A CC 1082995375 4 0048362 01 202305 2023-05-02 4 CC1082995375 A 2023-05-02 C 2 ... 1.0 300800.0 0.0 470029.0 169229.0 N 0.0 A CC 1082995375 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 968253 0045153 999 202410 2024-10-18 1 CC12555412 A NaN B 1 ... 1.0 5000.0 0.0 50314.0 45314.0 N 0.0 A CC 12555412 968254 0045156 999 202410 2024-10-18 1 CC1082974320 A NaN T 1 ... 1.0 5000.0 0.0 50314.0 45314.0 N 0.0 A CC 1082974320 968255 0045158 999 202410 2024-10-18 2 CC1103120382 A NaN C 2 ... 1.0 18400.0 0.0 38743.0 20343.0 N 0.0 A CC 1103120382 968256 0045159 999 202410 2024-10-18 1 CC36694980 A NaN T 1 ... 1.0 5000.0 0.0 50314.0 45314.0 N 0.0 A CC 36694980 968257 0045159 999 202410 2024-10-18 2 CC7634231 A NaN C 1 ... 1.0 5000.0 0.0 50314.0 45314.0 N 0.0 A CC 7634231 <p>968258 rows \u00d7 31 columns</p>"},{"location":"seccion/3.1-FactTransaccionesVentas/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH, verificando primero que la clave est\u00e9 en el diccionario <code>dim_names</code> para asignar el nombre de la tabla correspondiente. Si la clave no est\u00e1 presente, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra en el log, junto con los nombres de las tablas almacenadas correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-23 17:49:04,656 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-23 17:49:04,658 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-23 17:49:04,956 - INFO - Almacenando tabla BD_Fact_Ventas_Servicios en DWH como BD_Fact_Ventas_Servicios\n2024-10-23 17:51:45,220 - INFO - Tabla BD_Fact_Ventas_Servicios almacenada correctamente como BD_Fact_Ventas_Servicios.\n2024-10-23 17:51:46,478 - INFO - ALMACENAMIENTO ---  --- 2.70 minutes ---\n2024-10-23 17:51:46,479 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-23 17:51:46,491 - INFO - FINAL ETL --- 252.53 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/","title":"4.1 Fact Colegio fijos","text":""},{"location":"seccion/4.1-FactColegio_fijos/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>FactColegio_fijos</code> extrae y transforma datos de las tablas <code>colegio12</code> y <code>colegio13</code>, correspondientes a informaci\u00f3n de estudiantes y acudientes, respectivamente. La transformaci\u00f3n incluye la consolidaci\u00f3n de los datos en un DataFrame \u00fanico, la estandarizaci\u00f3n de nombres y la limpieza de duplicados. Una vez estructurados los datos, se verifica su existencia en la tabla <code>DimDatosFijos</code> del Data Warehouse (DWH). Los registros que no existen se almacenan en el DWH, asegurando la integridad y unicidad de la informaci\u00f3n de afiliados en la base de datos.</p>"},{"location":"seccion/4.1-FactColegio_fijos/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactColegio_fijos\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para cargar `colegio12` y `colegio13`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de estudiantes y acudientes\n    ETL_Script -&gt;&gt; ETL_Script: Combina datos y elimina duplicados\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Verifica datos en `DimDatosFijos`\n    DWH --&gt;&gt; ETL_Script: Retorna datos existentes\n    ETL_Script -&gt;&gt; DWH: Carga datos faltantes en `DimDatosFijos`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#etl","title":"ETL","text":""},{"location":"seccion/4.1-FactColegio_fijos/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo establece el entorno de procesamiento para cargar y transformar datos en el DWH. Configura librer\u00edas como <code>pandas</code> y <code>sqlalchemy</code> para manipulaci\u00f3n y conexi\u00f3n de datos. Adem\u00e1s, incluye funciones personalizadas de <code>Funciones.py</code>, como <code>StoreDuplicated</code> para gesti\u00f3n de duplicados, <code>obtener_conexion</code> para conexi\u00f3n a la base de datos, y <code>estandarizar_direccion</code> y <code>marcar_direcciones_estandarizadas</code> para procesar direcciones de manera uniforme. </p> <p>Al final, <code>testfunciones()</code> se ejecuta para validar la configuraci\u00f3n.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 27-10-2024 09:29\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#configuracion-del-logger-y-registro-de-inicio-de-proceso-etl","title":"Configuraci\u00f3n del Logger y Registro de Inicio de Proceso ETL","text":"<p>Este bloque configura el logger con <code>setup_logger</code>, estableciendo el archivo de log <code>Colegio.log</code> y el nivel de registro <code>INFO</code> para capturar eventos informativos y errores durante el proceso ETL. El mensaje <code>'COMIENZO ETL'</code> se registra para indicar el inicio del flujo de extracci\u00f3n, transformaci\u00f3n y carga de datos, ayudando a rastrear el proceso en el archivo de log.</p> <pre><code>logger = setup_logger(log_filename='Colegio.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-27 09:29:13,179 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#consultas-sql-para-cargar-datos-de-estudiantes-y-acudientes","title":"Consultas SQL para Cargar Datos de Estudiantes y Acudientes","text":"<p>Este c\u00f3digo define dos consultas SQL (<code>colegio12</code> y <code>colegio13</code>) para extraer datos de estudiantes y acudientes desde las tablas <code>colegio12</code> y <code>colegio13</code>. Ambas consultas construyen el campo <code>ID_AFILIADO</code> combinando los valores <code>coddoc</code> y <code>numdoc</code>, y crean <code>RAZON_SOCIAL</code> concatenando nombres y apellidos.</p> <p>La estructura tambi\u00e9n incluye <code>FECHA_NACIMIENTO</code>, <code>SEXO</code>, <code>CODDOC</code>, <code>DOCUMENTO</code>, y los nombres individuales (<code>PRIMER_NOMBRE</code>, <code>SEGUNDO_NOMBRE</code>, <code>PRIMER_APELLIDO</code>, <code>SEGUNDO_APELLIDO</code>). En <code>colegio13</code>, <code>FECHA_NACIMIENTO</code> se deja como <code>NULL</code>, dado que esta informaci\u00f3n podr\u00eda no estar disponible para acudientes.</p> <p>Finalmente, las consultas se registran en <code>qr_structure</code>, y el logger anota el inicio de la lectura de queries en el proceso ETL.</p> <pre><code># Definir las consultas SQL para cargar datos y asegurarse de crear la estructura con los campos requeridos\nrequired_columns = ['ID_AFILIADO', 'RAZON_SOCIAL','FECHA_NACIMIENTO', 'SEXO', 'CODDOC', 'DOCUMENTO', 'PRIMER_NOMBRE', 'SEGUNDO_NOMBRE', 'PRIMER_APELLIDO', 'SEGUNDO_APELLIDO']\nqr_structure = {\n    \"colegio12\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS ID_AFILIADO,\n        CONCAT(\n            IFNULL(prinom, ''), ' ', \n            IFNULL(segnom, ''), ' ', \n            IFNULL(priape, ''), ' ', \n            IFNULL(segape, '')\n            ) AS RAZON_SOCIAL,\n        fecnac AS FECHA_NACIMIENTO,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS documento,\n        prinom AS PRIMER_NOMBRE,\n        segnom AS SEGUNDO_NOMBRE,\n        priape AS PRIMER_APELLIDO,\n        segape AS SEGUNDO_APELLIDO,\n        \"colegio12\" AS FUENTE\n    FROM colegio.colegio12''',\n    \"colegio13\": '''\n    SELECT\n        CONCAT(coddoc, numdoc) AS ID_AFILIADO,\n        CONCAT(\n            IFNULL(prinom, ''), ' ', \n            IFNULL(segnom, ''), ' ', \n            IFNULL(priape, ''), ' ', \n            IFNULL(segape, '')\n            ) AS RAZON_SOCIAL,\n        NULL AS FECHA_NACIMIENTO,\n        codsex AS sexo,\n        coddoc,\n        numdoc AS documento,\n        prinom AS PRIMER_NOMBRE,\n        segnom AS SEGUNDO_NOMBRE,\n        priape AS PRIMER_APELLIDO,\n        segape AS SEGUNDO_APELLIDO,\n        \"colegio13\" AS FUENTE\n    FROM colegio.colegio13'''\n}\ndim_names = {\n    \"colegio12\":'BD_Fact_Colegio_Estudiantes',\n    \"colegio13\":'BD_Fact_Colegio_Acudientes',\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-27 09:29:13,219 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#cargar-y-estructurar-tablas-con-las-columnas-requeridas","title":"Cargar y estructurar tablas con las columnas requeridas","text":"<p>Se cargan las tablas <code>colegio12</code> y <code>colegio13</code> desde la base de datos, garantizando que todas las columnas requeridas est\u00e9n presentes en los dataframes resultantes. Si alguna columna falta, se crea con el valor <code>\"SinDato\"</code>. Posteriormente, se concatenan ambas tablas en un solo dataframe <code>df_colegio_combined</code>. Los eventos de carga y concatenaci\u00f3n se registran en el log.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 09:29:13,300 - INFO - CONEXION A BASE MINERVA\n2024-10-27 09:29:13,874 - INFO - Cargando colegio12 \n2024-10-27 09:29:15,038 - INFO - Cargada colegio12, 2,501 registros finales obtenidos. --- 1.16 seconds ---\n2024-10-27 09:29:15,039 - INFO - Cargando colegio13 \n2024-10-27 09:29:15,448 - INFO - Cargada colegio13, 1,291 registros finales obtenidos. --- 0.41 seconds ---\n2024-10-27 09:29:15,534 - INFO - CARGUE TABLAS DESDE MYSQL --- colegio13 --- 2.21 seconds ---\n</code></pre>"},{"location":"seccion/4.1-FactColegio_fijos/#estandarizacion-y-combinacion-de-datos-de-estudiantes-y-acudientes","title":"Estandarizaci\u00f3n y Combinaci\u00f3n de Datos de Estudiantes y Acudientes","text":"<p>Este c\u00f3digo asegura que las tablas de <code>colegio12</code> y <code>colegio13</code> contienen las columnas necesarias al transformar todas las columnas a may\u00fasculas y agregando columnas faltantes definidas en <code>required_columns</code> con el valor <code>\"SinDato\"</code>.</p> <p>Cada tabla (<code>colegio12</code> y <code>colegio13</code>) se estructura y verifica antes de ser registrada como cargada. Luego, se combinan en un solo DataFrame (<code>df_colegio_combined</code>) usando <code>pd.concat</code>, uniendo as\u00ed los datos de estudiantes y acudientes en una sola estructura, manteniendo consistencia y uniformidad en los datos. El proceso se registra en el logger para facilitar el seguimiento del ETL.</p> <pre><code>for ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    df = df_structure[ky].copy()\n    # Asegurarse de que las columnas requeridas est\u00e9n presentes en la estructura\n    for column in required_columns:\n        if column not in df.columns:\n            df[column] = \"SinDato\"\n\n    #df_structure[key] = df[required_columns]\n    logger.info(f'TABLA {ky} CARGADA Y ESTRUCTURADA')\n# Concatenar las tablas de colegio12 y colegio13\ndf_colegio_combined = pd.concat([df_structure['colegio12'], df_structure['colegio13']], ignore_index=True)\nlogger.info('TABLAS colegio12 Y colegio13 COMBINADAS')\n</code></pre> <pre><code>2024-10-27 09:29:15,555 - INFO - TABLA colegio12 CARGADA Y ESTRUCTURADA\n2024-10-27 09:29:15,559 - INFO - TABLA colegio13 CARGADA Y ESTRUCTURADA\n2024-10-27 09:29:15,562 - INFO - TABLAS colegio12 Y colegio13 COMBINADAS\n</code></pre> <pre><code>df_colegio_combined\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 0 TI1001941478 ISAAC DAVID SEVILLA LOPEZ 2003-08-18 M TI 1001941478 ISAAC DAVID SEVILLA LOPEZ colegio12 1 TI1001941478 ISAAC DAVID SEVILLA LOPEZ 2003-08-18 M TI 1001941478 ISAAC DAVID SEVILLA LOPEZ colegio12 2 TI1003266048 JORGE LUIS GARCIA JIMENEZ 2002-12-25 M TI 1003266048 JORGE LUIS GARCIA JIMENEZ colegio12 3 TI1003266048 JORGE LUIS GARCIA JIMENEZ 2002-12-25 M TI 1003266048 JORGE LUIS GARCIA JIMENEZ colegio12 4 TI1004358737 CARLOS DAVID GUZMAN SARMIENTO 2002-04-24 M TI 1004358737 CARLOS DAVID GUZMAN SARMIENTO colegio12 ... ... ... ... ... ... ... ... ... ... ... ... 3787 CC93400713 OSCAR ALEDT GIRALDO CORREA None M CC 93400713 OSCAR ALEDT GIRALDO CORREA colegio13 3788 CC94520471 JEAN PAUL TORRES VELEZ None M CC 94520471 JEAN PAUL TORRES VELEZ colegio13 3789 CC9770518 MAURICIO  CADENA HOYOS None M CC 9770518 MAURICIO None CADENA HOYOS colegio13 3790 CC98668306 JUAN CARLOS POSADA GALLARDO None M CC 98668306 JUAN CARLOS POSADA GALLARDO colegio13 3791 CCMAGDALENA FREDYS JOSE CAMPO MENDOZA None M CC MAGDALENA FREDYS JOSE CAMPO MENDOZA colegio13 <p>3792 rows \u00d7 11 columns</p>"},{"location":"seccion/4.1-FactColegio_fijos/#filtrar-registros-unicos-por-id","title":"Filtrar registros \u00fanicos por <code>id</code>","text":"<p>Se eliminan los registros duplicados en el dataframe <code>df_colegio_combined</code> manteniendo solo los registros con <code>id</code> \u00fanicos. El dataframe resultante se almacena en <code>df_colegio_unique</code>, y el proceso se registra en el log.</p> <pre><code># Obtener solo los registros con 'id' \u00fanicos\ndf_colegio_unique = df_colegio_combined.drop_duplicates(subset='ID_AFILIADO').copy()\nlogger.info('REGISTROS \u00daNICOS DE ID OBTENIDOS')\n</code></pre> <pre><code>2024-10-27 09:29:15,603 - INFO - REGISTROS \u00daNICOS DE ID OBTENIDOS\n</code></pre> <pre><code>df_colegio_unique\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 0 TI1001941478 ISAAC DAVID SEVILLA LOPEZ 2003-08-18 M TI 1001941478 ISAAC DAVID SEVILLA LOPEZ colegio12 2 TI1003266048 JORGE LUIS GARCIA JIMENEZ 2002-12-25 M TI 1003266048 JORGE LUIS GARCIA JIMENEZ colegio12 4 TI1004358737 CARLOS DAVID GUZMAN SARMIENTO 2002-04-24 M TI 1004358737 CARLOS DAVID GUZMAN SARMIENTO colegio12 6 TI1004367232 ARIADNA SOFIA EFFER OTERO 2003-06-10 F TI 1004367232 ARIADNA SOFIA EFFER OTERO colegio12 8 TI1004462648 SEBASTIAN ALEJANDRO JOYA MORALES 2003-05-10 M TI 1004462648 SEBASTIAN ALEJANDRO JOYA MORALES colegio12 ... ... ... ... ... ... ... ... ... ... ... ... 3787 CC93400713 OSCAR ALEDT GIRALDO CORREA None M CC 93400713 OSCAR ALEDT GIRALDO CORREA colegio13 3788 CC94520471 JEAN PAUL TORRES VELEZ None M CC 94520471 JEAN PAUL TORRES VELEZ colegio13 3789 CC9770518 MAURICIO  CADENA HOYOS None M CC 9770518 MAURICIO None CADENA HOYOS colegio13 3790 CC98668306 JUAN CARLOS POSADA GALLARDO None M CC 98668306 JUAN CARLOS POSADA GALLARDO colegio13 3791 CCMAGDALENA FREDYS JOSE CAMPO MENDOZA None M CC MAGDALENA FREDYS JOSE CAMPO MENDOZA colegio13 <p>2444 rows \u00d7 11 columns</p>"},{"location":"seccion/4.1-FactColegio_fijos/#consulta-y-filtrado-de-registros-unicos-en-el-data-warehouse","title":"Consulta y Filtrado de Registros \u00danicos en el Data Warehouse","text":"<p>Este c\u00f3digo consulta la tabla <code>BD_Dim_Datos_Fijos</code> en el Data Warehouse (DWH), almacenando el resultado en <code>df_structure_dwh</code> para el an\u00e1lisis de datos de identificaci\u00f3n. Despu\u00e9s de establecer la conexi\u00f3n con la base DWH y cargar los datos en <code>df_dwh</code>, se realiza un filtro para identificar los registros \u00fanicos de <code>df_colegio_unique</code> que no est\u00e1n presentes en <code>df_dwh</code>, usando el campo <code>ID_AFILIADO</code> como referencia.</p> <p>El DataFrame <code>df_missing_in_dwh</code> contiene los registros faltantes, que pueden requerir integraci\u00f3n en la tabla de datos fijos, y el proceso se registra en el logger para asegurar un rastreo claro del flujo de trabajo ETL.</p> <pre><code>#Lista de querys\nqr_structure_dwh = {\n    \"Datos_fijos\":'''SELECT * FROM dwh.BD_Dim_Datos_Fijos\n    '''\n               }\n\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS')\n\n#Conexion a base dwh\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-27 09:29:15,633 - INFO - LECTURA DE QUERYS\n2024-10-27 09:29:15,635 - INFO - CONEXION A BASE DWH\n2024-10-27 09:29:16,181 - INFO - Cargando Datos_fijos \n2024-10-27 09:30:47,920 - INFO - Cargada Datos_fijos, 1,038,532 registros finales obtenidos. --- 1.53 minutes ---\n2024-10-27 09:30:48,092 - INFO - CARGUE TABLAS DESDE MYSQL --- Datos_fijos --- 1.54 minutes ---\n</code></pre> <pre><code>df_dwh = df_structure_dwh[\"Datos_fijos\"].copy()\n</code></pre> <pre><code># Filtrar los registros \u00fanicos que no est\u00e1n presentes en DimDatosFijos\n\ndf_missing_in_dwh = df_colegio_unique[~df_colegio_unique['ID_AFILIADO'].isin(df_dwh['ID_AFILIADO'])]\nlogger.info('FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos')\n</code></pre> <pre><code>2024-10-27 09:30:48,820 - INFO - FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos\n</code></pre> <pre><code>df_missing_in_dwh\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 1843 TI1084063844 EMMANUEL ALFONSO GRANADOS PERTUZ 2017-08-18 M TI 1084063844 EMMANUEL ALFONSO GRANADOS PERTUZ colegio12 2256 TI1084607791 ALDO ANDRE HERRERA CARRILLO 2017-08-27 M TI 1084607791 ALDO ANDRE HERRERA CARRILLO colegio12"},{"location":"seccion/4.1-FactColegio_fijos/#guardar-registros-faltantes-en-la-tabla-dimdatosfijos","title":"Guardar registros faltantes en la tabla <code>DimDatosFijos</code>","text":"<p>Se conectan los registros faltantes en la tabla <code>BD_DimDatosFijos</code> en el DWH y se almacenan los registros del dataframe <code>df_missing_in_dwh</code> en la base de datos utilizando el modo <code>append</code>. El proceso de almacenamiento se registra en el log, y en caso de error, se captura y se registra el mensaje correspondiente.</p> <pre><code>#Conexion a base dwh\n\nguardar_en_dwh(df_missing_in_dwh, 'BD_Dim_Datos_Fijos', logger, multiple=False, if_exists='append')\n</code></pre> <pre><code>2024-10-27 09:30:48,853 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 09:30:48,855 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 09:30:49,380 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-27 09:30:50,115 - INFO - Tabla almacenada correctamente. 2 registros finales obtenidos.\n2024-10-27 09:30:50,222 - INFO - ALMACENAMIENTO ---  --- 1.37 seconds ---\n2024-10-27 09:30:50,223 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/4.2-FactColegio/","title":"4.2 Fact Colegio","text":""},{"location":"seccion/4.2-FactColegio/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>FactColegio</code> permite extraer, transformar y cargar informaci\u00f3n de estudiantes, acudientes, matr\u00edculas, y grados de una base de datos de colegio hacia un Data Warehouse (DWH). Este proceso se enfoca en estructurar datos personales y acad\u00e9micos, as\u00ed como en estandarizar las direcciones de estudiantes y acudientes. El flujo ETL emplea consultas SQL para extraer los datos, transformaciones para limpieza y deduplicaci\u00f3n, y una carga final en el DWH con nombres de tablas predefinidos. Este procedimiento asegura la disponibilidad de datos estandarizados y listos para an\u00e1lisis y reportes en el entorno del DWH.</p>"},{"location":"seccion/4.2-FactColegio/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactColegio\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para datos de estudiantes, acudientes, matr\u00edculas, y grados\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de entidades del colegio\n    ETL_Script -&gt;&gt; ETL_Script: Estandariza direcciones y elimina duplicados\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga tablas transformadas y dimensionales en el DWH\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/4.2-FactColegio/#etl","title":"ETL","text":""},{"location":"seccion/4.2-FactColegio/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para la manipulaci\u00f3n de datos en un Data Warehouse, importando librer\u00edas clave como <code>pandas</code>, <code>sqlalchemy</code> y <code>logging</code> para el manejo de datos, conexiones de base y registro de eventos. Tambi\u00e9n se cargan funciones personalizadas del m\u00f3dulo <code>Funciones.py</code>, incluyendo <code>guardar_en_dwh</code>, <code>StoreDuplicated</code>, <code>estandarizar_direccion</code>, y <code>marcar_direcciones_estandarizadas</code>, que facilitan la carga, estandarizaci\u00f3n y validaci\u00f3n de datos. La prueba <code>testfunciones()</code> asegura que estas funciones est\u00e1n correctamente importadas y operativas.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 12:28\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Colegio.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Colegio.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 12:28:00,767 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#definicion-de-consultas-para-datos-de-colegio","title":"Definici\u00f3n de Consultas para Datos de Colegio","text":"<p>Este c\u00f3digo configura las consultas SQL en el diccionario <code>qr_structure</code> para obtener datos de diversas tablas relacionadas con estudiantes, acudientes, matr\u00edculas, grados y niveles educativos en una base de datos de colegio. Cada consulta extrae y renombra columnas espec\u00edficas de las tablas <code>colegio12</code>, <code>colegio13</code>, <code>colegio15</code>, y <code>colegio03</code> para estandarizar los datos. El diccionario <code>dim_names</code> asigna nombres descriptivos para identificar cada consulta en <code>df_structure</code>, donde se almacenar\u00e1n los resultados. Esta configuraci\u00f3n permite la carga y procesamiento eficiente de datos escolares para an\u00e1lisis posteriores.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"colegio12\":'''select \n                ano as ANIO,\n                CONCAT(coddoc,numdoc) AS ID_AFILIADO,\n                coddoc AS TIPO_DOCUMENTO,\n                numdoc AS DOCUMENTO,\n                ciuexp AS CIUDAD_EXPEDICION,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                rh,\n                eps,\n                codsex AS CODSEX,\n                ciunac AS CIUDAD_NACIMIENTO,\n                fecnac AS FECHA_NACIMIENTO,\n                dirres AS DIRECCION,\n                barrio AS BARRIO,\n                estrato AS ESTRATO,\n                codciu AS CODIGO_CIUDAD,\n                telres AS TELEFONO,\n                email,\n                nota,\n                observacion,\n                docpad AS DOCUMENTO_PADRE,\n                docmad AS DOCUMENTO_MADRE,\n                docacu AS DOCUMENTO_ACUDIENTE,\n                codben,\n                codcat,\n                tipo AS TIPO_ESTUDIANTE\n                from colegio.colegio12''',\n    \"colegio13\":'''select \n                CONCAT(coddoc,numdoc) AS ID_AFILIADO,\n                coddoc AS TIPO_DOCUMENTO,\n                numdoc AS DOCUMENTO,\n                ciuexp AS CIUDAD_EXPEDICION,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                codpar AS COD_PARENTESCO,\n                profesion,\n                empresa,\n                ocupacion,\n                codsex,\n                direccion,\n                codciu AS CODIGO_CIUDAD,\n                telefono,\n                email\n                from colegio.colegio13''',\n    \"colegio15\":'''select \n                c15.ano as ANIO,\n                c15.nummat AS NUMERO_MATRICULA,\n                concat(c12.coddoc,c15.numdoc) AS ID_AFILIADO,\n                c15.codgra AS COD_GRADO,\n                c15.codgru AS COD_GRUPO,\n                c15.fecmat AS FECHA_MATRICULA,\n                c15.colant AS COLEGIO_ANTERIOR,\n                c15.numlib AS NUMERO_LIBRO_REGISTRO,\n                c15.numfol AS NUMERO_FOLIO_REGISTRO,\n                c15.codcon AS COD_CONVENIO,\n                c15.codest AS COD_ESTADO,\n                c15.codcat,\n                c15.estado,\n                c15.fecest AS FECHA_ESTADO,\n                c15.motivo AS MOTIVO_NOVEDAD,\n                c15.usuario,\n                c15.estbol AS ESTADO_BOLETIN\n                from colegio.colegio15 c15\n            LEFT JOIN colegio.colegio12 c12 on c15.ano = c12.ano and c15.numdoc = c12.numdoc''',\n    \"colegio03\": '''SELECT \n                c03.codgra AS COD_GRADO,\n                c03.detalle AS GRADO,\n                c02.detalle AS NIVEL_EDUCATIVO,\n                c03.numcup AS NUMERO_CUPOS\n            FROM colegio.colegio03 c03\n            LEFT JOIN colegio.colegio02 c02 on c03.nivedu = c02.nivedu    \n    '''\n               }\ndim_names = {\n    \"colegio12\":'BD_Fact_Colegio_Estudiantes',\n    \"colegio13\":'BD_Fact_Colegio_Acudientes',\n    \"colegio15\":'BD_Fact_Colegio_Matriculas',\n    \"BD_Dim_Colegio_Calendario\" : \"BD_Dim_Colegio_Calendario\",\n    \"colegio03\": \"BD_Dim_Colegio_Grados\"\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 12:28:00,782 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n a la base de datos de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 12:28:00,852 - INFO - CONEXION A BASE MINERVA\n2024-10-30 12:28:01,177 - INFO - Cargando colegio12 \n2024-10-30 12:28:01,640 - INFO - Cargada colegio12, 2,501 registros finales obtenidos. --- 0.46 seconds ---\n2024-10-30 12:28:01,641 - INFO - Cargando colegio13 \n2024-10-30 12:28:01,815 - INFO - Cargada colegio13, 1,291 registros finales obtenidos. --- 0.17 seconds ---\n2024-10-30 12:28:01,816 - INFO - Cargando colegio15 \n2024-10-30 12:28:01,970 - INFO - Cargada colegio15, 2,243 registros finales obtenidos. --- 0.15 seconds ---\n2024-10-30 12:28:01,971 - INFO - Cargando colegio03 \n2024-10-30 12:28:02,003 - INFO - Cargada colegio03, 13 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 12:28:02,062 - INFO - CARGUE TABLAS DESDE MYSQL --- colegio03 --- 1.21 seconds ---\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#validacion-y-eliminacion-de-campos-duplicados-en-tablas","title":"Validaci\u00f3n y Eliminaci\u00f3n de Campos Duplicados en Tablas","text":"<p>Este c\u00f3digo recorre las tablas almacenadas en <code>df_structure</code> y realiza una validaci\u00f3n para detectar y registrar valores duplicados en cada tabla. Para cada tabla, genera una lista de columnas a comparar (<code>ColumnsToCompare</code>), excluyendo el campo <code>id</code>. La funci\u00f3n <code>StoreDuplicated</code> verifica duplicados en funci\u00f3n de estas columnas y genera un archivo de trazabilidad con el nombre <code>\\trazaDuplicados_</code> seguido del nombre de la tabla.</p> <p>Una vez verificados los duplicados, se eliminan las filas duplicadas, dejando solo registros \u00fanicos. Los tiempos de ejecuci\u00f3n de esta operaci\u00f3n se registran en <code>validador_time</code>, proporcionando un resumen del proceso en segundos al finalizar.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:02,124 - INFO - VALIDADOR TABLA: colegio12\n2024-10-30 12:28:02,148 - INFO - VALIDADOR TABLA: colegio13\n2024-10-30 12:28:02,171 - INFO - VALIDADOR TABLA: colegio15\n2024-10-30 12:28:02,178 - INFO - VALIDADOR TABLA: colegio03\n2024-10-30 12:28:02,180 - INFO - VALIDADOR DUPLICADOS --- 0.11 seconds ---\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#transformacion-y-limpieza-de-datos-en-tablas","title":"Transformaci\u00f3n y Limpieza de Datos en Tablas","text":"<p>Este c\u00f3digo realiza una serie de transformaciones de limpieza en cada tabla dentro de <code>df_structure</code>. Primero, convierte todos los nombres de columnas a may\u00fasculas para asegurar consistencia en el formato. Luego, se aplica una transformaci\u00f3n en todas las columnas de tipo texto para pasar su contenido a may\u00fasculas y eliminar espacios en blanco al inicio y al final de cada valor, manteniendo la uniformidad en el formato.</p> <p>Finalmente, reemplaza los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code> de NumPy, asegurando que los datos faltantes est\u00e9n correctamente etiquetados. El tiempo de ejecuci\u00f3n de estas transformaciones se registra y se muestra en <code>limpieza_time</code>.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA  --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:02,326 - INFO - LIMPIEZA  --- 0.14 seconds ---\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#creacion-de-tabla-dimensional-para-anos-de-calendario","title":"Creaci\u00f3n de Tabla Dimensional para A\u00f1os de Calendario","text":"<p>Este c\u00f3digo extrae la columna <code>ANIO</code> de la tabla de matr\u00edculas (<code>colegio15</code>) en <code>df_structure</code>, obtiene los valores \u00fanicos de esa columna y crea un nuevo DataFrame <code>df_valores_unicos</code> con estos valores. Este DataFrame sirve como una tabla dimensional de a\u00f1os (<code>BD_Dim_Colegio_Calendario</code>), almacenando los distintos a\u00f1os de calendario de las matr\u00edculas en <code>df_structure</code> para su uso en an\u00e1lisis de tiempo o relaciones entre tablas.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['colegio15']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ANIO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ANIO'])\n\n# Crear Dim\ndf_structure['BD_Dim_Colegio_Calendario'] = df_valores_unicos.copy()\ndf_structure['BD_Dim_Colegio_Calendario']\n</code></pre> ANIO 0 2020 1 2021 2 2022 3 2023 4 2024"},{"location":"seccion/4.2-FactColegio/#estandarizacion-de-direcciones-para-estudiantes-y-acudientes","title":"Estandarizaci\u00f3n de Direcciones para Estudiantes y Acudientes","text":"<p>Este bloque de c\u00f3digo realiza la estandarizaci\u00f3n de direcciones en los DataFrames <code>df_estudiantes</code> y <code>df_acudientes</code>, extra\u00eddos de <code>df_structure</code>. Primero, renombra la columna <code>DIRECCION</code> a <code>DIRECCION_ORIGINAL</code> y asegura que sea de tipo texto para ambos DataFrames. Luego, aplica la funci\u00f3n <code>estandarizar_direccion</code> en <code>DIRECCION_ORIGINAL</code> para generar una columna <code>DIRECCION_LIMPIA</code> con las direcciones en un formato estandarizado.</p> <p>Posteriormente, utiliza <code>marcar_direcciones_estandarizadas</code> para agregar una columna que indique si la direcci\u00f3n estandarizada comienza con una de las abreviaturas esperadas. Finalmente, las tablas estandarizadas se actualizan en <code>df_structure</code>, dejando las direcciones listas para su an\u00e1lisis y asegurando consistencia en el formato de direcci\u00f3n.</p> <pre><code># Para estudiantes\ndf_estudiantes = df_structure['colegio12'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_estudiantes.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_estudiantes['DIRECCION_ORIGINAL'] = df_estudiantes['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_estudiantes['DIRECCION_LIMPIA'] = df_estudiantes['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_estudiantes, 'DIRECCION_LIMPIA')\n\ndf_structure['colegio12'] = df_estudiantes.copy()\n#df_structure['colegio12']\n</code></pre> <pre><code># Para acudientes\ndf_acudientes = df_structure['colegio13'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_acudientes.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_acudientes['DIRECCION_ORIGINAL'] = df_acudientes['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_acudientes['DIRECCION_LIMPIA'] = df_acudientes['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_acudientes, 'DIRECCION_LIMPIA')\n\ndf_structure['colegio13'] = df_acudientes.copy()\n#df_structure['colegio13']\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 12:28:02,430 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/4.2-FactColegio/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>#Conexion a base dwh\n\nguardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 12:28:02,440 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 12:28:02,442 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 12:28:02,730 - INFO - Almacenando tabla colegio12 en DWH como BD_Fact_Colegio_Estudiantes\n2024-10-30 12:28:04,964 - INFO - Tabla colegio12 almacenada correctamente como BD_Fact_Colegio_Estudiantes.\n2024-10-30 12:28:04,965 - INFO - Almacenando tabla colegio13 en DWH como BD_Fact_Colegio_Acudientes\n2024-10-30 12:28:06,820 - INFO - Tabla colegio13 almacenada correctamente como BD_Fact_Colegio_Acudientes.\n2024-10-30 12:28:06,820 - INFO - Almacenando tabla colegio15 en DWH como BD_Fact_Colegio_Matriculas\n2024-10-30 12:28:08,041 - INFO - Tabla colegio15 almacenada correctamente como BD_Fact_Colegio_Matriculas.\n2024-10-30 12:28:08,042 - INFO - Almacenando tabla colegio03 en DWH como BD_Dim_Colegio_Grados\n2024-10-30 12:28:08,625 - INFO - Tabla colegio03 almacenada correctamente como BD_Dim_Colegio_Grados.\n2024-10-30 12:28:08,627 - INFO - Almacenando tabla BD_Dim_Colegio_Calendario en DWH como BD_Dim_Colegio_Calendario\n2024-10-30 12:28:09,123 - INFO - Tabla BD_Dim_Colegio_Calendario almacenada correctamente como BD_Dim_Colegio_Calendario.\n2024-10-30 12:28:09,181 - INFO - ALMACENAMIENTO ---  --- 6.74 seconds ---\n2024-10-30 12:28:09,182 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:09,191 - INFO - FINAL ETL --- 8.44 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/","title":"5.1 Fact Servicio Social Fijos","text":""},{"location":"seccion/5.1-FactServicioSocial_Fijos/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL <code>FactServicioSocial_Fijos</code> extrae y transforma datos de personas desde dos fuentes (<code>nsijec.personas</code> y <code>saipi.personas</code>). Este flujo garantiza la integraci\u00f3n de informaci\u00f3n clave de identificaci\u00f3n en el Data Warehouse (DWH), especialmente en la tabla <code>DimDatosFijos</code>. Cada registro se verifica para asegurar su unicidad, y aquellos que no existen en el DWH se almacenan, asegurando una estructura de datos consolidada y uniforme.</p>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactServicioSocial_Fijos\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant Neith_DB as Base de Datos Neith\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer `personas_jec`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de `personas_jec`\n    ETL_Script -&gt;&gt; Neith_DB: Conecta a BD Neith\n    Neith_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Neith_DB: Ejecuta consulta SQL para extraer `persona_saipi`\n    Neith_DB --&gt;&gt; ETL_Script: Retorna datos de `persona_saipi`\n    ETL_Script -&gt;&gt; ETL_Script: Combina y filtra registros \u00fanicos\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Verifica datos en `DimDatosFijos`\n    DWH --&gt;&gt; ETL_Script: Retorna datos existentes\n    ETL_Script -&gt;&gt; DWH: Carga datos faltantes en `DimDatosFijos`\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#etl","title":"ETL","text":""},{"location":"seccion/5.1-FactServicioSocial_Fijos/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para operaciones de carga y transformaci\u00f3n de datos en un Data Warehouse, importando librer\u00edas esenciales (<code>pandas</code>, <code>sqlalchemy</code>, <code>pymysql</code>) y funciones personalizadas desde <code>Funciones.py</code>, como <code>StoreDuplicated</code> para duplicados, <code>estandarizar_direccion</code>, y <code>marcar_direcciones_estandarizadas</code> para estandarizaci\u00f3n de direcciones, as\u00ed como <code>obtener_conexion</code> y <code>cargar_tablas</code> para la gesti\u00f3n de datos. La prueba <code>testfunciones()</code> asegura que las funciones est\u00e1n operativas.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, obtener_conexion, cargar_tablas, testfunciones, setup_logger\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 27-10-2024 09:51\n</code></pre> <pre><code>logger = setup_logger(log_filename='ServicioSocial_Fijos.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-27 09:51:27,760 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#definicion-de-consultas-sql-para-la-carga-de-datos-de-personas","title":"Definici\u00f3n de Consultas SQL para la Carga de Datos de Personas","text":"<p>Este c\u00f3digo establece consultas SQL en <code>qr_structure</code> y <code>qr_structureNeith</code> para extraer datos de personas desde dos fuentes diferentes (<code>nsijec.personas</code> y <code>saipi.personas</code>). Estas consultas recuperan informaci\u00f3n relevante como <code>ID_AFILIADO</code>, <code>RAZON_SOCIAL</code>, <code>FECHA_NACIMIENTO</code>, <code>SEXO</code>, <code>CODDOC</code>, <code>DOCUMENTO</code>, y nombres y apellidos, asegurando que los campos requeridos en <code>required_columns</code> est\u00e9n presentes en ambas estructuras. Esto permite integrar y estandarizar datos personales de distintas fuentes dentro del Data Warehouse, facilitando el an\u00e1lisis y reportes de manera unificada.</p> <pre><code># Definir las consultas SQL para cargar datos y asegurarse de crear la estructura con los campos requeridos\nrequired_columns = ['ID_AFILIADO', 'RAZON_SOCIAL','FECHA_NACIMIENTO', 'SEXO', 'CODDOC', 'DOCUMENTO', 'PRIMER_NOMBRE', 'SEGUNDO_NOMBRE', 'PRIMER_APELLIDO', 'SEGUNDO_APELLIDO']\n#Lista de querys\nqr_structure = {\n    \"personas_jec\":'''select \n                CONCAT(t.tipo,p.identificacion) AS ID_AFILIADO,\n                CONCAT(\n                    IFNULL(pri_nombre, ''), ' ', \n                    IFNULL(seg_nombre, ''), ' ', \n                    IFNULL(pri_apellido, ''), ' ', \n                    IFNULL(seg_apellido, '')\n                    ) AS RAZON_SOCIAL,\n                fecha_nac as FECHA_NACIMIENTO,\n                sexo as SEXO,\n                t.tipo as CODDOC,\n                p.identificacion as DOCUMENTO,\n                pri_nombre AS PRIMER_NOMBRE,\n                seg_nombre AS SEGUNDO_NOMBRE,\n                pri_apellido AS PRIMER_APELLIDO,\n                seg_apellido AS SEGUNDO_APELLIDO,\n                \"nsijec.personas\" AS FUENTE\n                from nsijec.personas p \n            INNER JOIN nsijec.tipos_idens t on p.tipo_identificacion = t.id'''\n\n               }\n\n#Lista de querys Neith\nqr_structureNeith = {\n    \"persona_saipi\":'''select \n                CONCAT(t.referencia,p.identificacion) AS ID_AFILIADO,\n                CONCAT(\n                        IFNULL(prinom, ''), ' ', \n                        IFNULL(segnom, ''), ' ', \n                        IFNULL(priape, ''), ' ', \n                        IFNULL(segape, '')\n                        ) AS RAZON_SOCIAL,\n                fecnac as FECHA_NACIMIENTO,\n                t.referencia as CODDOC,\n                p.identificacion as DOCUMENTO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                \"saipi.personas\" AS FUENTE\n                from saipi.personas p\n            INNER JOIN saipi.tipos_identificacion t on p.tipiden = t.id'''\n    }\n\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-27 09:51:27,781 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#conexion-y-carga-de-tablas-de-personas-en-bases-minerva-y-neith","title":"Conexi\u00f3n y Carga de Tablas de Personas en Bases Minerva y Neith","text":"<p>Este c\u00f3digo se conecta a las bases de datos <code>minerva</code> y <code>neith</code>, cargando datos de las tablas <code>personas_jec</code> y <code>persona_saipi</code> en <code>df_structure</code>. Tras cargar las tablas, asegura que todas las columnas necesarias (<code>required_columns</code>) est\u00e1n presentes, agregando columnas faltantes con el valor <code>\"SinDato\"</code>.</p> <p>Luego, las tablas <code>personas_jec</code> y <code>persona_saipi</code> se combinan en un solo DataFrame <code>df_combined</code>, integrando todos los datos en una estructura com\u00fan. Este proceso garantiza que las tablas cumplen con la estructura requerida, permitiendo an\u00e1lisis unificado de los datos de ambas fuentes.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 09:51:27,949 - INFO - CONEXION A BASE MINERVA\n2024-10-27 09:51:28,947 - INFO - Cargando personas_jec \n2024-10-27 09:51:37,773 - INFO - Cargada personas_jec, 65,568 registros finales obtenidos. --- 8.83 seconds ---\n2024-10-27 09:51:37,875 - INFO - CARGUE TABLAS DESDE MYSQL --- personas_jec --- 9.92 seconds ---\n</code></pre> <pre><code># Cargar tablas desde la base 'neith'\nmotorNeith = create_engine(obtener_conexion('neith'))\ncargar_tablas(motorNeith, qr_structureNeith, df_structure, logger)\n</code></pre> <pre><code>2024-10-27 09:51:39,178 - INFO - Cargando persona_saipi \n2024-10-27 09:51:42,120 - INFO - Cargada persona_saipi, 15,097 registros finales obtenidos. --- 2.94 seconds ---\n2024-10-27 09:51:42,210 - INFO - CARGUE TABLAS DESDE MYSQL --- persona_saipi --- 4.32 seconds ---\n</code></pre> <pre><code>for ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    df = df_structure[ky].copy()\n    # Asegurarse de que las columnas requeridas est\u00e9n presentes en la estructura\n    for column in required_columns:\n        if column not in df.columns:\n            df[column] = \"SinDato\"\n\n    #df_structure[key] = df[required_columns]\n    logger.info(f'TABLA {ky} CARGADA Y ESTRUCTURADA')\n</code></pre> <pre><code>2024-10-27 09:51:42,237 - INFO - TABLA personas_jec CARGADA Y ESTRUCTURADA\n2024-10-27 09:51:42,252 - INFO - TABLA persona_saipi CARGADA Y ESTRUCTURADA\n</code></pre> <pre><code># Concatenar las tablas de \ndf_combined = pd.concat([df_structure['personas_jec'], df_structure['persona_saipi']], ignore_index=True)\nlogger.info('TABLAS personas_jec y persona_saipi COMBINADAS')\n</code></pre> <pre><code>2024-10-27 09:51:42,282 - INFO - TABLAS personas_jec y persona_saipi COMBINADAS\n</code></pre> <pre><code>df_combined\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 0 CC1782509 FRANCISCO  IZQUIERDO IZQUIERDO 1980-11-12 MASCULINO CC 1782509 FRANCISCO None IZQUIERDO IZQUIERDO nsijec.personas 1 CC26883143 DELIS MARIA  PACHECO GUERRERO 1981-05-05 FEMENINO CC 26883143 DELIS MARIA PACHECO GUERRERO nsijec.personas 2 CC26905242 MELISSA  DAVILA 1992-03-04 FEMENINO CC 26905242 MELISSA DAVILA nsijec.personas 3 CC26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO 1900-01-13 FEMENINO CC 26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO nsijec.personas 4 CC34511420 JHONAIKER JOSUE GUERRERO  CORDOVA 2010-03-18 MASCULINO CC 34511420 JHONAIKER JOSUE GUERRERO CORDOVA nsijec.personas ... ... ... ... ... ... ... ... ... ... ... ... 80660 PE7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO 2019-12-27 NaN PE 7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO saipi.personas 80661 PE7824433 SAMANTHA LUCIA LEON HRRERA 2019-03-11 NaN PE 7824433 SAMANTHA LUCIA LEON HRRERA saipi.personas 80662 PE7829018 ELISBETH VICTORIA ARDITO MEDINA 2017-12-22 NaN PE 7829018 ELISBETH VICTORIA ARDITO MEDINA saipi.personas 80663 PE7950951 ANGEL JAVIER HERNANDEZ VEGA 2018-11-17 NaN PE 7950951 ANGEL JAVIER HERNANDEZ VEGA saipi.personas 80664 PE987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ 1985-03-10 NaN PE 987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ saipi.personas <p>80665 rows \u00d7 11 columns</p>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#filtrar-registros-unicos-por-id","title":"Filtrar registros \u00fanicos por <code>id</code>","text":"<p>Se eliminan los registros duplicados en el dataframe <code>df_colegio_combined</code> manteniendo solo los registros con <code>id</code> \u00fanicos. El dataframe resultante se almacena en <code>df_colegio_unique</code>, y el proceso se registra en el log.</p> <pre><code># Obtener solo los registros con 'id' \u00fanicos\ndf_unique = df_combined.drop_duplicates(subset='ID_AFILIADO').copy()\nlogger.info('REGISTROS \u00daNICOS DE ID OBTENIDOS')\n</code></pre> <pre><code>2024-10-27 09:51:42,473 - INFO - REGISTROS \u00daNICOS DE ID OBTENIDOS\n</code></pre> <pre><code>df_unique\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 0 CC1782509 FRANCISCO  IZQUIERDO IZQUIERDO 1980-11-12 MASCULINO CC 1782509 FRANCISCO None IZQUIERDO IZQUIERDO nsijec.personas 1 CC26883143 DELIS MARIA  PACHECO GUERRERO 1981-05-05 FEMENINO CC 26883143 DELIS MARIA PACHECO GUERRERO nsijec.personas 2 CC26905242 MELISSA  DAVILA 1992-03-04 FEMENINO CC 26905242 MELISSA DAVILA nsijec.personas 3 CC26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO 1900-01-13 FEMENINO CC 26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO nsijec.personas 4 CC34511420 JHONAIKER JOSUE GUERRERO  CORDOVA 2010-03-18 MASCULINO CC 34511420 JHONAIKER JOSUE GUERRERO CORDOVA nsijec.personas ... ... ... ... ... ... ... ... ... ... ... ... 80660 PE7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO 2019-12-27 NaN PE 7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO saipi.personas 80661 PE7824433 SAMANTHA LUCIA LEON HRRERA 2019-03-11 NaN PE 7824433 SAMANTHA LUCIA LEON HRRERA saipi.personas 80662 PE7829018 ELISBETH VICTORIA ARDITO MEDINA 2017-12-22 NaN PE 7829018 ELISBETH VICTORIA ARDITO MEDINA saipi.personas 80663 PE7950951 ANGEL JAVIER HERNANDEZ VEGA 2018-11-17 NaN PE 7950951 ANGEL JAVIER HERNANDEZ VEGA saipi.personas 80664 PE987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ 1985-03-10 NaN PE 987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ saipi.personas <p>80587 rows \u00d7 11 columns</p>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#carga-de-datos-fijos-desde-dwh-y-filtrado-de-registros-faltantes","title":"Carga de Datos Fijos desde DWH y Filtrado de Registros Faltantes","text":"<p>Este c\u00f3digo se conecta a la base de datos DWH y carga la tabla <code>BD_Dim_Datos_Fijos</code> en <code>df_structure_dwh</code>, verificando los datos actuales en el Data Warehouse. Luego, realiza un filtrado en <code>df_unique</code> para encontrar registros que no est\u00e1n presentes en <code>df_dwh</code> comparando el campo <code>ID_AFILIADO</code>.</p> <p>El resultado, <code>df_missing_in_dwh</code>, contiene los registros \u00fanicos que faltan en <code>BD_Dim_Datos_Fijos</code>, permitiendo identificar qu\u00e9 datos deben ser actualizados o a\u00f1adidos en el Data Warehouse.</p> <pre><code>#Lista de querys\nqr_structure_dwh = {\n    \"Datos_fijos\":'''SELECT * FROM dwh.BD_Dim_Datos_Fijos\n    '''\n               }\n\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS')\n\n#Conexion a base dwh\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-27 09:51:42,537 - INFO - LECTURA DE QUERYS\n2024-10-27 09:51:42,540 - INFO - CONEXION A BASE DWH\n2024-10-27 09:51:44,202 - INFO - Cargando Datos_fijos \n2024-10-27 09:51:44,253 - INFO - Cargada Datos_fijos, 2 registros finales obtenidos. --- 0.05 seconds ---\n2024-10-27 09:51:44,344 - INFO - CARGUE TABLAS DESDE MYSQL --- Datos_fijos --- 1.80 seconds ---\n</code></pre> <pre><code>df_dwh = df_structure_dwh[\"Datos_fijos\"].copy()\n</code></pre> <pre><code># Filtrar los registros \u00fanicos que no est\u00e1n presentes en DimDatosFijos\n\ndf_missing_in_dwh = df_unique[~df_unique['ID_AFILIADO'].isin(df_dwh['ID_AFILIADO'])]\nlogger.info('FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos')\n</code></pre> <pre><code>2024-10-27 09:51:44,390 - INFO - FILTRADOS REGISTROS \u00danICOS QUE NO EST\u00c1N EN DimDatosFijos\n</code></pre> <pre><code>df_missing_in_dwh\n</code></pre> ID_AFILIADO RAZON_SOCIAL FECHA_NACIMIENTO SEXO CODDOC DOCUMENTO PRIMER_NOMBRE SEGUNDO_NOMBRE PRIMER_APELLIDO SEGUNDO_APELLIDO FUENTE 0 CC1782509 FRANCISCO  IZQUIERDO IZQUIERDO 1980-11-12 MASCULINO CC 1782509 FRANCISCO None IZQUIERDO IZQUIERDO nsijec.personas 1 CC26883143 DELIS MARIA  PACHECO GUERRERO 1981-05-05 FEMENINO CC 26883143 DELIS MARIA PACHECO GUERRERO nsijec.personas 2 CC26905242 MELISSA  DAVILA 1992-03-04 FEMENINO CC 26905242 MELISSA DAVILA nsijec.personas 3 CC26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO 1900-01-13 FEMENINO CC 26914344 ROSIRIS DEL CARMEN CANTILLO GAMERO nsijec.personas 4 CC34511420 JHONAIKER JOSUE GUERRERO  CORDOVA 2010-03-18 MASCULINO CC 34511420 JHONAIKER JOSUE GUERRERO CORDOVA nsijec.personas ... ... ... ... ... ... ... ... ... ... ... ... 80660 PE7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO 2019-12-27 NaN PE 7780022 EMMA ROSSIBELL VASQUEZ ZAMBRANO saipi.personas 80661 PE7824433 SAMANTHA LUCIA LEON HRRERA 2019-03-11 NaN PE 7824433 SAMANTHA LUCIA LEON HRRERA saipi.personas 80662 PE7829018 ELISBETH VICTORIA ARDITO MEDINA 2017-12-22 NaN PE 7829018 ELISBETH VICTORIA ARDITO MEDINA saipi.personas 80663 PE7950951 ANGEL JAVIER HERNANDEZ VEGA 2018-11-17 NaN PE 7950951 ANGEL JAVIER HERNANDEZ VEGA saipi.personas 80664 PE987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ 1985-03-10 NaN PE 987518 YOELIS CHIQUINQUIRA MORALES MARQUEZ saipi.personas <p>80587 rows \u00d7 11 columns</p> <pre><code>\n</code></pre>"},{"location":"seccion/5.1-FactServicioSocial_Fijos/#guardar-registros-faltantes-en-la-tabla-dimdatosfijos","title":"Guardar registros faltantes en la tabla <code>DimDatosFijos</code>","text":"<p>Se conectan los registros faltantes en la tabla <code>BD_DimDatosFijos</code> en el DWH y se almacenan los registros del dataframe <code>df_missing_in_dwh</code> en la base de datos utilizando el modo <code>append</code>. El proceso de almacenamiento se registra en el log, y en caso de error, se captura y se registra el mensaje correspondiente.</p> <pre><code>guardar_en_dwh(df_missing_in_dwh, 'BD_Dim_Datos_Fijos', logger, multiple=False, if_exists='append')\n</code></pre> <pre><code>2024-10-27 09:51:44,422 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-27 09:51:44,425 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-27 09:51:45,442 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Datos_Fijos\n2024-10-27 09:51:56,325 - INFO - Tabla almacenada correctamente. 80,587 registros finales obtenidos.\n2024-10-27 09:51:56,471 - INFO - ALMACENAMIENTO ---  --- 12.05 seconds ---\n2024-10-27 09:51:56,473 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-27 09:51:56,505 - INFO - FINAL ETL --- 28.80 seconds ---\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/","title":"5.2 Fact Servicio Social","text":""},{"location":"seccion/5.2-FactServicioSocial/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL <code>FactServicioSocial</code> permite extraer, transformar y cargar informaci\u00f3n clave sobre estudiantes y sus parientes en un Data Warehouse. Las tablas origen se extraen de dos bases (<code>nsijec</code> y <code>saipi</code>), se realizan validaciones de duplicados, limpieza de texto y estandarizaci\u00f3n de direcciones, asegurando que los datos cargados en el DWH sean completos, consistentes y listos para an\u00e1lisis posteriores.</p>"},{"location":"seccion/5.2-FactServicioSocial/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactServicioSocial\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant Neith_DB as Base de Datos Neith\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer `estudiantes`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de `estudiantes`\n    ETL_Script -&gt;&gt; Neith_DB: Conecta a BD Neith\n    Neith_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Neith_DB: Ejecuta consultas SQL para extraer `personas_saipi`, `acudientes`, `registros`, `valoracion`\n    Neith_DB --&gt;&gt; ETL_Script: Retorna datos de `saipi`\n    ETL_Script -&gt;&gt; ETL_Script: Valida duplicados y transforma datos\n    ETL_Script -&gt;&gt; ETL_Script: Estandariza direcciones\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en tablas de DWH (`Fact_nsijec_Estudiantes`, `Dim_saipi_Personas`, etc.)\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre> <p>Este diagrama muestra c\u00f3mo el proceso ETL asegura que los datos de los estudiantes y sus parientes est\u00e9n consolidados en el DWH, permitiendo mantener una base de datos uniforme y de calidad.</p>"},{"location":"seccion/5.2-FactServicioSocial/#etl","title":"ETL","text":""},{"location":"seccion/5.2-FactServicioSocial/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 10:45\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>ServicioSocial.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='ServicioSocial.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 10:45:43,742 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#consultas-para-extraer-datos-de-estudiantes-y-registros-de-saipi","title":"Consultas para Extraer Datos de Estudiantes y Registros de SAIPI","text":"<p>Este c\u00f3digo configura las consultas SQL para extraer datos de estudiantes y beneficiarios desde las bases de datos <code>nsijec</code> y <code>saipi</code>. El diccionario <code>qr_structure</code> contiene una consulta para la tabla de <code>estudiantes</code> en <code>nsijec</code>, extrayendo detalles personales y de inscripci\u00f3n. <code>qr_structureNeith</code> agrupa consultas para tablas en <code>saipi</code>, incluyendo <code>personas_saipi</code>, <code>acudientes</code>, <code>registros</code>, y <code>valoracion</code>, cada una de las cuales selecciona informaci\u00f3n detallada sobre identificaci\u00f3n, parentesco, registros de vinculaci\u00f3n y valoraciones m\u00e9dicas.</p> <p>El diccionario <code>dim_names</code> define nombres descriptivos para los resultados en <code>df_structure</code>, donde se almacenar\u00e1n estos datos para an\u00e1lisis y reportes. Esta configuraci\u00f3n asegura una carga organizada y estandarizada para un an\u00e1lisis de datos integral sobre los beneficiarios y sus relaciones en ambas bases de datos.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"estudiantes\":'''select \n                e.id AS ID_REGISTRO,\n                CONCAT(t.tipo,p.identificacion) AS ID_AFILIADO,\n                DATE_FORMAT(fecha_inscripcion, '%Y%m') AS PERIODO,\n                t.tipo as TIPO_DOCUMENTO,\n                p.identificacion as DOCUMENTO,\n                #e.persona AS ID_PERSONA, \n                #e.convenio AS NUMERO_CONVENIO,\n                con.convenio as NOMBRE_CONVENIO,\n                e.enfermedad_alergia,\n                e.estado,\n                e.fecha_inscripcion,\n                #e.grado,\n                gr.descripcion as GRADO,\n                #e.institucion,\n                ins.institucion,\n                ins.codigo as CODIGO_DANE_INSTITUCION,\n                e.nombre_enfermedad_alergia,\n                e.nombres_medicamentos,\n                e.observacion,\n                e.pob_estudiante,\n                e.toma_medicamentos,\n                e.trabajador_adolecente AS TRABAJADOR_ADOLESCENTE,\n                e.updated_at as FECHA_ACTUALIZACION\n                from nsijec.estudiantes e\n            INNER JOIN nsijec.personas p ON e.persona = p.id\n            INNER JOIN nsijec.tipos_idens t on p.tipo_identificacion = t.id\n            INNER JOIN nsijec.convenios con on e.convenio = con.id\n            INNER JOIN nsijec.grados gr on e.grado = gr.id\n            INNER JOIN nsijec.instituciones_educativas ins on e.institucion = ins.id\n\n\n            WHERE fecha_inscripcion &gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and fecha_inscripcion &lt;= curdate();'''\n    #,\n    #Tabla parientes vac\u00eda\n    #\"colegio13\":'''select \n    #            id AS ID_REGISTRO,\n    #            acudiente AS NOMBRE_ACUDIENTE,\n    #            created_at,\n    #            created_by,\n    #            direccion,\n    #            email,\n    #            estrato,\n    #            estudiante,\n    #            municipio,\n    #            parentesco,\n    #            pariente,\n    #            telefono,\n    #            updated_at,\n    #            updated_by\n    #            from nsijec.parientes'''                \n               }\n\n#Lista de querys Neith\nqr_structureNeith = {\n    \"personas_saipi\":'''select \n                p.id as ID_PERSONA,\n                CONCAT(t.referencia,p.identificacion) AS ID_AFILIADO,               \n                p.direccion,\n                p.telefono,\n                mun.codigo as CODIGO_CIUDAD,\n                p.estado,\n                p.created_at AS FECHA_CREACION,\n                p.updated_at AS FECHA_ACTUALIZACION\n                from saipi.personas p\n            INNER JOIN saipi.tipos_identificacion t on p.tipiden = t.id\n            INNER JOIN saipi.municipios mun on p.municipio_nacimiento = mun.id''',\n\n    \"acudientes\":'''SELECT \n                a.id as ID_ACUDIENTE,\n                a.registro_id AS ID_REGISTRO,\n                a.persona_id AS ID_PERSONA,\n                pa.detalle as PARENTESCO\n                FROM saipi.acudientes a\n                left join saipi.parentescos pa on a.parentesco_id = pa.id ''',\n\n    \"registros\":'''SELECT \n                    r.id AS ID_REGISTRO,\n                    r.persona AS ID_PERSONA,\n                    DATE_FORMAT(r.fecvin, '%Y%m') AS PERIODO,\n                    #r.convenio,\n                    c.detalle as CONVENIO,\n                    #r.modalidad,\n                    m.detalle as MODALIDAD,\n                    #r.tipben,\n                    b.detalle as TIPO_BENEFICIARIO,\n                    r.fecvin AS FECHA_VINCULACION,\n                    #r.municipio,\n                    mun.codigo as CODIGO_CIUDAD,\n                    #r.caracteristica_poblacion,\n                    cp.detalle as CARACTERISTICA_POBLACION,\n                    #r.poblacion,\n                    pb.detalle as POBLACION,\n                    r.sisben,\n                    r.puntaje AS PUNTAJE_SISBEN,\n                    r.icbf,\n                    r.discapacidad,\n                    r.afiliado,\n                    r.categoria,\n                    r.estafi AS ESTADO_AFILIACION,\n                    #r.pueblo,\n                    r.estado,\n                    r.created_at AS FECHA_CREACION,\n                    r.updated_at AS FECHA_ACTUALIZACION\n                FROM saipi.registros r\n            INNER JOIN saipi.convenios c on r.convenio = c.id\n            INNER JOIN saipi.modalidades m on r.modalidad = m.id\n            INNER JOIN saipi.beneficiarios b on r.tipben = b.id\n            INNER JOIN saipi.municipios mun on r.municipio = mun.id\n            INNER JOIN saipi.caracteristica_poblaciones cp on r.caracteristica_poblacion = cp.id\n            INNER JOIN saipi.poblaciones pb on r.poblacion = pb.id\n            WHERE DATE_FORMAT(r.fecvin, '%Y%m%01')&gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and r.fecvin &lt;=CURDATE()''',\n\n    \"valoracion\":'''SELECT \n            v.id AS ID_VALORACION,\n            v.registro_id AS ID_REGISTRO,\n            v.fecval AS FECHA_VALORACION,\n            v.talla,\n            v.peso,\n            v.p_braquial,\n            v.p_cefalico,\n            v.observaciones,\n            v.estado,\n            v.created_at AS FECHA_CREACION,\n            v.updated_at AS FECHA_ACTUALIZACION\n        FROM saipi.valoracion v\n        INNER JOIN (select \n        r.id, r.fecvin \n        FROM saipi.registros r\n        WHERE DATE_FORMAT(r.fecvin, '%Y%m%01')&gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and r.fecvin &lt;=CURDATE() ) reg\n        on reg.id = v.registro_id'''\n    }\n\ndim_names = {\n    \"estudiantes\":'BD_Fact_nsijec_Estudiantes',\n    #\"colegio13\":'BD_Fact_nsijec_Parientes',\n    \"personas_saipi\" : 'BD_Dim_saipi_Personas',\n    \"acudientes\":'BD_Fact_saipi_Acudientes',\n    \"registros\":'BD_Fact_saipi_Registros',\n    \"valoracion\":'BD_Fact_saipi_Valoraciones'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 10:45:43,759 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code># Cargar tablas desde la base 'minerva'\nmotor = create_engine(obtener_conexion('minerva'))\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 10:45:44,205 - INFO - Cargando estudiantes \n2024-10-30 10:45:45,745 - INFO - Cargada estudiantes, 19,777 registros finales obtenidos. --- 1.54 seconds ---\n2024-10-30 10:45:45,813 - INFO - CARGUE TABLAS DESDE MYSQL --- estudiantes --- 2.02 seconds ---\n</code></pre> <pre><code># Cargar tablas desde la base 'neith'\nmotorNeith = create_engine(obtener_conexion('neith'))\ncargar_tablas(motorNeith, qr_structureNeith, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 10:45:46,301 - INFO - Cargando personas_saipi \n2024-10-30 10:45:46,973 - INFO - Cargada personas_saipi, 15,210 registros finales obtenidos. --- 0.67 seconds ---\n2024-10-30 10:45:46,975 - INFO - Cargando acudientes \n2024-10-30 10:45:47,122 - INFO - Cargada acudientes, 8,178 registros finales obtenidos. --- 0.15 seconds ---\n2024-10-30 10:45:47,123 - INFO - Cargando registros \n2024-10-30 10:45:47,727 - INFO - Cargada registros, 8,182 registros finales obtenidos. --- 0.60 seconds ---\n2024-10-30 10:45:47,728 - INFO - Cargando valoracion \n2024-10-30 10:45:48,264 - INFO - Cargada valoracion, 15,211 registros finales obtenidos. --- 0.54 seconds ---\n2024-10-30 10:45:48,330 - INFO - CARGUE TABLAS DESDE MYSQL --- valoracion --- 2.50 seconds ---\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas de <code>df_structure</code>, excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:45:48,489 - INFO - VALIDADOR TABLA: estudiantes\n2024-10-30 10:45:48,530 - INFO - VALIDADOR TABLA: personas_saipi\n2024-10-30 10:45:48,547 - INFO - VALIDADOR TABLA: acudientes\n2024-10-30 10:45:48,602 - INFO - VALIDADOR TABLA: registros\n2024-10-30 10:45:48,629 - INFO - VALIDADOR TABLA: valoracion\n2024-10-30 10:45:48,631 - INFO - VALIDADOR DUPLICADOS --- 0.29 seconds ---\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:45:49,671 - INFO - LIMPIEZA --- 1.03 seconds ---\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#estandarizar-direcciones","title":"Estandarizar direcciones","text":"<p>Este bloque de c\u00f3digo llama la funci\u00f3n estandarizaci\u00f3n de direcciones, lo cual permite limpiar las direcciones y marcar aquellas que empiezan con las siglas de carrera, calle, diagonal, transversal, kilometro, manzana, lote, carretera, finca o las que no registran como estandarizadas.</p> <pre><code>df_personas = df_structure['personas_saipi'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_personas.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_personas['DIRECCION_ORIGINAL'] = df_personas['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_personas['DIRECCION_LIMPIA'] = df_personas['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_personas, 'DIRECCION_LIMPIA')\n\ndf_structure['personas_saipi'] = df_personas.copy()\n#df_structure['personas_saipi']\n</code></pre>"},{"location":"seccion/5.2-FactServicioSocial/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH utilizando los nombres definidos en <code>dim_names</code>. Si la clave de la tabla no est\u00e1 presente en <code>dim_names</code>, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra, junto con los nombres de las tablas que se almacenaron correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:45:50,048 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:45:50,051 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:45:50,462 - INFO - Almacenando tabla estudiantes en DWH como BD_Fact_nsijec_Estudiantes\n2024-10-30 10:45:55,969 - INFO - Tabla estudiantes almacenada correctamente como BD_Fact_nsijec_Estudiantes.\n2024-10-30 10:45:55,971 - INFO - Almacenando tabla personas_saipi en DWH como BD_Dim_saipi_Personas\n2024-10-30 10:45:59,065 - INFO - Tabla personas_saipi almacenada correctamente como BD_Dim_saipi_Personas.\n2024-10-30 10:45:59,066 - INFO - Almacenando tabla acudientes en DWH como BD_Fact_saipi_Acudientes\n2024-10-30 10:46:00,581 - INFO - Tabla acudientes almacenada correctamente como BD_Fact_saipi_Acudientes.\n2024-10-30 10:46:00,582 - INFO - Almacenando tabla registros en DWH como BD_Fact_saipi_Registros\n2024-10-30 10:46:03,558 - INFO - Tabla registros almacenada correctamente como BD_Fact_saipi_Registros.\n2024-10-30 10:46:03,559 - INFO - Almacenando tabla valoracion en DWH como BD_Fact_saipi_Valoraciones\n2024-10-30 10:46:07,558 - INFO - Tabla valoracion almacenada correctamente como BD_Fact_saipi_Valoraciones.\n2024-10-30 10:46:07,655 - INFO - ALMACENAMIENTO ---  --- 17.61 seconds ---\n2024-10-30 10:46:07,656 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:46:07,668 - INFO - FINAL ETL --- 23.94 seconds ---\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/","title":"6.1 Fact Encuestas","text":""},{"location":"seccion/6.1-FactEncuestas/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL se enfoca en la obtenci\u00f3n, transformaci\u00f3n y almacenamiento de datos de encuestas en el Data Warehouse (DWH). Para ello, se realiza la extracci\u00f3n de datos desde diferentes tablas relacionadas con encuestas (<code>lime_questions</code>, <code>lime_answers</code>, entre otras) de bases de datos de encuestas y Neith, se transforman mediante limpieza y normalizaci\u00f3n, y finalmente, se cargan en la tabla <code>BD_Fact_Encuestas</code>. </p> <p>Se establecen conexiones a diferentes bases de datos para el acceso a tablas de encuestas y metadatos de usuarios, permitiendo crear relaciones entre identificadores, c\u00f3digos y respuestas de encuestas. El flujo del proceso de datos asegura que los datos se integren correctamente, permitiendo un an\u00e1lisis posterior en el DWH.</p>"},{"location":"seccion/6.1-FactEncuestas/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL para la Tabla FactEncuestas\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant ETL\n  participant BD_DWH as Base de Datos DWH\n  participant BD_Neith as Base de Datos Neith\n  \ud83d\udc64 Usuario-&gt;&gt;ETL: Solicitar procesamiento de datos\n  ETL-&gt;&gt;BD_Neith: Extraer encuestas y preguntas\n  BD_Neith--&gt;&gt;ETL: Datos de encuestas y preguntas\n  ETL-&gt;&gt;BD_DWH: Extraer inventario de tablas y diccionario de campos\n  BD_DWH--&gt;&gt;ETL: Inventario y diccionario\n  ETL-&gt;&gt;ETL: Transformar datos (limpieza, estandarizaci\u00f3n)\n  ETL-&gt;&gt;BD_DWH: Cargar datos transformados en `BD_Fact_Encuestas`\n  BD_DWH--&gt;&gt;ETL: Confirmaci\u00f3n de carga exitosa\n  ETL--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#etl","title":"ETL","text":""},{"location":"seccion/6.1-FactEncuestas/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno necesario para la manipulaci\u00f3n y carga de datos en un Data Warehouse, importando librer\u00edas esenciales (<code>pandas</code>, <code>sqlalchemy</code>, <code>BeautifulSoup</code> para procesamiento de HTML, y <code>logging</code>). Tambi\u00e9n incluye funciones personalizadas del archivo <code>Funciones.py</code>, como <code>guardar_en_dwh</code> para almacenar datos en el DWH, <code>Conexion_dwh</code> y <code>Conexion_neith</code> para la conexi\u00f3n a diferentes bases de datos, y <code>setup_logger</code> para el registro de eventos. La prueba <code>testfunciones()</code> verifica que las funciones se hayan importado correctamente y est\u00e1n listas para su uso.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time \nimport os \nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport logging\nfrom datetime import date\n\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, testfunciones, setup_logger, Conexion_dwh, Conexion_neith\n\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 10:26\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Encuestas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Encuestas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 10:26:52,904 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#funciones-para-limpieza-de-html-y-carga-de-consultas-en-paralelo","title":"Funciones para Limpieza de HTML y Carga de Consultas en Paralelo","text":"<p>Este c\u00f3digo define dos funciones para manejar datos: <code>limpiar_html</code> usa <code>BeautifulSoup</code> para extraer el contenido de texto de un HTML, eliminando etiquetas para dejar solo texto limpio. La funci\u00f3n <code>load_query</code> ejecuta una consulta SQL y carga los resultados en un DataFrame (<code>df_query</code>). Dise\u00f1ada para uso con <code>ThreadPoolExecutor</code>, permite cargar varias consultas en paralelo y devuelve <code>None</code> en caso de error, sin interrumpir el flujo principal.</p> <pre><code>def limpiar_html(texto_html):\n    soup = BeautifulSoup(texto_html, 'html.parser')\n    return soup.get_text()\n###Librerias para paralelizar\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef load_query(query):\n    try: \n        df_query = pd.read_sql_query(query, motor_consulta)\n        return df_query\n    except:\n        return None\n</code></pre> <pre><code>#Medir tiempos\nstart_time = time.time()\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion_pruebas = Conexion_dwh()\nmotor_pruebas = create_engine(cadena_conexion_pruebas)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 10:26:53,000 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-de-inventario-de-tablas-en-base-de-datos-de-encuestas","title":"Consulta de Inventario de Tablas en Base de Datos de Encuestas","text":"<p>Este bloque de c\u00f3digo consulta el inventario de tablas en la base de datos <code>encuestas</code> para identificar aquellas incluidas en la bodega de datos (<code>SeIncluyeEnBodega = \"Si\"</code>). La consulta se ejecuta en la tabla de inventario <code>gb_Dim_Bodega_Inventario de Tablas</code> en el Data Warehouse (<code>dwh</code>), filtrando las tablas del servidor con <code>IdServidor = 3</code>. El resultado se almacena en <code>df_tablas</code>, que contendr\u00e1 los nombres de las bases de datos y tablas relevantes para su posterior uso en an\u00e1lisis o cargas de datos en el entorno de encuestas.</p> <pre><code>##1. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla FROM dwh.`gb_Dim_Bodega_Inventario de Tablas`  \n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_tablas = pd.read_sql_query(sa.text(qr_structure), conn)\n#df_tablas\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#extraccion-de-codigos-de-encuestas-desde-nombres-de-tablas","title":"Extracci\u00f3n de C\u00f3digos de Encuestas desde Nombres de Tablas","text":"<p>Este c\u00f3digo extrae los c\u00f3digos de encuestas (<code>sid</code>) a partir de la columna <code>NombreTabla</code> en <code>df_tablas</code>. Utiliza el m\u00e9todo <code>str.split</code> con <code>\"_\"</code> como delimitador y expande el resultado en varias columnas, seleccionando el tercer elemento (<code>[2]</code>) como el c\u00f3digo de encuesta. Luego, convierte <code>sid</code> a tipo <code>str</code> para asegurar la consistencia de formato, facilitando su uso en an\u00e1lisis o filtrado posterior.</p> <pre><code>## Lista de codigos de encuestas\n#df_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\")\ndf_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\",expand = True)[2]\ndf_tablas[\"sid\"] = df_tablas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#conexion-a-la-base-de-datos-neith","title":"Conexi\u00f3n a la base de datos Neith","text":"<p>Se establece la conexi\u00f3n con la base de datos Neith utilizando la funci\u00f3n <code>Conexion_neith()</code> para generar la cadena de conexi\u00f3n y <code>create_engine()</code> para crear el motor de conexi\u00f3n. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base Neith\ncadena_conexion_neith = Conexion_neith()\nmotor_neith = create_engine(cadena_conexion_neith)\nlogger.info('CONEXION A BASE Neith')\n</code></pre> <pre><code>2024-10-30 10:26:53,541 - INFO - CONEXION A BASE Neith\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-de-nombres-de-encuestas-en-la-base-de-datos-de-limesurvey","title":"Consulta de Nombres de Encuestas en la Base de Datos de LimeSurvey","text":"<p>Este c\u00f3digo extrae los nombres de las encuestas desde la tabla <code>lime_surveys_languagesettings</code> en la base de datos <code>encuestas</code>. La consulta selecciona el ID de la encuesta (<code>surveyls_survey_id</code> como <code>sid</code>) y el t\u00edtulo (<code>surveyls_title</code> como <code>NombreEncuesta</code>). El resultado se almacena en el DataFrame <code>df_nombre_encuestas</code>, con la columna <code>sid</code> convertida a tipo <code>str</code> para facilitar su uso en futuras uniones o consultas. </p> <p>Este DataFrame proporciona una referencia de los nombres y IDs de las encuestas, \u00fatil para an\u00e1lisis de respuestas o estructuras en encuestas de LimeSurvey.</p> <pre><code>##2. Consultar nombres de las encuestas\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT surveyls_survey_id as sid, surveyls_title as NombreEncuesta \n    FROM encuestas.lime_surveys_languagesettings \"\"\"\n    df_nombre_encuestas = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_nombre_encuestas[\"sid\"] = df_nombre_encuestas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#fusion-de-codigos-y-nombres-de-encuestas-y-exportacion-a-excel","title":"Fusi\u00f3n de C\u00f3digos y Nombres de Encuestas y Exportaci\u00f3n a Excel","text":"<p>Este c\u00f3digo combina los DataFrames <code>df_tablas</code> y <code>df_nombre_encuestas</code> mediante una uni\u00f3n izquierda (<code>merge</code>) en la columna <code>sid</code>, agregando los nombres de las encuestas (<code>NombreEncuesta</code>) a sus respectivos c\u00f3digos en <code>df_tablas</code>. Las filas con valores nulos resultantes de la combinaci\u00f3n se eliminan, y el \u00edndice se reinicia para un formato uniforme. </p> <p>El DataFrame resultante <code>df_tablas_nombres</code> se exporta a un archivo Excel (<code>df_tablas.xlsx</code>) para facilitar su visualizaci\u00f3n y an\u00e1lisis externo, mientras que <code>df_tablas_codigos</code> se define como una vista simplificada con solo las columnas <code>NombreBaseDeDatos</code>, <code>NombreTabla</code> y <code>sid</code>.</p> <pre><code>## Nombres de las tablas de encuestas\ndf_tablas_nombres = pd.merge(df_tablas,df_nombre_encuestas, how ='left', on = 'sid')\ndf_tablas_nombres = df_tablas_nombres.dropna().copy().reset_index(drop =True)\n</code></pre> <pre><code>df_tablas_nombres.to_excel('df_tablas.xlsx',index=False)\n</code></pre> <pre><code>df_tablas_codigos = df_tablas_nombres[['NombreBaseDeDatos', 'NombreTabla', 'sid']]\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-del-diccionario-de-campos-de-la-base-de-encuestas","title":"Consulta del diccionario de campos de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los campos de las tablas de la base de datos <code>encuestas</code>, que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, y <code>NombreCampo</code> desde la tabla <code>gb_Dim_Bodega_Diccionario de Campos</code> en el DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que los campos est\u00e9n marcados como incluidos en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_campos</code>.</p> <pre><code>##3. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla, NombreCampo FROM dwh.`gb_Dim_Bodega_Diccionario de Campos`\n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_campos = pd.read_sql_query(sa.text(qr_structure), conn)\n</code></pre> <pre><code>df_campos_codigo = pd.merge(df_campos,df_tablas_codigos, how ='right', on = ['NombreBaseDeDatos', 'NombreTabla'])\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#extraccion-y-transformacion-de-codigos-qid-y-parent_qid","title":"Extracci\u00f3n y transformaci\u00f3n de c\u00f3digos <code>qid</code> y <code>parent_qid</code>","text":"<p>Se crea la columna <code>qid</code> en el dataframe <code>df_campos_codigo</code>, extrayendo parte del nombre de campo dividiendo el texto por \"X\". Luego, se limpian los datos y se convierten los valores a formato <code>str</code>. Adem\u00e1s, se genera la columna <code>parent_qid</code> tomando los primeros d\u00edgitos del c\u00f3digo <code>qid</code>, mientras que <code>qid</code> contiene los \u00faltimos cinco caracteres.</p> <pre><code>df_campos_codigo['qid'] = df_campos_codigo['NombreCampo'].str.split(\"X\",expand = True)[2]\ndf_campos_codigo_qid = df_campos_codigo.dropna().copy().reset_index(drop =True)\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid[\"qid\"].astype(str)\ndf_campos_codigo_qid[\"parent_qid\"] = df_campos_codigo_qid['qid'].str[:-5]\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid['qid'].str[-5:]\n</code></pre> <pre><code>#df_campos_codigo_qid.to_excel('t1.xlsx',index=False)\n</code></pre> <pre><code>#df_campos_codigo_qid['qid'].str[-5:]\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-de-la-tabla-lime_questions","title":"Consulta de la tabla <code>lime_questions</code>","text":"<p>Se ejecuta una consulta SQL para obtener los datos de la tabla <code>lime_questions</code> en la base de datos <code>encuestas</code>. La consulta selecciona las columnas <code>qid</code>, <code>parent_qid</code>, <code>sid</code>, y <code>title</code>. Posteriormente, las columnas <code>qid</code>, <code>parent_qid</code>, y <code>sid</code> se convierten al tipo <code>str</code> para asegurar una correcta manipulaci\u00f3n de datos. El resultado de la consulta se almacena en el dataframe <code>df_lime_questions</code>.</p> <pre><code>##4. Consultar la tabla lime_questions\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT qid, parent_qid, sid,title FROM encuestas.lime_questions \"\"\"\n    df_lime_questions = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions[\"qid\"] = df_lime_questions[\"qid\"].astype(str)\ndf_lime_questions[\"parent_qid\"] = df_lime_questions[\"parent_qid\"].astype(str)\ndf_lime_questions[\"sid\"] = df_lime_questions[\"sid\"].astype(str)\ndf_lime_questions\n</code></pre> qid parent_qid sid title 0 216 186 832591 SQ004 1 217 186 832591 SQ005 2 218 186 832591 SQ006 3 219 186 832591 SQ007 4 220 186 832591 SQ008 ... ... ... ... ... 1526 2753 0 354133 Q22 1527 2754 0 354133 Q23 1528 2755 0 354133 SUMRALO 1529 2756 0 354133 SUMTOTAL 1530 2757 0 354133 RESULTADO <p>1531 rows \u00d7 4 columns</p>"},{"location":"seccion/6.1-FactEncuestas/#ajuste-de-qid-para-las-tablas","title":"Ajuste de <code>qid</code> para las tablas","text":"<p>Se realiza un proceso de combinaci\u00f3n entre los dataframes <code>df_campos_codigo_qid</code> y <code>df_lime_questions</code> para arreglar los campos <code>qid</code>. Se ajustan los nombres de columnas y se completan los valores faltantes de <code>qid</code> utilizando los valores de respaldo. El dataframe resultante, <code>df_campos_2</code>, contiene las columnas clave como <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, <code>NombreCampo</code>, <code>sid</code>, y el <code>qid</code> corregido.</p> <pre><code>#Arreglar qid para las tablas\ndf_campos_1 = pd.merge( df_campos_codigo_qid , df_lime_questions , how ='left', on = ['sid', 'qid'])\ndf_campos_1.rename(columns={'title': 'title_1', 'parent_qid_x':'parent_qid'}, inplace=True)\ndf_campos_1['title'] = df_campos_1['qid']\ndf_campos_2 = pd.merge(df_campos_1,df_lime_questions, how ='left', on = ['sid', 'title', 'parent_qid'])\ndf_campos_2['qid_y'].fillna(df_campos_2['qid_x'], inplace=True)\ndf_campos_2 = df_campos_2[['NombreBaseDeDatos', 'NombreTabla', 'NombreCampo', 'sid', 'qid_y']].copy()\ndf_campos_2.rename(columns={'qid_y':'qid'}, inplace=True)\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-y-limpieza-de-preguntas-de-encuestas-lime_questions_l10ns","title":"Consulta y limpieza de preguntas de encuestas (<code>lime_questions_l10ns</code>)","text":"<p>Se consulta la tabla <code>lime_question_l10ns</code> de la base de datos <code>encuestas</code> para obtener informaci\u00f3n detallada sobre las preguntas de encuestas. Se combina este resultado con el dataframe <code>df_campos_2</code>, eliminando columnas irrelevantes como <code>help</code>, <code>script</code>, y <code>language</code>. Luego, se limpian las preguntas usando la funci\u00f3n <code>limpiar_html</code> para eliminar etiquetas HTML, y se eliminan los caracteres especiales, como corchetes. Finalmente, se filtran las filas para excluir registros con <code>qid</code> no v\u00e1lidos como <code>'count'</code> y <code>'other'</code>, y el dataframe se reorganiza con los datos limpios.</p> <pre><code>##4. Consultar la tabla lime_questions q10\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT * FROM encuestas.lime_question_l10ns \"\"\"\n    df_lime_questions_names = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions_names[\"qid\"] = df_lime_questions_names[\"qid\"].astype(str)\ndf_sol = pd.merge(df_campos_2,df_lime_questions_names, how='left', on='qid')\ndf_sol = df_sol.drop(['help', 'script', 'language'], axis=1)\ndf_sol['question'] = df_sol['question'].astype(str)\ndf_sol['question'] = df_sol['question'].str.strip()\ndf_sol['question_clean'] = df_sol['question'].apply(limpiar_html)\ndf_sol['question_clean'] = df_sol['question_clean'].str.replace(r'[\\[\\]]', '', regex=True)\ndf_clean = df_sol[~df_sol['qid'].isin(['count', 'other'])].copy().reset_index(drop =True)\n</code></pre> <pre><code>C:\\Users\\Consultor_QCS\\AppData\\Local\\Temp\\ipykernel_8392\\1878959599.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(texto_html, 'html.parser')\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-de-grupos-de-encuestas-lime_groups","title":"Consulta de grupos de encuestas (<code>lime_groups</code>)","text":"<p>Se ejecuta una consulta SQL para obtener los grupos de encuestas desde la tabla <code>lime_groups</code> y su descripci\u00f3n desde la tabla <code>lime_group_l10ns</code>. La consulta selecciona el <code>gid</code>, <code>sid</code>, y el nombre del grupo (<code>group_name</code>) como <code>DetalleTabla</code>. El resultado se almacena en el dataframe <code>df_lime_groups</code>, asegurando que la columna <code>sid</code> est\u00e9 en formato <code>str</code>.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT e1.gid, e1.sid, e2.group_name as DetalleTabla FROM encuestas.lime_groups as e1\n                    LEFT JOIN encuestas.lime_group_l10ns as e2\n                    ON e1.gid= e2.gid\"\"\"\n    df_lime_groups = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_groups['sid'] = df_lime_groups['sid'].astype(str)\n</code></pre> <pre><code>df_clean = pd.merge( df_clean , df_lime_groups , how='left', on='sid')\n</code></pre> <pre><code>df_clean\n</code></pre> NombreBaseDeDatos NombreTabla NombreCampo sid qid id question question_clean gid DetalleTabla 0 encuestas lime_survey_124282 124282X29X488 124282 488 488.0 &lt;p style=\"text-align: center;\"&gt;&lt;span style=\"fo... PLANCHA N\u00b01 \\n\\n\\n\\nPRINCIPALES\\n\\n\\n\\n\\n\\n\\n\\... 29 REPRESENTANTES 1 encuestas lime_survey_161825 161825X51X1103 161825 1103 1103.0 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 2 encuestas lime_survey_161825 161825X51X1104 161825 1104 1104.0 \u00a1SU OPINION NOS INTERESA! Si desea registrar u... \u00a1SU OPINION NOS INTERESA! Si desea registrar u... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 3 encuestas lime_survey_161825 161825X51X1105SQ001 161825 1113 1113.0 LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 4 encuestas lime_survey_161825 161825X51X1105SQ002 161825 1114 1114.0 LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO ... ... ... ... ... ... ... ... ... ... ... 915 encuestas lime_survey_995137 995137X42X861SQ007 995137 877 877.0 E-mail: E-mail: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 916 encuestas lime_survey_995137 995137X42X862 995137 862 862.0 ESTADO LLAMADA: ESTADO LLAMADA: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 917 encuestas lime_survey_995137 995137X42X863 995137 863 863.0 A\u00d1O: A\u00d1O: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 918 encuestas lime_survey_995137 995137X42X864 995137 864 864.0 SEMESTRE: SEMESTRE: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 919 encuestas lime_survey_995137 995137X42X865 995137 865 865.0 MES: MES: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE <p>920 rows \u00d7 10 columns</p> <pre><code>#df_sol.to_excel('df_sol.xlsx',index=False)\n#df_clean.to_excel('df_clean.xlsx',index=False)\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#consulta-de-respuestas-de-encuestas-lime_answers","title":"Consulta de respuestas de encuestas (<code>lime_answers</code>)","text":"<p>Se ejecuta una consulta SQL para obtener las respuestas de encuestas desde la tabla <code>lime_answers</code> y sus descripciones desde la tabla <code>lime_answer_l10ns</code>. La consulta selecciona el <code>aid</code>, <code>qid</code>, <code>code</code>, y la respuesta (<code>answer</code>). El resultado de la consulta se almacena en el dataframe <code>df_lime_answers</code>, asegurando que las columnas <code>qid</code> y <code>aid</code> est\u00e9n en formato <code>str</code> para facilitar su manipulaci\u00f3n.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT a1.aid, a1.qid, a1.code, a2.answer \n                    FROM encuestas.lime_answers as a1\n                    LEFT JOIN encuestas.lime_answer_l10ns as a2\n                    ON a1.aid = a2.aid\"\"\" \n\n    df_lime_answers = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_answers['qid'] = df_lime_answers['qid'].astype(str)\ndf_lime_answers['aid'] = df_lime_answers['aid'].astype(str)\n</code></pre> <pre><code>tables_survey_names = df_clean['NombreTabla'].unique().tolist()\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#proceso-de-limpieza-y-estandarizacion-de-datos-en-encuestas","title":"Proceso de Limpieza y Estandarizaci\u00f3n de Datos en Encuestas","text":"<p>Este bloque de c\u00f3digo extrae y estandariza datos de m\u00faltiples tablas de encuestas (<code>tables_survey_names</code>), realizando operaciones de limpieza, validaci\u00f3n, y normalizaci\u00f3n de campos relevantes como identificadores y fechas. Primero, cada tabla de encuestas se carga en <code>df_survey</code> y se mapea su estructura con <code>df_clean</code> y <code>df_lime_answers</code>, combinando campos y respuestas relacionadas. Solo se conservan los campos con valores v\u00e1lidos, descartando aquellos sin relaci\u00f3n en <code>df_lime_answers</code>.</p> <p>Para cada tabla de encuestas, se renombra y estandariza los nombres de las columnas identificadoras (<code>C\u00e9dula:</code>, <code>N\u00famero Documento de Identidad</code>, etc.) a <code>documento</code> y <code>tipo_documento</code>. Luego, se utiliza la funci\u00f3n <code>melt</code> para transformar las tablas en un formato largo (<code>Pregunta</code>, <code>Respuesta</code>), facilitando su an\u00e1lisis y manipulaci\u00f3n. Adem\u00e1s, se eliminan espacios y tabulaciones no deseados en los valores de texto, reemplazando valores faltantes con <code>NaN</code>.</p> <p>Las tablas finales son agregadas en <code>tablesToConcat</code> para unificaci\u00f3n y exportaci\u00f3n, mientras que las que no cumplen con los criterios se guardan como archivos CSV en el directorio actual para su revisi\u00f3n. La columna <code>DetalleEncuesta</code> tambi\u00e9n se a\u00f1ade a cada tabla, proporcionando contexto adicional.</p> <pre><code>df_survey = dict()\ncolumnsStatic = ['C\u00e9dula:','NIT:','Tipo Documento de Identidad','N\u00famero Documento de Identidad','Tipo Documento Identidad', 'Numero de identificaci\u00f3n']\ncolumnsId = ['C\u00e9dula:', 'N\u00famero Documento de Identidad', 'Numero de identificaci\u00f3n' ]\ncolumnsTipoId = ['Tipo Documento de Identidad', 'Tipo Documento Identidad' ]\ntablesToConcat = []\n\nfor table_name in tables_survey_names:\n    print(table_name)\n    with motor_neith.begin() as conn:\n        qr_structure = \"\"\"SELECT * FROM encuestas. \"\"\" + table_name\n        df_survey[table_name] = pd.read_sql_query(sa.text(qr_structure), conn)\n\n    dfColumns = pd.DataFrame({'NombreCampo':df_survey[table_name].columns.tolist()})\n    dfColumns_p2 = pd.merge( dfColumns , df_clean , how = 'left' , on = ['NombreCampo'] )\n    dfColumns_p2 = dfColumns_p2[ ~dfColumns_p2['NombreTabla'].isna() ]\n    dfColumns_p2 = dfColumns_p2[['NombreCampo','NombreTabla','sid','qid','question_clean']]\n    dfToValidateAnswer = pd.merge( dfColumns_p2 , df_lime_answers , how = 'left' , on = ['qid'] )\n    dfToValidateAnswer = dfToValidateAnswer[~dfToValidateAnswer['aid'].isna()]\n    columnsTVA = dfToValidateAnswer['NombreCampo'].unique().tolist()\n    df_survey[table_name] = df_survey[table_name][['submitdate'] + dfColumns_p2['NombreCampo'].unique().tolist()] \n    for col in df_survey[table_name].columns.tolist():\n        if col in columnsTVA:\n            df_survey[table_name][col + '_answer'] = pd.merge( df_survey[table_name][[col]] , dfToValidateAnswer[ dfToValidateAnswer['NombreCampo'] == col ] , how = 'left' , left_on = col, right_on = 'code' )['answer']\n            df_survey[table_name] = df_survey[table_name].drop([col], axis=1)\n            df_survey[table_name] = df_survey[table_name].rename({ col + '_answer': col }, axis=1)\n        newName = dfColumns_p2[ dfColumns_p2['NombreCampo'] == col ]['question_clean'].tolist()\n\n        if len(newName) != 0:\n            df_survey[table_name] = df_survey[table_name].rename({ col: newName[0]  }, axis=1)\n\n    columnsToMantain = [x for x in df_survey[table_name].columns.tolist() if x in columnsStatic]\n    df_survey[table_name].fillna(value=np.nan, inplace=True)\n    try:\n        if len(columnsToMantain) == 0:\n            print('No aplica', os.getcwd() +'\\\\' +table_name +'.csv')\n            df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n        elif 'FECHA:' in df_survey[table_name].columns.tolist():\n            print('tiene fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\",'FECHA:'] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        else:\n            print('sin fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\"] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        for colClean in columnsToMantain:\n            df_survey[table_name][colClean] = df_survey[table_name][colClean].astype(str).apply(lambda x: x.replace(\"\\t\" , \"\" ))\n\n    except:\n        print('posible vacio')\n        df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n\n\n    df_survey[table_name] = df_survey[table_name].replace('nan', np.nan)    \n    df_survey[table_name].dropna( subset = columnsToMantain , inplace=True)\n    df_survey[table_name]['DetalleEncuesta'] = df_clean[df_clean['NombreTabla'] == table_name]['DetalleTabla'].tolist()[0] \n\n    for col in columnsId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'documento' }, axis=1)\n    for col in columnsTipoId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'tipo_documento' }, axis=1)\n</code></pre> <pre><code>lime_survey_124282\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_124282.csv\nlime_survey_161825\ntiene fecha\nlime_survey_189858\ntiene fecha\nlime_survey_267495\ntiene fecha\nlime_survey_385698\nsin fecha\nlime_survey_413658\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_413658.csv\nlime_survey_452416\ntiene fecha\nlime_survey_478847\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_478847.csv\nlime_survey_548818\ntiene fecha\nlime_survey_567215\nsin fecha\nlime_survey_576377\ntiene fecha\nlime_survey_586872\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_586872.csv\nlime_survey_599513\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_599513.csv\nlime_survey_699399\nsin fecha\nposible vacio\nlime_survey_757259\nsin fecha\nlime_survey_767954\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_767954.csv\nlime_survey_773674\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_773674.csv\nlime_survey_818378\nsin fecha\nlime_survey_832591\nsin fecha\nlime_survey_934423\ntiene fecha\nlime_survey_959511\ntiene fecha\nlime_survey_995137\ntiene fecha\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#concatenacion-final-de-encuestas","title":"Concatenaci\u00f3n final de encuestas","text":"<p>Se filtran las tablas procesadas en <code>df_survey</code> que est\u00e1n presentes en la lista <code>tablesToConcat</code> y se almacenan en el diccionario <code>df_survey_2</code>. Luego, se concatenan todas las tablas seleccionadas en un \u00fanico dataframe <code>df_survey_final</code>, combinando los resultados y asegurando un \u00edndice continuo.</p> <pre><code>df_survey_2 = dict( filter(lambda item: item[0] in tablesToConcat, df_survey.items()) )\ndf_survey_final =  pd.concat( list(df_survey_2.values()) , ignore_index = True )\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#conexion-transformacion-y-estandarizacion-de-datos-de-encuestas","title":"Conexi\u00f3n, Transformaci\u00f3n y Estandarizaci\u00f3n de Datos de Encuestas","text":"<p>Este c\u00f3digo conecta a la base de datos DWH, consulta datos de <code>BD_Dim_Datos_Fijos</code>, y realiza operaciones de limpieza y normalizaci\u00f3n en <code>df_survey_final</code>. </p> <p>Primero, se establece la conexi\u00f3n mediante <code>motor3</code> y se extrae <code>DOCUMENTO</code> y <code>CODDOC</code> de <code>BD_Dim_Datos_Fijos</code>, que luego se combina con <code>df_survey_final</code> para asignar el tipo de documento (<code>tipo_documento</code>), reemplazando valores faltantes por \"CC\". La columna <code>ID_AFILIADO</code> se crea concatenando <code>tipo_documento</code> y <code>documento</code>.</p> <p>A continuaci\u00f3n, las columnas se renombran para facilitar su identificaci\u00f3n y el campo <code>FECHA</code> se convierte al tipo <code>datetime</code>, permitiendo generar el campo <code>PERIODO</code> en formato <code>YYYYMM</code>. Las columnas se reordenan, colocando <code>ID_AFILIADO</code> al inicio para una organizaci\u00f3n clara, y finalmente se verifica el resultado en los logs.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion3 = Conexion_dwh()\nmotor3 = create_engine(cadena_conexion3)\nlogger.info('CONEXION A BASE DWH')\n\n\n# Bloque 1: Consulta y carga de datos\nlogger.info(\"Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\")\nquery = \"SELECT DOCUMENTO, CODDOC FROM BD_Dim_Datos_Fijos\"\ndf_datos_fijos = pd.read_sql(query, con=motor3)\nlogger.info(f\"Consulta a BD_Dim_Datos_Fijos completada: {df_datos_fijos.shape[0]} filas cargadas.\")\n\n# Bloque 2: Merge y limpieza de columnas\nlogger.info(\"Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\")\ndf_survey_final = pd.merge(df_survey_final, df_datos_fijos, how='left', left_on='documento', right_on='DOCUMENTO')\ndf_survey_final = df_survey_final.drop(columns=['DOCUMENTO'])\ndf_survey_final['tipo_documento'] = df_survey_final['CODDOC'].fillna('CC')\ndf_survey_final = df_survey_final.drop(columns=['CODDOC'])\n\n# Bloque 3: Generaci\u00f3n de ID y reorganizaci\u00f3n de columnas\nlogger.info(\"Generando 'ID_AFILIADO' y reorganizando columnas.\")\ndf_survey_final['ID_AFILIADO'] = df_survey_final['tipo_documento'].astype(str) + df_survey_final['documento'].astype(str)\n\n# Renombrar columnas\ndf_survey_final = df_survey_final.rename(columns={\n    'submitdate': 'FECHA_ENCUESTA',\n    'FECHA:' : 'FECHA',\n    'documento': 'DOCUMENTO',\n    'Pregunta': 'PREGUNTA',\n    'Respuesta': 'RESPUESTA',\n    'DetalleEncuesta': 'NOMBRE_ENCUESTA',\n    'NIT:': 'NIT_EMPRESA',\n    'tipo_documento': 'TIPO_DOCUMENTO',\n})\n\n# Asegurarse que FECHA es de tipo datetime\ndf_survey_final['FECHA'] = pd.to_datetime(df_survey_final['FECHA'])\n\n# Crear el campo PERIODO con el formato 'YYYYMM'\ndf_survey_final['PERIODO'] = df_survey_final['FECHA'].dt.strftime('%Y%m')\n\n\n# Reordenar las columnas colocando 'ID_AFILIADO' en la primera posici\u00f3n\ncolumnas = ['ID_AFILIADO', 'PERIODO', 'FECHA_ENCUESTA', 'FECHA', 'TIPO_DOCUMENTO','DOCUMENTO',\n       'NOMBRE_ENCUESTA','PREGUNTA', 'RESPUESTA', 'NIT_EMPRESA', ]\ndf_survey_final = df_survey_final.reindex(columns=columnas)\n\n# Mostrar las columnas finales\nlogger.info(f\"Proceso completado. Columnas finales: {df_survey_final.columns.tolist()}\")\n</code></pre> <pre><code>2024-10-30 10:27:02,768 - INFO - CONEXION A BASE DWH\n2024-10-30 10:27:02,770 - INFO - Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\n2024-10-30 10:27:08,813 - INFO - Consulta a BD_Dim_Datos_Fijos completada: 970252 filas cargadas.\n2024-10-30 10:27:08,814 - INFO - Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\n2024-10-30 10:27:09,197 - INFO - Generando 'ID_AFILIADO' y reorganizando columnas.\n2024-10-30 10:27:09,687 - INFO - Proceso completado. Columnas finales: ['ID_AFILIADO', 'PERIODO', 'FECHA_ENCUESTA', 'FECHA', 'TIPO_DOCUMENTO', 'DOCUMENTO', 'NOMBRE_ENCUESTA', 'PREGUNTA', 'RESPUESTA', 'NIT_EMPRESA']\n</code></pre> <pre><code># Convertir el campo PERIODO a datetime para poder filtrar\ndf_survey_final['PERIODO_DT'] = pd.to_datetime(df_survey_final['PERIODO'] + '01', format='%Y%m%d')\n\n# Calcular la fecha l\u00edmite de 18 meses atr\u00e1s desde la fecha actual y ajustarla al primer d\u00eda del mes siguiente\nfecha_limite = pd.Timestamp.today() - pd.DateOffset(months=18)\nprimer_dia_mes_siguiente = (fecha_limite + pd.offsets.MonthBegin(1)).normalize()\n\n# Filtrar los resultados para los \u00faltimos 18 meses a partir del campo PERIODO\ndf_filtrado = df_survey_final[df_survey_final['PERIODO_DT'] &gt;= primer_dia_mes_siguiente]\n\n# Eliminar el campo 'PERIODO_DT'\ndf_survey_final = df_filtrado.drop(columns=['PERIODO_DT','NIT_EMPRESA']).copy()\ndf_survey_final\n</code></pre> ID_AFILIADO PERIODO FECHA_ENCUESTA FECHA TIPO_DOCUMENTO DOCUMENTO NOMBRE_ENCUESTA PREGUNTA RESPUESTA 0 CC4158282 202404 2024-04-10 10:53:55 2024-04-10 CC 4158282 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA LOPEZ LUIS 1 CC79690928 202404 2024-04-10 10:57:38 2024-04-10 CC 79690928 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: ESPINOSA TORO JOSE JAVIER 2 CC1032470737 202404 2024-04-10 11:00:04 2024-04-10 CC 1032470737 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN NEIFY ESPERANZA 3 CC1073609064 202404 2024-04-10 11:03:14 2024-04-10 CC 1073609064 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MONICA 4 CC1056029010 202404 2024-04-10 11:05:07 2024-04-10 CC 1056029010 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MARTA ... ... ... ... ... ... ... ... ... ... 282363 CC1081918879 202407 2024-07-17 10:02:42 2024-07-17 CC 1081918879 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282364 CC57293976 202407 2024-07-17 10:04:07 2024-07-17 CC 57293976 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282365 CC1081915588 202407 2024-07-17 10:06:40 2024-07-17 CC 1081915588 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282366 CC1082243440 202407 2024-07-17 10:08:00 2024-07-17 CC 1082243440 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282367 CC1081916237 202407 2024-07-17 10:09:58 2024-07-17 CC 1081916237 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: sandra.perez <p>78778 rows \u00d7 9 columns</p>"},{"location":"seccion/6.1-FactEncuestas/#crear-dim-encuestas","title":"Crear Dim Encuestas","text":"<p>En esta parte se identifican las encuestas \u00fanicas y se asigna un indice que sirve como llave primaria. Luego se cruza con la df_survey_final para asignar la llave correspondiente</p> <pre><code># Encontrar valor unicos de nombres de encuestas\ndf_unique_nombre_encuesta = pd.DataFrame(df_survey_final['NOMBRE_ENCUESTA'].unique(), columns=['NOMBRE_ENCUESTA'])\n\n# Ordenar por orden alfab\u00e9tico\ndf_unique_nombre_encuesta = df_unique_nombre_encuesta.sort_values(by='NOMBRE_ENCUESTA').reset_index(drop=True)\n\n# Asignar un \u00edndice empezando desde 1\ndf_unique_nombre_encuesta['ID_ENCUESTA'] = df_unique_nombre_encuesta.index + 1\n\n# Reorganizar las columnas para tener 'Index' como la primera columna\ndf_unique_nombre_encuesta = df_unique_nombre_encuesta[['ID_ENCUESTA', 'NOMBRE_ENCUESTA']]\ndf_unique_nombre_encuesta\n</code></pre> ID_ENCUESTA NOMBRE_ENCUESTA 0 1 ENCUESTA DE SATISFACCION AFILIACION DE EMPLEAD... 1 2 ENCUESTA DE SATISFACCION DE AFILIACION DEL TRA... 2 3 ENCUESTA DE SATISFACCION ESCUELAS DEPORTIVAS 3 4 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 4 5 ENCUESTA DE SATISFACCION SERVICIO DE ALIMENTOS... 5 6 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 6 7 ENCUESTA DE SATISFACCION SERVICIO DE RECREACION 7 8 ENCUESTA DE SATISFACCION SERVICIO DE VACUNACION 8 9 ENCUESTA DE SATISFACCI\u00d3N DE VIVIENDA FOVIS <pre><code># Realizar el inner join con el DataFrame df_survey_final por el campo NOMBRE_ENCUESTA\ndf_survey_final = pd.merge(df_survey_final, df_unique_nombre_encuesta, on='NOMBRE_ENCUESTA', how='inner')\ndf_survey_final\n</code></pre> ID_AFILIADO PERIODO FECHA_ENCUESTA FECHA TIPO_DOCUMENTO DOCUMENTO NOMBRE_ENCUESTA PREGUNTA RESPUESTA ID_ENCUESTA 0 CC4158282 202404 2024-04-10 10:53:55 2024-04-10 CC 4158282 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA LOPEZ LUIS 6 1 CC79690928 202404 2024-04-10 10:57:38 2024-04-10 CC 79690928 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: ESPINOSA TORO JOSE JAVIER 6 2 CC1032470737 202404 2024-04-10 11:00:04 2024-04-10 CC 1032470737 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN NEIFY ESPERANZA 6 3 CC1073609064 202404 2024-04-10 11:03:14 2024-04-10 CC 1073609064 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MONICA 6 4 CC1056029010 202404 2024-04-10 11:05:07 2024-04-10 CC 1056029010 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MARTA 6 ... ... ... ... ... ... ... ... ... ... ... 78773 CC1081918879 202407 2024-07-17 10:02:42 2024-07-17 CC 1081918879 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78774 CC57293976 202407 2024-07-17 10:04:07 2024-07-17 CC 57293976 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78775 CC1081915588 202407 2024-07-17 10:06:40 2024-07-17 CC 1081915588 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78776 CC1082243440 202407 2024-07-17 10:08:00 2024-07-17 CC 1082243440 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78777 CC1081916237 202407 2024-07-17 10:09:58 2024-07-17 CC 1081916237 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: sandra.perez 4 <p>78778 rows \u00d7 10 columns</p>"},{"location":"seccion/6.1-FactEncuestas/#conexion-a-la-base-de-datos-dwh_1","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion2 = Conexion_dwh()\nmotor2 = create_engine(cadena_conexion2)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 10:27:09,850 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/6.1-FactEncuestas/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>df_survey_final</code> en la tabla <code>BD_Fact_Encuestas</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>guardar_en_dwh(df_survey_final, 'BD_Fact_Encuestas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:27:09,860 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:27:09,862 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:27:10,261 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Encuestas\n2024-10-30 10:27:22,427 - INFO - Tabla almacenada correctamente. 78,778 registros finales obtenidos.\n2024-10-30 10:27:22,542 - INFO - ALMACENAMIENTO ---  --- 12.68 seconds ---\n2024-10-30 10:27:22,542 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>guardar_en_dwh(df_unique_nombre_encuesta, 'BD_Dim_Encuestas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:27:22,549 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:27:22,551 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:27:22,940 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Encuestas\n2024-10-30 10:27:23,504 - INFO - Tabla almacenada correctamente. 9 registros finales obtenidos.\n2024-10-30 10:27:23,580 - INFO - ALMACENAMIENTO ---  --- 1.03 seconds ---\n2024-10-30 10:27:23,581 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:27:57,385 - INFO - FINAL ETL --- 64.46 seconds ---\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/","title":"6.2 Fact Encuesta afil empleador","text":""},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#introduccion","title":"Introducci\u00f3n","text":"<p>El proceso ETL para la tabla <code>Fact_Encuesta_afil_empleador</code> se inicia configurando el entorno de ejecuci\u00f3n mediante la importaci\u00f3n de librer\u00edas y funciones esenciales para la conexi\u00f3n a bases de datos y la manipulaci\u00f3n de datos. Los datos de encuestas, provenientes de la tabla <code>Encuesta_cuota_monetaria_PB_</code>, se extraen a trav\u00e9s de consultas SQL con m\u00faltiples uniones, permitiendo estructurar los datos de diferentes preguntas de la encuesta en un formato uniforme. Posteriormente, los datos se transforman, agregando identificadores clave, campos de validaci\u00f3n y estandarizaci\u00f3n de tipos de datos. Finalmente, los datos transformados se cargan en la tabla de destino del DWH (<code>BD_Fact_Encuesta_afil_empleador</code>) para an\u00e1lisis y consultas posteriores.</p>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL para la Tabla Fact_Encuesta_afil_empleador\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant ETL\n  participant BD_Minerva as Base de Datos Minerva\n  participant BD_DWH as Base de Datos DWH\n  \ud83d\udc64 Usuario-&gt;&gt;ETL: Solicitar procesamiento de datos de encuesta\n  ETL-&gt;&gt;BD_Minerva: Ejecutar consulta para extraer datos de encuesta\n  BD_Minerva--&gt;&gt;ETL: Datos de encuesta extra\u00eddos\n  ETL-&gt;&gt;ETL: Transformar datos (estandarizaci\u00f3n y validaci\u00f3n)\n  ETL-&gt;&gt;BD_DWH: Cargar datos transformados en `BD_Fact_Encuesta_afil_empleador`\n  BD_DWH--&gt;&gt;ETL: Confirmaci\u00f3n de carga exitosa\n  ETL--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#etl","title":"ETL","text":""},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque de c\u00f3digo configura el entorno necesario para el procesamiento de datos y la conexi\u00f3n a bases de datos. Se importan librer\u00edas como <code>sqlalchemy</code> para manejar conexiones, <code>pandas</code> para manipulaci\u00f3n de datos, y m\u00f3dulos est\u00e1ndar como <code>time</code>, <code>os</code> y <code>logging</code> para control de tiempo, manejo de rutas y registro de eventos. Tambi\u00e9n se establece el acceso al directorio <code>funciones</code> en <code>sys.path</code> para importar un conjunto de funciones personalizadas desde <code>Funciones.py</code>, que incluyen <code>convertir_columnas_mayusculas</code> para renombrar columnas, <code>guardar_en_dwh</code> y <code>cargar_tablas</code> para la carga y almacenamiento de datos en el Data Warehouse, y <code>setup_logger</code> para configurar el sistema de registro, facilitando un flujo de trabajo integral para el manejo de datos y conexi\u00f3n a bases.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time \nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import  convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Satisfaccion.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Satisfaccion.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-26 10:07:35,946 - INFO - Importacion de funciones correcta, 26-10-2024 10:07\n2024-10-26 10:07:35,948 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#consulta-sql-para-estructura-de-encuestas-de-cuota-monetaria","title":"Consulta SQL para Estructura de Encuestas de Cuota Monetaria","text":"<p>Este c\u00f3digo define una consulta SQL en el diccionario <code>qr_structure</code> que extrae y organiza datos de la tabla <code>Encuesta_cuota_monetaria_PB_</code>. La consulta utiliza <code>UNION ALL</code> para combinar varias selecciones, cada una de las cuales obtiene una pregunta distinta de la encuesta (como <code>Pregunta1</code>, <code>Pregunta2</code>, <code>Pregunta3</code>). Cada selecci\u00f3n incluye columnas como <code>C\u00c9DULA:</code>, <code>Proceso</code>, el periodo (<code>Periodo</code>), el tipo de cliente (<code>Tipo cliente</code>), la pregunta (<code>Pregunta</code>), su calificaci\u00f3n (<code>Calificacion</code>), y la fuente de datos (<code>Fuente</code>). Este dise\u00f1o permite transformar los datos de la encuesta en un formato que facilita el an\u00e1lisis y la comparaci\u00f3n de calificaciones por pregunta y periodo.</p> <pre><code>#1. Consulta de estructura\n\nqr_structure = {'query':\"\"\"\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta1' Pregunta, Pregunta1 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    union all\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta2' Pregunta, Pregunta2 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    union all\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta3' Pregunta, Pregunta3 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    -- Y as\u00ed sucesivamente con el resto de las uniones de la consulta SQL.\n\"\"\"}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure['query'])\n</code></pre> <pre><code>2024-10-26 10:07:35,959 - INFO - LECTURA DE QUERYS\n\n\n\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta1' Pregunta, Pregunta1 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    union all\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta2' Pregunta, Pregunta2 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    union all\n    select `C\u00c9DULA:`, Proceso, CONCAT(Anio, Mes) Periodo, `Tipo cliente`, 'Pregunta3' Pregunta, Pregunta3 Calificacion, 'Encuesta_cuota_monetaria_PB' Fuente\n    from Encuesta_cuota_monetaria_PB_\n    -- Y as\u00ed sucesivamente con el resto de las uniones de la consulta SQL.\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-26 10:07:35,972 - INFO - CONEXION A BASE DWH\n2024-10-26 10:07:36,557 - INFO - Cargando query \n2024-10-26 10:07:37,205 - INFO - Cargada query --- 0.65 seconds ---\n2024-10-26 10:07:37,302 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 1.33 seconds ---\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#transformaciones-del-dataframe-para-analisis-de-encuestas-de-cuota-monetaria","title":"Transformaciones del DataFrame para An\u00e1lisis de Encuestas de Cuota Monetaria","text":"<p>Este bloque aplica varias transformaciones al DataFrame <code>df</code> para estructurar y estandarizar los datos de encuestas. Primero, convierte columnas como <code>Periodo</code>, <code>Proceso</code> y <code>Tipo cliente</code> a tipo texto para garantizar consistencia en el formato. A continuaci\u00f3n, se agregan columnas clave como <code>KeyHistoricos</code>, <code>ID_AFILIADO</code>, <code>Validador_total_encuestas</code>, y <code>Fuente_pregunta</code> para facilitar la identificaci\u00f3n, validaci\u00f3n y clasificaci\u00f3n de los datos. La columna <code>Calificacion</code> se convierte a tipo entero con soporte para valores nulos, mientras que <code>C\u00c9DULA:</code> y <code>Validador_total_encuestas</code> se estandarizan como texto. Finalmente, se renombra la columna <code>Tipo cliente</code> a <code>TIPO_CLIENTE</code> utilizando <code>convertir_columnas_mayusculas</code> y se reordena el DataFrame <code>df_conv</code> para obtener una estructura consolidada y f\u00e1cil de analizar.</p> <pre><code>df = df_structure['query']\n\n# Cambiar el tipo de las columnas\ndf['Periodo'] = df['Periodo'].astype(str)\ndf['Proceso'] = df['Proceso'].astype(str)\ndf['Tipo cliente'] = df['Tipo cliente'].astype(str)\n\n\n# Agregar la columna 'KeyHistoricos'\ndf['KeyHistoricos'] = df['Periodo'] + df['C\u00c9DULA:'].astype(str)\n\n# Convertir columnas 'Calificacion' a tipo entero y 'C\u00c9DULA:' a texto\ndf['Calificacion'] = df['Calificacion'].astype('Int64')  # Usa 'Int64' para manejar valores nulos\ndf['C\u00c9DULA:'] = df['C\u00c9DULA:'].astype(str)\n\ndf['ID_AFILIADO']= 'CC' + df['C\u00c9DULA:'].astype(str)\n\n# Agregar la columna 'Validador_total_encuestas' seg\u00fan la condici\u00f3n\ndf['Validador_total_encuestas'] = df.apply(lambda row: row['C\u00c9DULA:'] + row['Proceso'] + row['Periodo'] if row['Pregunta'] == 'Pregunta1' else '', axis=1)\n\n# Convertir 'Validador_total_encuestas' a texto\ndf['Validador_total_encuestas'] = df['Validador_total_encuestas'].astype(str)\n\n# Agregar la columna 'Fuente_pregunta'\ndf['Fuente_pregunta'] = df['Fuente'] + df['Pregunta']\n\n\n# Diccionario de renombrado para las columnas\nrename_dict = {\n    'Tipo cliente': 'TIPO_CLIENTE',\n}\n\ndf_conv = convertir_columnas_mayusculas(df,rename_dict)\ndf_conv=df_conv[[\n    'ID_AFILIADO',\n    'PROCESO',\n    'PERIODO',\n    'TIPO_CLIENTE',\n    'PREGUNTA',\n    'CALIFICACION',\n    'FUENTE',\n    'KEYHISTORICOS',\n    'VALIDADOR_TOTAL_ENCUESTAS',\n    'C\u00c9DULA:',\n    'FUENTE_PREGUNTA']]\n\ndf=df_conv\n</code></pre>"},{"location":"seccion/6.2-Fact_Encuesta_afil_empleador/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_Fact_Encuesta_afil_empleador</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df, 'BD_Fact_Encuesta_afil_empleador', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-26 10:07:37,356 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-26 10:07:37,358 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-26 10:07:37,917 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Encuesta_afil_empleador\n2024-10-26 10:07:39,661 - INFO - Tabla almacenada correctamente.\n2024-10-26 10:07:39,773 - INFO - ALMACENAMIENTO ---  --- 2.42 seconds ---\n2024-10-26 10:07:39,775 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-26 10:07:39,787 - INFO - FINAL ETL --- 3.87 seconds ---\n</code></pre>"},{"location":"seccion/Bloque1/","title":"Optimizaci\u00f3n del Proceso de Datos en un Data Warehouse: Descripci\u00f3n de los Bloques 1.X","text":""},{"location":"seccion/Bloque1/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada de los distintos bloques de datos que intervienen en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). Cada bloque tiene un prop\u00f3sito espec\u00edfico orientado a mantener la integridad y disponibilidad de los datos provenientes de diversas fuentes, como beneficiarios, trabajadores y empresas. Estas actividades incluyen la importaci\u00f3n de librer\u00edas, el uso de funciones especializadas y la implementaci\u00f3n de c\u00f3digo que permite manipular la informaci\u00f3n de manera eficiente y precisa.</p>"},{"location":"seccion/Bloque1/#objetivos","title":"Objetivos","text":"<ol> <li>Proporcionar una estructura organizada y clara para la integraci\u00f3n de datos de diferentes entidades (beneficiarios, trabajadores, empresas, etc.).</li> <li>Garantizar la calidad de los datos mediante la aplicaci\u00f3n de transformaciones y validaciones apropiadas.</li> <li>Automatizar el proceso de carga al DWH utilizando librer\u00edas y funciones especializadas.</li> <li>Documentar y explicar el uso de cada uno de los bloques de datos, sus funciones y c\u00f3mo estos contribuyen al objetivo general del Data Warehouse.</li> </ol>"},{"location":"seccion/Bloque1/#descripcion-de-los-bloques-de-datos","title":"Descripci\u00f3n de los Bloques de Datos","text":""},{"location":"seccion/Bloque1/#1-dimdatosfijos-bloque-11","title":"1. DimDatosFijos (Bloque 1.1)","text":"<ul> <li>Tabla salida: <code>DimDatosFijos</code></li> <li>Descripci\u00f3n: Este bloque se utiliza para procesar informaci\u00f3n fija de las entidades, como datos est\u00e1ticos que no cambian con el tiempo. Estos datos proporcionan contexto adicional que enriquece los an\u00e1lisis dentro del DWH.</li> </ul>"},{"location":"seccion/Bloque1/#2-dimcalendariomensual-bloque-12","title":"2. DimCalendarioMensual (Bloque 1.2)","text":"<ul> <li>Tabla salida: <code>DimCalendarioMensual</code></li> <li>Descripci\u00f3n: Este bloque administra la dimensi\u00f3n de calendario mensual, esencial para los an\u00e1lisis temporales. Se generan entradas que describen cada mes, incluyendo atributos como fechas de inicio y fin, y otros indicadores \u00fatiles para el an\u00e1lisis de series temporales.</li> </ul>"},{"location":"seccion/Bloque1/#3-dimensiones-bloque-13","title":"3. Dimensiones (Bloque 1.3)","text":"<ul> <li>Tabla salida: <code>Dimensiones</code></li> <li>Descripci\u00f3n: Este bloque gestiona diversas dimensiones clave dentro del DWH, como localizaci\u00f3n y categor\u00edas. Estas dimensiones enriquecen los datos transaccionales y facilitan la realizaci\u00f3n de an\u00e1lisis multidimensionales.</li> </ul>"},{"location":"seccion/Bloque1/#4-dimensiones_xml-bloque-14","title":"4. Dimensiones_xml (Bloque 1.4)","text":"<ul> <li>Tabla salida: <code>DimensionesXML</code></li> <li>Descripci\u00f3n: Este bloque se ocupa de la gesti\u00f3n de dimensiones a partir de archivos XML. Se leen, transforman y normalizan los datos, asegurando su correcta integraci\u00f3n en el DWH para facilitar la consulta eficiente de informaci\u00f3n estructurada.</li> </ul>"},{"location":"seccion/Bloque1/#5-factdatoscontacto-bloque-15","title":"5. FactDatosContacto (Bloque 1.5)","text":"<ul> <li>Tabla salida: <code>FactDatosContacto</code></li> <li>Descripci\u00f3n: Este bloque gestiona y almacena la informaci\u00f3n de contacto de beneficiarios, trabajadores y empresas. Se asegura de que los datos est\u00e9n normalizados y sean precisos para ser utilizados en an\u00e1lisis posteriores o para la integraci\u00f3n con otros sistemas.</li> </ul>"},{"location":"seccion/Bloque1/#6-fact_historico_afiliacion_empresas-bloque-16","title":"6. Fact_Historico_Afiliacion_Empresas (Bloque 1.6)","text":"<ul> <li>Tabla salida: <code>FactHistoricoAfiliacionEmpresas</code></li> <li>Descripci\u00f3n: Este bloque procesa la informaci\u00f3n hist\u00f3rica de la afiliaci\u00f3n de empresas. Se realizan transformaciones para estandarizar los datos y validar su consistencia, asegurando la correcta integraci\u00f3n en el DWH y la vinculaci\u00f3n con otras dimensiones, como beneficiarios y trabajadores.</li> </ul>"},{"location":"seccion/Bloque1/#7-fact_historico_trabajadores2-bloque-17","title":"7. Fact_Historico_Trabajadores2 (Bloque 1.7)","text":"<ul> <li>Tabla salida: <code>FactHistoricoTrabajadores2</code></li> <li>Descripci\u00f3n: En este bloque se gestionan los datos hist\u00f3ricos de los trabajadores. Se aplican transformaciones para estandarizar la informaci\u00f3n, garantizando que los datos sean consistentes y se integren correctamente con otros bloques, mejorando as\u00ed la calidad de los an\u00e1lisis realizados en el DWH.</li> </ul>"},{"location":"seccion/Bloque1/#8-fact_historico_trabajadores1-bloque-17","title":"8. Fact_Historico_Trabajadores1 (Bloque 1.7)","text":"<ul> <li>Tabla salida: <code>FactHistoricoTrabajadores1</code></li> <li>Descripci\u00f3n: Al igual que el bloque de trabajadores 2, este bloque gestiona un conjunto adicional de datos hist\u00f3ricos de trabajadores. Las transformaciones y validaciones aseguran la consistencia y calidad de los datos antes de ser cargados en el DWH, permitiendo su uso en an\u00e1lisis longitudinales.</li> </ul>"},{"location":"seccion/Bloque1/#9-fact_historicobeneficiarios-bloque-18","title":"9. Fact_HistoricoBeneficiarios (Bloque 1.8)","text":"<ul> <li>Tabla salida: <code>FactHistoricoBeneficiarios</code></li> <li>Descripci\u00f3n: Este bloque utiliza librer\u00edas y funciones espec\u00edficas para conectarse a la base de datos y manejar los datos con Pandas y NumPy. Utiliza SQLAlchemy para establecer la conexi\u00f3n y el m\u00f3dulo <code>Funciones.py</code> para acceder a funciones personalizadas. Su principal objetivo es procesar los datos hist\u00f3ricos de los beneficiarios, transform\u00e1ndolos y carg\u00e1ndolos adecuadamente en el DWH para futuras consultas y an\u00e1lisis.</li> </ul>"},{"location":"seccion/Bloque1/#10-dim_parametricas-bloque-19","title":"10. Dim_Parametricas (Bloque 1.9)","text":"<ul> <li>Tabla salida: <code>DimParametricas</code></li> <li>Descripci\u00f3n: Este bloque se encarga de la gesti\u00f3n de datos param\u00e9tricos, que son generalmente est\u00e1ticos o semi-est\u00e1ticos, como c\u00f3digos o descripciones. Estos datos enriquecen la informaci\u00f3n transaccional, facilitando el an\u00e1lisis y la consulta dentro del DWH.</li> </ul>"},{"location":"seccion/Bloque1/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos en un Data Warehouse es esencial para garantizar la disponibilidad de informaci\u00f3n confiable y consistente. Cada bloque descrito en este documento desempe\u00f1a un papel crucial en este proceso, desde la gesti\u00f3n de datos hist\u00f3ricos de beneficiarios y trabajadores hasta la administraci\u00f3n de dimensiones param\u00e9tricas y datos fijos. La implementaci\u00f3n de funciones personalizadas y el uso de librer\u00edas especializadas optimizan el flujo de datos, minimizan errores y maximizan la eficiencia del sistema de informaci\u00f3n.</p> <p>Un DWH bien dise\u00f1ado y gestionado proporciona una base s\u00f3lida para la toma de decisiones empresariales basadas en datos precisos y consistentes, lo cual es esencial para el \u00e9xito en un entorno de negocios competitivo y din\u00e1mico.</p>"},{"location":"seccion/Bloque2/","title":"Optimizaci\u00f3n del Proceso de Datos en Data Warehouse: Descripci\u00f3n de los Bloques 2.X","text":""},{"location":"seccion/Bloque2/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada de los diferentes bloques de datos y tablas que se utilizan en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). Cada bloque tiene un prop\u00f3sito espec\u00edfico, orientado a garantizar la integridad y disponibilidad de los datos provenientes de distintas fuentes, como los aportes empresariales, las cuotas, los subsidios y la gesti\u00f3n de morosidad. Las actividades descritas incluyen la importaci\u00f3n de librer\u00edas y funciones, as\u00ed como la implementaci\u00f3n de c\u00f3digo personalizado para manejar los datos de manera eficiente y efectiva.</p>"},{"location":"seccion/Bloque2/#objetivos","title":"Objetivos","text":"<ol> <li>Describir detalladamente cada uno de los bloques involucrados en el proceso de integraci\u00f3n de datos.</li> <li>Asegurar la calidad y la consistencia de los datos mediante transformaciones y validaciones adecuadas.</li> <li>Automatizar la carga y gesti\u00f3n de datos para optimizar su integraci\u00f3n en el DWH.</li> <li>Proporcionar una estructura clara y organizada que facilite la toma de decisiones basada en datos precisos.</li> </ol>"},{"location":"seccion/Bloque2/#descripcion-de-los-bloques","title":"Descripci\u00f3n de los Bloques","text":""},{"location":"seccion/Bloque2/#1-factaportesempafiliadas-bloque-21","title":"1. FactAportesEmpAfiliadas (Bloque 2.1)","text":"<ul> <li>Tabla utilizada: <code>AportesEmpresasAfiliadas</code></li> <li>Este bloque gestiona los aportes realizados por empresas afiliadas. Para asegurar la correcta transformaci\u00f3n y carga de los datos en el DWH, se utilizan librer\u00edas como <code>SQLAlchemy</code> y <code>Pandas</code>, que permiten la conexi\u00f3n a la base de datos y la manipulaci\u00f3n de los datos. El objetivo principal de este bloque es asegurar la consistencia de la informaci\u00f3n y su integraci\u00f3n con otros bloques del DWH.</li> </ul>"},{"location":"seccion/Bloque2/#2-facthistoricoaportestotales-bloque-21","title":"2. FactHistoricoAportesTotales (Bloque 2.1)","text":"<ul> <li>Tabla utilizada: <code>HistoricoAportesTotales</code></li> <li>Este bloque trabaja con los datos hist\u00f3ricos de los aportes totales. Las transformaciones buscan homogenizar la informaci\u00f3n y asegurar su integraci\u00f3n adecuada en el DWH, lo cual permite realizar an\u00e1lisis longitudinales que son esenciales para la toma de decisiones estrat\u00e9gicas.</li> </ul>"},{"location":"seccion/Bloque2/#3-facthistoricocuota-bloque-22","title":"3. FactHistoricoCuota (Bloque 2.2)","text":"<ul> <li>Tabla utilizada: <code>HistoricoCuotas</code></li> <li>Este bloque se encarga de la gesti\u00f3n de los datos hist\u00f3ricos relacionados con las cuotas. Asegura que la informaci\u00f3n sea precisa y est\u00e9 lista para ser cargada al DWH. Las validaciones aplicadas a los datos garantizan su consistencia y facilitan el an\u00e1lisis adecuado de los aportes.</li> </ul>"},{"location":"seccion/Bloque2/#4-factestadogirocuota-bloque-22","title":"4. FactEstadoGiroCuota (Bloque 2.2)","text":"<ul> <li>Tabla utilizada: <code>EstadoGiroCuota</code></li> <li>Este bloque registra los estados de giro de las cuotas. Se aplican transformaciones y validaciones para garantizar que los datos sean precisos y est\u00e9n disponibles para su consulta en el DWH, enriqueciendo as\u00ed los an\u00e1lisis financieros.</li> </ul>"},{"location":"seccion/Bloque2/#5-factsubsidiovivienda-bloque-23","title":"5. FactSubsidioVivienda (Bloque 2.3)","text":"<ul> <li>Tabla utilizada: <code>SubsidioVivienda</code></li> <li>Este bloque gestiona la informaci\u00f3n sobre subsidios de vivienda otorgados. Se transforman los datos para asegurar su registro adecuado en el DWH, lo cual facilita el an\u00e1lisis de los beneficios y su impacto en los beneficiarios.</li> </ul>"},{"location":"seccion/Bloque2/#6-factfosfec-bloque-24","title":"6. FactFosfec (Bloque 2.4)","text":"<ul> <li>Tabla utilizada: <code>Fosfec</code></li> <li>El bloque Fosfec se encarga del procesamiento de datos relacionados con los fondos de solidaridad y los servicios de educaci\u00f3n. Las transformaciones aplicadas garantizan la coherencia y completitud de los datos antes de ser cargados en el DWH, contribuyendo al an\u00e1lisis de impacto y la gesti\u00f3n eficiente de recursos.</li> </ul>"},{"location":"seccion/Bloque2/#7-fact_historicomoraemp-bloque-25","title":"7. Fact_HistoricoMoraEmp (Bloque 2.5)","text":"<ul> <li>Tabla utilizada: <code>HistoricoMoraEmpresas</code></li> <li>Este bloque gestiona los datos hist\u00f3ricos sobre la morosidad de las empresas. Las transformaciones y validaciones aseguran la integraci\u00f3n precisa de los datos en el DWH, lo cual proporciona informaci\u00f3n valiosa para la gesti\u00f3n de riesgos y la toma de decisiones relacionadas con la recuperaci\u00f3n de cr\u00e9ditos.</li> </ul>"},{"location":"seccion/Bloque2/#8-facthistoricoans-bloque-26","title":"8. FactHistoricoANS (Bloque 2.6)","text":"<ul> <li>Tabla utilizada: <code>HistoricoANS</code></li> <li>Este bloque trabaja con los datos hist\u00f3ricos relacionados con los Acuerdos de Nivel de Servicio (ANS). Se utilizan librer\u00edas como <code>Pandas</code> y <code>SQLAlchemy</code> para la conexi\u00f3n y transformaci\u00f3n de los datos, asegurando as\u00ed su calidad y preparaci\u00f3n para el an\u00e1lisis y la toma de decisiones estrat\u00e9gicas.</li> </ul>"},{"location":"seccion/Bloque2/#9-facthistoricosubprescrito-bloque-27","title":"9. FactHistoricoSubPrescrito (Bloque 2.7)","text":"<ul> <li>Tabla utilizada: <code>HistoricoSubsidiosPrescritos</code></li> <li>Este bloque gestiona los datos de subsidios prescritos. Las transformaciones aseguran la precisi\u00f3n de los datos y su integraci\u00f3n efectiva en el DWH, permitiendo realizar an\u00e1lisis detallados sobre los subsidios y su ciclo de vida.</li> </ul>"},{"location":"seccion/Bloque2/#10-facthistoricorecobro-bloque-28","title":"10. FactHistoricoRecobro (Bloque 2.8)","text":"<ul> <li>Tabla utilizada: <code>HistoricoRecobro</code></li> <li>Este bloque procesa los datos hist\u00f3ricos de recobro. Se realizan validaciones y transformaciones para garantizar que la informaci\u00f3n sea precisa y est\u00e9 lista para an\u00e1lisis financieros, lo cual es esencial para evaluar el desempe\u00f1o en la recuperaci\u00f3n de cartera.</li> </ul>"},{"location":"seccion/Bloque2/#11-facthistoricorecuperado-bloque-29","title":"11. FactHistoricoRecuperado (Bloque 2.9)","text":"<ul> <li>Tabla utilizada: <code>HistoricoRecuperado</code></li> <li>Este bloque registra los datos hist\u00f3ricos de lo recuperado. Las transformaciones aseguran la consistencia de los datos y su disponibilidad para el an\u00e1lisis en el DWH, ayudando a medir la efectividad de los procesos de recobro.</li> </ul>"},{"location":"seccion/Bloque2/#12-factigestion_mercurio-bloque-210","title":"12. FactIgestion_Mercurio (Bloque 2.10)","text":"<ul> <li>Tabla utilizada: <code>IgestionMercurio</code></li> <li>Este bloque se enfoca en la gesti\u00f3n de los datos provenientes del sistema Mercurio. Eval\u00faa el desempe\u00f1o de la gesti\u00f3n administrativa y soporta la toma de decisiones operativas. Los datos se transforman y cargan al DWH para permitir un an\u00e1lisis detallado sobre la efectividad de las operaciones.</li> </ul>"},{"location":"seccion/Bloque2/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos hacia un Data Warehouse es fundamental para garantizar la disponibilidad de informaci\u00f3n precisa y consistente. Los bloques descritos en este documento cubren aspectos esenciales de la gesti\u00f3n de datos empresariales, que incluyen aportes, subsidios, morosidad y recobro. Cada uno de estos bloques tiene un papel importante en asegurar la calidad de la informaci\u00f3n y facilitar an\u00e1lisis detallados que apoyen la toma de decisiones estrat\u00e9gicas.</p> <p>Un DWH bien dise\u00f1ado permite una gesti\u00f3n eficiente de los datos y proporciona a la organizaci\u00f3n una ventaja significativa para tomar decisiones basadas en informaci\u00f3n s\u00f3lida y confiable.</p>"},{"location":"seccion/Bloque3/","title":"Optimizaci\u00f3n del Proceso de Datos en un Data Warehouse: Descripci\u00f3n del Bloque 3.X","text":""},{"location":"seccion/Bloque3/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada del bloque de datos que interviene en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). El bloque tiene un prop\u00f3sito espec\u00edfico orientado a mantener la integridad y disponibilidad de los datos provenientes de diversas fuentes, como las ventas de servicios. Estas actividades incluyen la importaci\u00f3n de librer\u00edas, el uso de funciones especializadas y la implementaci\u00f3n de c\u00f3digo que permite manipular la informaci\u00f3n de manera eficiente y precisa.</p>"},{"location":"seccion/Bloque3/#objetivos","title":"Objetivos","text":"<ol> <li>Proporcionar una estructura organizada y clara para la integraci\u00f3n de datos de ventas de servicios.</li> <li>Garantizar la calidad de los datos mediante la aplicaci\u00f3n de transformaciones y validaciones apropiadas.</li> <li>Automatizar el proceso de carga al DWH utilizando librer\u00edas y funciones especializadas.</li> <li>Documentar y explicar el uso del bloque de datos, sus funciones y c\u00f3mo este contribuye al objetivo general del Data Warehouse.</li> </ol>"},{"location":"seccion/Bloque3/#descripcion-del-bloque-de-datos","title":"Descripci\u00f3n del Bloque de Datos","text":""},{"location":"seccion/Bloque3/#1-facttransaccionesventas-bloque-31","title":"1. FactTransaccionesVentas (Bloque 3.1)","text":"<ul> <li>Tabla utilizada: <code>FactTransaccionesVentas</code></li> <li>Descripci\u00f3n: Este bloque se centra en la extracci\u00f3n, transformaci\u00f3n y carga de los datos de ventas de servicios. Utiliza consultas SQL para extraer informaci\u00f3n de las tablas <code>servi21</code>, <code>servi20</code> y <code>servi03</code>, combinando detalles como documento de venta, marca, fecha de venta, tipo de beneficiario, y valores de costo, servicio y subsidio. Los datos transformados se almacenan en la tabla <code>FactTransaccionesVentas</code> dentro del DWH, proporcionando un an\u00e1lisis detallado de las transacciones realizadas. Se utilizan t\u00e9cnicas de limpieza y normalizaci\u00f3n para garantizar la calidad e integridad de los datos cargados, asegurando la preparaci\u00f3n de los mismos para an\u00e1lisis futuros.</li> </ul>"},{"location":"seccion/Bloque3/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos en un Data Warehouse es esencial para garantizar la disponibilidad de informaci\u00f3n confiable y consistente. El bloque descrito en este documento desempe\u00f1a un papel crucial en este proceso, gestionando los datos de ventas de servicios. La implementaci\u00f3n de funciones personalizadas y el uso de librer\u00edas especializadas optimizan el flujo de datos, minimizan errores y maximizan la eficiencia del sistema de informaci\u00f3n.</p> <p>Un DWH bien dise\u00f1ado y gestionado proporciona una base s\u00f3lida para la toma de decisiones empresariales basadas en datos precisos y consistentes, lo cual es esencial para el \u00e9xito en un entorno de negocios competitivo y din\u00e1mico.</p>"},{"location":"seccion/Bloque4/","title":"Optimizaci\u00f3n del Proceso de Datos en un Data Warehouse: Descripci\u00f3n del Bloque 4.X","text":""},{"location":"seccion/Bloque4/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada del bloque de datos que interviene en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). El bloque tiene un prop\u00f3sito espec\u00edfico orientado a mantener la integridad y disponibilidad de los datos provenientes de diversas fuentes, como las entidades relacionadas con instituciones educativas. Estas actividades incluyen la importaci\u00f3n de librer\u00edas, el uso de funciones especializadas y la implementaci\u00f3n de c\u00f3digo que permite manipular la informaci\u00f3n de manera eficiente y precisa.</p>"},{"location":"seccion/Bloque4/#objetivos","title":"Objetivos","text":"<ol> <li>Proporcionar una estructura organizada y clara para la integraci\u00f3n de datos relacionados con estudiantes, acudientes, matr\u00edculas y grados.</li> <li>Garantizar la calidad de los datos mediante la aplicaci\u00f3n de transformaciones y validaciones apropiadas.</li> <li>Automatizar el proceso de carga al DWH utilizando librer\u00edas y funciones especializadas.</li> <li>Documentar y explicar el uso del bloque de datos, sus funciones y c\u00f3mo este contribuye al objetivo general del Data Warehouse.</li> </ol>"},{"location":"seccion/Bloque4/#descripcion-del-bloque-de-datos","title":"Descripci\u00f3n del Bloque de Datos","text":""},{"location":"seccion/Bloque4/#1-factcolegio_fijos-bloque-41","title":"1. FactColegio_fijos (Bloque 4.1)","text":"<ul> <li>Tabla utilizada: <code>FactColegio_fijos</code></li> <li>Descripci\u00f3n: Este bloque est\u00e1 orientado a la gesti\u00f3n de datos b\u00e1sicos y consistentes relacionados con estudiantes y acudientes. Utiliza consultas SQL para extraer la informaci\u00f3n personal de estudiantes (<code>colegio12</code>) y acudientes (<code>colegio13</code>). Se asegura que todos los registros se estructuren de manera uniforme y se concatenan para garantizar un \u00fanico punto de referencia para cada afiliado. La informaci\u00f3n se transforma y se limpia, eliminando duplicados y estandarizando las direcciones, lo cual facilita la integraci\u00f3n y uso de estos datos en an\u00e1lisis posteriores.</li> </ul>"},{"location":"seccion/Bloque4/#2-factcolegio-bloque-42","title":"2. FactColegio (Bloque 4.2)","text":"<ul> <li>Tabla utilizada: <code>FactColegio</code></li> <li>Descripci\u00f3n: Este bloque se centra en la extracci\u00f3n, transformaci\u00f3n y carga de los datos relacionados con estudiantes, acudientes, matr\u00edculas y grados. Utiliza consultas SQL para extraer informaci\u00f3n de las tablas <code>colegio12</code>, <code>colegio13</code>, <code>colegio15</code> y <code>colegio03</code>, combinando detalles como datos personales de los estudiantes, informaci\u00f3n de los acudientes, detalles de matr\u00edcula, y grados acad\u00e9micos. Los datos transformados se almacenan en la tabla <code>FactColegio</code> dentro del DWH, proporcionando un an\u00e1lisis detallado de las entidades educativas. Se utilizan t\u00e9cnicas de limpieza, estandarizaci\u00f3n de direcciones y normalizaci\u00f3n para garantizar la calidad e integridad de los datos cargados, asegurando la preparaci\u00f3n de los mismos para an\u00e1lisis futuros.</li> </ul>"},{"location":"seccion/Bloque4/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos en un Data Warehouse es esencial para garantizar la disponibilidad de informaci\u00f3n confiable y consistente. El bloque descrito en este documento desempe\u00f1a un papel crucial en este proceso, gestionando los datos de las instituciones educativas. La implementaci\u00f3n de funciones personalizadas y el uso de librer\u00edas especializadas optimizan el flujo de datos, minimizan errores y maximizan la eficiencia del sistema de informaci\u00f3n.</p> <p>Un DWH bien dise\u00f1ado y gestionado proporciona una base s\u00f3lida para la toma de decisiones empresariales basadas en datos precisos y consistentes, lo cual es esencial para el \u00e9xito en un entorno de negocios competitivo y din\u00e1mico.</p>"},{"location":"seccion/Bloque5/","title":"Optimizaci\u00f3n del Proceso de Datos en un Data Warehouse: Descripci\u00f3n del Bloque 5.X","text":""},{"location":"seccion/Bloque5/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada de los bloques de datos que intervienen en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). Estos bloques tienen un prop\u00f3sito espec\u00edfico orientado a mantener la integridad y disponibilidad de los datos provenientes de diversas fuentes, como las entidades relacionadas con instituciones educativas y el servicio social. Estas actividades incluyen la importaci\u00f3n de librer\u00edas, el uso de funciones especializadas y la implementaci\u00f3n de c\u00f3digo que permite manipular la informaci\u00f3n de manera eficiente y precisa.</p>"},{"location":"seccion/Bloque5/#objetivos","title":"Objetivos","text":"<ol> <li>Proporcionar una estructura organizada y clara para la integraci\u00f3n de datos relacionados con estudiantes, acudientes, matr\u00edculas, grados y servicio social.</li> <li>Garantizar la calidad de los datos mediante la aplicaci\u00f3n de transformaciones y validaciones apropiadas.</li> <li>Automatizar el proceso de carga al DWH utilizando librer\u00edas y funciones especializadas.</li> <li>Documentar y explicar el uso de los bloques de datos, sus funciones y c\u00f3mo estos contribuyen al objetivo general del Data Warehouse.</li> </ol>"},{"location":"seccion/Bloque5/#descripcion-de-los-bloques-de-datos","title":"Descripci\u00f3n de los Bloques de Datos","text":""},{"location":"seccion/Bloque5/#1-factserviciosocial_fijos-bloque-51","title":"1. FactServicioSocial_Fijos (Bloque 5.1)","text":"<ul> <li>Tabla utilizada: <code>FactServicioSocial_Fijos</code></li> <li>Descripci\u00f3n: Este bloque gestiona datos personales relacionados con el servicio social, integrando informaci\u00f3n desde dos fuentes diferentes (<code>nsijec.personas</code> y <code>saipi.personas</code>). Se extraen datos clave como la identificaci\u00f3n, nombres, apellidos, y detalles de direcci\u00f3n de las personas involucradas. Los registros de ambas fuentes se combinan y se estructuran para garantizar consistencia. Adem\u00e1s, se realiza una limpieza y estandarizaci\u00f3n de las direcciones para asegurar la calidad de los datos antes de su integraci\u00f3n al Data Warehouse.</li> </ul>"},{"location":"seccion/Bloque5/#2-factserviciosocial-bloque-52","title":"2. FactServicioSocial (Bloque 5.2)","text":"<ul> <li>Tablas utilizadas: <code>FactServicioSocial</code></li> <li>Descripci\u00f3n: Este bloque se encarga de la integraci\u00f3n de datos m\u00e1s complejos relacionados con el servicio social, incluyendo informaci\u00f3n de estudiantes, acudientes, valoraciones y registros adicionales. Utiliza consultas SQL para extraer informaci\u00f3n desde diversas tablas (<code>estudiantes</code>, <code>personas_saipi</code>, <code>acudientes</code>, <code>registros</code>, <code>valoracion</code>). Se aplican t\u00e9cnicas de limpieza y estandarizaci\u00f3n, as\u00ed como validaci\u00f3n de duplicados. Los datos se cargan en la base de datos DWH para permitir un an\u00e1lisis detallado de los servicios sociales proporcionados, mejorando as\u00ed la visibilidad y el control sobre la gesti\u00f3n de estos servicios.</li> </ul>"},{"location":"seccion/Bloque5/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos en un Data Warehouse es esencial para garantizar la disponibilidad de informaci\u00f3n confiable y consistente. Los bloques descritos en este documento desempe\u00f1an un papel crucial en este proceso, gestionando los datos de las instituciones educativas y de servicios sociales. La implementaci\u00f3n de funciones personalizadas y el uso de librer\u00edas especializadas optimizan el flujo de datos, minimizan errores y maximizan la eficiencia del sistema de informaci\u00f3n.</p> <p>Un DWH bien dise\u00f1ado y gestionado proporciona una base s\u00f3lida para la toma de decisiones empresariales basadas en datos precisos y consistentes, lo cual es esencial para el \u00e9xito en un entorno de negocios competitivo y din\u00e1mico.</p>"},{"location":"seccion/Bloque6/","title":"Optimizaci\u00f3n del Proceso de Datos en un Data Warehouse: Descripci\u00f3n del Bloque 6.X","text":""},{"location":"seccion/Bloque6/#introduccion","title":"Introducci\u00f3n","text":"<p>Este documento presenta una descripci\u00f3n detallada de los bloques de datos que intervienen en el proceso de carga, transformaci\u00f3n y almacenamiento dentro de un Data Warehouse (DWH). Estos bloques tienen un prop\u00f3sito espec\u00edfico orientado a mantener la integridad y disponibilidad de los datos provenientes de diversas fuentes, como las entidades relacionadas con instituciones educativas y el servicio social. Estas actividades incluyen la importaci\u00f3n de librer\u00edas, el uso de funciones especializadas y la implementaci\u00f3n de c\u00f3digo que permite manipular la informaci\u00f3n de manera eficiente y precisa.</p>"},{"location":"seccion/Bloque6/#objetivos","title":"Objetivos","text":"<ol> <li>Proporcionar una estructura organizada y clara para la integraci\u00f3n de datos relacionados con estudiantes, acudientes, matr\u00edculas, grados y servicio social.</li> <li>Garantizar la calidad de los datos mediante la aplicaci\u00f3n de transformaciones y validaciones apropiadas.</li> <li>Automatizar el proceso de carga al DWH utilizando librer\u00edas y funciones especializadas.</li> <li>Documentar y explicar el uso de los bloques de datos, sus funciones y c\u00f3mo estos contribuyen al objetivo general del Data Warehouse.</li> </ol>"},{"location":"seccion/Bloque6/#descripcion-de-los-bloques-de-datos","title":"Descripci\u00f3n de los Bloques de Datos","text":""},{"location":"seccion/Bloque6/#1-factencuestas-bloque-61","title":"1. FactEncuestas (Bloque 6.1)","text":"<ul> <li>Tabla utilizada: <code>FactEncuestas</code></li> <li>Descripci\u00f3n: Este bloque gestiona la carga de datos provenientes de diversas encuestas almacenadas en el sistema. Se utilizan funciones para la extracci\u00f3n de las encuestas y sus respuestas, estandarizaci\u00f3n de los datos y transformaci\u00f3n en un formato adecuado para el an\u00e1lisis. Se aplican t\u00e9cnicas de limpieza de datos, tales como la eliminaci\u00f3n de caracteres HTML y la validaci\u00f3n de duplicados, asegurando la calidad y consistencia de la informaci\u00f3n antes de su carga en el Data Warehouse.</li> </ul>"},{"location":"seccion/Bloque6/#2-fact_encuesta_afil_empleador-bloque-62","title":"2. Fact_Encuesta_afil_empleador (Bloque 6.2)","text":"<ul> <li>Tablas utilizadas: <code>Fact_Encuesta_afil_empleador</code></li> <li>Descripci\u00f3n: Este bloque se enfoca en la integraci\u00f3n de datos espec\u00edficos de encuestas realizadas a afiliados y empleadores. Se extraen datos de m\u00faltiples preguntas de encuestas relacionadas con servicios brindados, utilizando consultas SQL que unifican y transforman los datos en un formato estandarizado. Se a\u00f1aden columnas para asegurar la trazabilidad y facilitar el an\u00e1lisis, tales como identificadores \u00fanicos de afiliados y claves hist\u00f3ricas. La limpieza de los datos y la normalizaci\u00f3n aseguran que la informaci\u00f3n est\u00e9 lista para an\u00e1lisis de desempe\u00f1o y satisfacci\u00f3n, permitiendo la toma de decisiones informadas.</li> </ul>"},{"location":"seccion/Bloque6/#conclusion","title":"Conclusi\u00f3n","text":"<p>El proceso de carga y transformaci\u00f3n de datos en un Data Warehouse es esencial para garantizar la disponibilidad de informaci\u00f3n confiable y consistente. Los bloques descritos en este documento desempe\u00f1an un papel crucial en este proceso, gestionando los datos de encuestas y respuestas de afiliados y empleadores. La implementaci\u00f3n de funciones personalizadas y el uso de librer\u00edas especializadas optimizan el flujo de datos, minimizan errores y maximizan la eficiencia del sistema de informaci\u00f3n.</p> <p>Un DWH bien dise\u00f1ado y gestionado proporciona una base s\u00f3lida para la toma de decisiones empresariales basadas en datos precisos y consistentes, lo cual es esencial para el \u00e9xito en un entorno de negocios competitivo y din\u00e1mico.</p>"},{"location":"seccion/cola_documentacion/","title":"cola_documentacion","text":""},{"location":"seccion/cola_documentacion/#13-dimensiones","title":"1.3-Dimensiones","text":""},{"location":"seccion/cola_documentacion/#13-dimensiones_1","title":"1.3-Dimensiones","text":""},{"location":"seccion/cola_documentacion/#13-dimensiones_2","title":"1.3 Dimensiones","text":""},{"location":"seccion/cola_documentacion/#intoduccion","title":"Intoducci\u00f3n","text":"<p>Este proceso ETL (Extract, Transform, Load) tiene como objetivo consolidar y estructurar datos de diversas fuentes en tablas dimensionales para su almacenamiento en un Data Warehouse (DWH), facilitando el an\u00e1lisis y reporte posterior. Utilizando Python y SQL, el flujo ETL extrae datos de m\u00faltiples tablas de entrada provenientes de bases de datos como <code>subsidio</code>, <code>schoolkits</code>, <code>empresa</code>, y <code>xml4</code>, realizando transformaciones para estandarizar y mejorar la calidad de los datos. Cada tabla de entrada aporta informaci\u00f3n clave sobre personas, actividades econ\u00f3micas, zonas, sucursales y otros atributos necesarios para el an\u00e1lisis.</p> <p>Las tablas de entrada incluyen <code>sat25</code>, que define el tipo de persona; <code>subsi04</code>, que contiene informaci\u00f3n de actividades econ\u00f3micas; y <code>subsi36</code>, que proporciona estados de inactivaci\u00f3n y c\u00f3digos de afiliaci\u00f3n. Tambi\u00e9n se usan <code>SK13</code> para zonas geogr\u00e1ficas, <code>gener18</code> para c\u00f3digos de documentos, <code>xml4c085</code> con informaci\u00f3n de sucursales de empresas, y <code>gener08</code> que ofrece datos de ciudades. Adem\u00e1s, se utilizan tablas en el DWH, como <code>DimAuxEstadoAfiliacion</code> y <code>DimTipoVinculacion</code>, que proveen datos sobre estados de afiliaci\u00f3n y tipos de calidad del aportante.</p> <p>Las tablas resultantes se almacenan en el DWH bajo nombres descriptivos, tales como <code>BD_Dim_Tipo_Persona</code> para la estructura de tipos de personas, <code>BD_Dim_Actividad</code> para actividades econ\u00f3micas, <code>BD_Dim_Zona</code> para zonas geogr\u00e1ficas, <code>BD_Dim_Empresas_Sucursales</code> para detalles de sucursales, y <code>BD_Dim_Ciudades</code> para la estandarizaci\u00f3n de datos de ciudades. Estas dimensiones permiten un an\u00e1lisis organizado de datos estructurados y categorizados para facilitar los reportes y m\u00e9tricas clave.</p>"},{"location":"seccion/cola_documentacion/#etl","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL de Dimensiones\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant Script as Script ETL\n  participant DB as Bases de Datos\n  participant DWH as Data Warehouse\n\n  \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar construcci\u00f3n de dimensiones\n  Script-&gt;&gt;DB: Conectar y extraer datos de tablas de entrada &lt;br&gt; (e.g., sat25, subsi04, xml4c085, gener08)\n  DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n  Script-&gt;&gt;Script: Transformar datos (limpieza, estandarizaci\u00f3n)\n  Script-&gt;&gt;DWH: Cargar tablas de salida en DWH &lt;br&gt; (e.g., BD_Dim_Tipo_Persona, BD_Dim_Actividad, BD_Dim_Ciudades)\n  DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\n  Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este bloque de c\u00f3digo inicializa el entorno para la conexi\u00f3n y manipulaci\u00f3n de datos con SQLAlchemy y Pandas, junto con <code>numpy</code> para el manejo de datos num\u00e9ricos y <code>logging</code> para la generaci\u00f3n de registros. Se configura el <code>sys.path</code> para importar funciones personalizadas desde un archivo externo (<code>Funciones.py</code>). Entre las funciones importadas se encuentran herramientas espec\u00edficas para el flujo de datos como <code>log_tiempo</code>, <code>guardar_en_dwh</code>, <code>cargar_tablas</code>, y <code>StoreDuplicated</code>, as\u00ed como utilidades para el procesamiento y estandarizaci\u00f3n de direcciones (<code>estandarizar_direccion</code>, <code>marcar_direcciones_estandarizadas</code>). El script ejecuta <code>testfunciones()</code> para verificar que las funciones importadas est\u00e1n listas para su uso, asegurando un flujo controlado y adecuado para el procesamiento y almacenamiento de datos.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\n#from datetime import date\nstart_time = time.time()\n#from dateutil.relativedelta import relativedelta\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import log_tiempo, guardar_en_dwh, cargar_tablas, StoreDuplicated, obtener_conexion, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 14:32\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-del-logger","title":"Configuraci\u00f3n del logger","text":"<p>Se configura el logger para registrar los eventos del proceso ETL en el archivo <code>Dimensiones.log</code> con el nivel de detalle <code>INFO</code>, e inicia el registro indicando el comienzo del proceso.</p> <pre><code>logger = setup_logger(log_filename='Dimensiones.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 14:32:29,302 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#consultas-sql-para-carga-de-datos-dimensionales-en-data-warehouse","title":"Consultas SQL para Carga de Datos Dimensionales en Data Warehouse","text":"<p>Este c\u00f3digo define una serie de consultas SQL en <code>qr_structure</code> que extraen datos desde diversas tablas relacionadas con subsidios y empresas. Cada consulta selecciona columnas relevantes y realiza uniones con otras tablas para enriquecer los datos:</p> <ul> <li><code>sat25</code>, <code>subsi04</code>, y <code>subsi36</code> extraen detalles de tipo de persona, actividad econ\u00f3mica, y estados de inactivaci\u00f3n de afiliados, respectivamente.</li> <li><code>SK13</code> y <code>gener18</code> seleccionan informaci\u00f3n de zonas y c\u00f3digos de documentos.</li> <li><code>xml4c085</code> y <code>gener08</code> consolidan informaci\u00f3n sobre sucursales empresariales y ciudades.</li> </ul> <p>El diccionario <code>dim_names</code> asigna un nombre de tabla para cada consulta en el Data Warehouse, lo que facilita la organizaci\u00f3n y carga de estas tablas en <code>df_structure</code>.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"sat25\":'''select tipper as TIPPER, detalle as TIPO_PERSONA from subsidio.sat25''',\n    \"subsi04\":'''select codact as COD_ACTIVIDAD, detalle as ACTIVIDAD_ECONOMICA from subsidio.subsi04''',\n    \"subsi36\":'''select codest as COD_EST_INAC, detalle as ESTADO_INACTIVACION, tipo as ID_TIPO_AFILIADO, codsat as COD_SAT from subsidio.subsi36''',\n    \"SK13\":'''select CODIGOZONA as COD_ZONA, NOMBREZONA as ZONA from schoolkits.SK13''',\n    \"gener18\": '''select coddoc as CODDOC, detdoc as TIPO_DOCUMENTO, codssf as COD_SUPERSUBSIDIO,codgia as COD_GIASS from empresa.gener18''',\n    \"subsi02\":'''select \n        'nit' as 'NIT',\n        'digver' as 'DIGITODEVERIFICACIONDENITDELAEMPRESA',\n        'tipper' as 'INDICAELTIPODEPERSONA.N=NATURAL,J=JUR\u00cdDICA',\n        'coddoc' as 'TIPODEDOCUMENTO',\n        'razsoc' as 'RAZ\u00d3NSOCIALDELAEMPRESA',\n        'priape' as 'PRIMERAPELLIDO.PARAPERSONASNATURALES.',\n        'segape' as 'SEGUNDOAPELLIDO.PARAPERSONASNATURALES.',\n        'prinom' as 'PRIMERNOMBRE.PARAPERSONASNATURALES.',\n        'segnom' as 'SEGUNDONOMBRE.PARAPERSONASNATURALES.',\n        'sigla' as 'SIGLAEMPRESA',\n        'nomcom' as 'NOMBRECOMERCIAL',\n        'coddocreppri' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALPRINCIPAL',\n        'cedrep' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALPRINCIPAL',\n        'repleg' as 'NOMBREDELREPRESENTANTELEGALPRINCIPAL',\n        'coddocrepsup' as 'C\u00d3DIGODELDOCUMENTODELREPRESENTANTELEGALSUPLENTE',\n        'cedrepsup' as 'N\u00daMERODEDOCUMENTODEREPRESENTANTELEGALSUPLENTE',\n        'replegsup' as 'NOMBREDELREPRESENTANTELEGALSUPLENTE',\n        'jefper' as 'NOMBREDELJEFEDEPERSONAL',\n        'direccion' as 'DIRECCIONDEDELAEMPRESAENELFORMULARIODEREGISTRO',\n        'codciu' as 'C\u00d3DIGOCIIUDANE',\n        'codbar' as 'C\u00d3DIGODELBARRIO',\n        'celular' as 'N\u00daMERODECELULARDECONTACTO',\n        'telefono' as 'N\u00daMERODETEL\u00c9FONODECONTACTO',\n        'fax' as 'N\u00daMERODEFAXDECONTACTO',\n        'email' as 'CORREOELECTR\u00d3NICODECONTACTO',\n        'codzon' as 'C\u00d3DIGODELAZONADELAEMPRESA'        \n        from subsidio.subsi02''',\n    \"subsi15\":'''select * from subsidio.subsi15''',    \n    \"xml4c085\":'''select CONCAT(ID_AFILIADO, \"_\", COD_SUCURSAL) as ID_SUCURSAL,\n                    bf.*\n                    from\n                    (select \n                    CONCAT(\n                    CAST(\n                    CASE \n                    WHEN b.tipide = 1 THEN 'CC'\n                    WHEN b.tipide = 3 THEN 'RC'\n                    WHEN b.tipide = 4 THEN 'CE'\n                    WHEN b.tipide = 6 THEN 'PA'\n                    WHEN b.tipide = 7 THEN 'NI'            \n                    ELSE b.tipide\n                    END AS CHAR\n                    ),\n                    b.nit\n                    ) AS ID_AFILIADO,\n                    b.nit as NIT_SUCURSAL,\n                    c4.codsuc as COD_SUCURSAL,\n                    c4.detalle as NOMBRE_SUCURSAL,\n                    c4.direccion as DIRECCION_SUCURSAL,\n                    c1.codciu as CODIGO_CIUDAD,\n                    c4.telefono as TELEFONO_SUCURSAL,\n                    c4.fax as FAX_SUCURSAL,\n                    b.divpol as CODZON_SUCURSAL,\n                    c4.ofiafi  as COD_OFICINA,\n                    c4.nomcon as NOMBRE_CONTACTO,\n                    c4.email as EMAIL_SUCURSAL,\n                    c2.calsuc as COD_CALIDAD_SUCURSAL,\n                    b.codact as COD_ACTIVIDAD,\n                    c4.codind as COD_INDEPENDIENTES,\n                    c4.traapo as TRABAJADORES_APORTANTES,\n                    c4.valapo as VALOR_APORTES,\n                    c4.actapr as ACTA_APROBACION,\n                    c3.perafi as PERIODO_AFILIACION,\n                    c4.fecmod as ULT_FECHA_AFILIACION,\n                    IF(b.estado = 1, \"A\", \"I\") AS ESTADO_SUCURSAL,\n                    c4.resest as FECHA_RESP_ESTADO,\n                    c1.codest COD_ESTADO,\n                    c1.fecest as FECHA_ESTADO,\n                    c4.totapo as TOTAL_APORTES,\n                    c4.observacion as OBSERVACIONES\n                from xml4.xml4c085 as b\n                left join \n                    (select CONCAT(\n                            CAST(\n                            CASE \n                            WHEN coddoc = 'CC' THEN 1 \n                            WHEN coddoc = 'CE' THEN 4 \n                            WHEN coddoc = 'NI' THEN 7 \n                            WHEN coddoc = 'PA' THEN 6 \n                            WHEN coddoc = 'RC' THEN 3 \n                            ELSE coddoc \n                            END AS CHAR\n                            ),\n                            nit\n                            ) AS cod, \n                            codciu, \n                            codest, \n                            fecest\n                    from subsidio.subsi02 group by cod) as c1\n                on CONCAT(b.tipide, b.nit) = c1.cod \n                left join subsidio.subsi48 as c2\n                on b.nit = c2.nit\n                left join \n                    (select CONCAT( tipide , nit ) as cod, \n                    MIN(periodo) as perafi\n                    from xml4.xml4c085 \n                    where estado=1 \n                    group by tipide , nit) as c3\n                on CONCAT(b.tipide, b.nit) = c3.cod\n                left join subsidio.subsi48 as c4\n                on b.nit = c4.nit\n                group by b.tipide, b.nit) as bf''',\n    \"gener08\":'''SELECT * \n                FROM empresa.gener08 as GER08\n                INNER JOIN empresa.gener07 AS GER07 \n                ON substring(GER08.codciu,1,2) COLLATE latin1_spanish_ci = GER07.coddep\n                GROUP BY substring(GER08.codciu,1,2);        \n                '''\n            }\ndim_names = {\n    \"sat25\":'BD_Dim_Tipo_Persona',\n    \"subsi04\":'BD_Dim_Actividad',\n    \"subsi36\":'BD_Dim_Estados_Inactivacion',\n    \"SK13\":'BD_Dim_Zona',\n    \"gener18\":'BD_Dim_Codigo_Documento',\n    \"xml4c085\":'BD_Dim_Empresas_Sucursales',\n    \"BD_Dim_Tipo_Afiliado\":'BD_Dim_Tipo_Afiliado',\n    \"gener08\":'BD_Dim_Ciudades'\n            }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 14:32:29,312 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-y-carga-de-tablas-desde-sql","title":"Conexi\u00f3n y carga de tablas desde SQL","text":"<p>Este c\u00f3digo se conecta a la base de datos Minerva y carga las tablas especificadas en <code>qr_structure</code> utilizando SQL. Para cada tabla, el proceso de carga es registrado en el <code>logger</code>, y al final se registra el tiempo total que tom\u00f3 cargar todas las tablas desde MySQL. Esto permite monitorear la ejecuci\u00f3n y duraci\u00f3n del proceso de carga de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 14:32:29,387 - INFO - CONEXION A BASE MINERVA\n2024-10-30 14:32:29,722 - INFO - Cargando sat25 \n2024-10-30 14:32:29,753 - INFO - Cargada sat25, 2 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,754 - INFO - Cargando subsi04 \n2024-10-30 14:32:29,855 - INFO - Cargada subsi04, 526 registros finales obtenidos. --- 0.10 seconds ---\n2024-10-30 14:32:29,855 - INFO - Cargando subsi36 \n2024-10-30 14:32:29,883 - INFO - Cargada subsi36, 113 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,884 - INFO - Cargando SK13 \n2024-10-30 14:32:29,913 - INFO - Cargada SK13, 31 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,914 - INFO - Cargando gener18 \n2024-10-30 14:32:29,946 - INFO - Cargada gener18, 10 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:32:29,947 - INFO - Cargando subsi02 \n2024-10-30 14:32:35,023 - INFO - Cargada subsi02, 33,088 registros finales obtenidos. --- 5.08 seconds ---\n2024-10-30 14:32:35,024 - INFO - Cargando subsi15 \n2024-10-30 14:34:11,558 - INFO - Cargada subsi15, 433,375 registros finales obtenidos. --- 1.61 minutes ---\n2024-10-30 14:34:11,559 - INFO - Cargando xml4c085 \n2024-10-30 14:34:30,461 - INFO - Cargada xml4c085, 20,638 registros finales obtenidos. --- 18.90 seconds ---\n2024-10-30 14:34:30,462 - INFO - Cargando gener08 \n2024-10-30 14:34:30,496 - INFO - Cargada gener08, 33 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:34:30,554 - INFO - CARGUE TABLAS DESDE MYSQL --- gener08 --- 2.02 minutes ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#validador-de-campos-repetidos","title":"Validador de campos repetidos","text":"<p>Se valida la existencia de registros duplicados en cada una de las tablas cargadas en <code>df_structure</code>. Para ello, se excluye la columna <code>id</code> y se comparan las dem\u00e1s columnas. Los registros duplicados se almacenan usando la funci\u00f3n <code>StoreDuplicated</code>, y posteriormente se eliminan los duplicados del dataframe. El proceso se registra en el log para cada tabla, junto con el tiempo total de ejecuci\u00f3n del validador.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlog_tiempo(logger, f'VALIDADOR DUPLICADOS ', time.time() - validador_time)\n</code></pre> <pre><code>2024-10-30 14:34:30,576 - INFO - VALIDADOR TABLA: sat25\n2024-10-30 14:34:30,585 - INFO - VALIDADOR TABLA: subsi04\n2024-10-30 14:34:30,598 - INFO - VALIDADOR TABLA: subsi36\n2024-10-30 14:34:30,609 - INFO - VALIDADOR TABLA: SK13\n2024-10-30 14:34:30,620 - INFO - VALIDADOR TABLA: gener18\n2024-10-30 14:34:31,274 - INFO - VALIDADOR TABLA: subsi02\n2024-10-30 14:34:44,150 - INFO - VALIDADOR TABLA: subsi15\n2024-10-30 14:34:44,416 - INFO - VALIDADOR TABLA: xml4c085\n2024-10-30 14:34:44,427 - INFO - VALIDADOR TABLA: gener08\n2024-10-30 14:34:44,428 - INFO - VALIDADOR DUPLICADOS  --- 13.87 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#transformacion-de-datos","title":"Transformaci\u00f3n de datos","text":"<p>En esta fase, se realiza una limpieza de los datos en cada tabla de <code>df_structure</code>. Se transforman todas las columnas de tipo texto a may\u00fasculas, se eliminan los espacios en blanco al inicio y al final de los valores, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 14:35:42,956 - INFO - LIMPIEZA --- 58.51 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#creacion-de-tabla-de-dimension-de-tipos-de-afiliados","title":"Creaci\u00f3n de Tabla de Dimensi\u00f3n de Tipos de Afiliados","text":"<p>Este c\u00f3digo crea una tabla de dimensi\u00f3n en <code>df_structure</code> llamada <code>BD_Dim_Tipo_Afiliado</code>, que contiene los tipos \u00fanicos de afiliados obtenidos de la columna <code>ID_TIPO_AFILIADO</code> en <code>subsi36</code>. Para cada tipo de afiliado \u00fanico, asigna un <code>ID_REGISTRO</code> numerado y mapea el valor a una descripci\u00f3n (<code>Trabajadores</code>, <code>Beneficiarios</code>, <code>Empresas</code>) mediante un diccionario.</p> <p>Las columnas se ordenan como <code>ID_REGISTRO</code>, <code>ID_TIPO_AFILIADO</code> y <code>TIPO_AFILIADO</code> para mejorar la organizaci\u00f3n. Esta tabla de dimensi\u00f3n proporciona una referencia estandarizada de los tipos de afiliados para el Data Warehouse.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['subsi36']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ID_TIPO_AFILIADO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ID_TIPO_AFILIADO'])\n\n# Agregar manualmente una columna numerada como 'index'\ndf_valores_unicos['ID_REGISTRO'] = range(len(df_valores_unicos))\n\n# Agregar manualmente el tipo de afiliado\ntipo_afiliado = { 'T':'Trabajadores', 'B': 'Beneficiarios', 'E':'Empresas'}\ndf_valores_unicos['TIPO_AFILIADO'] = df_valores_unicos['ID_TIPO_AFILIADO'].map(tipo_afiliado)\n\n# Ordenar columnas\ncolumnas_orden = ['ID_REGISTRO','ID_TIPO_AFILIADO', 'TIPO_AFILIADO']\ndf_valores_unicos = df_valores_unicos.reindex(columns=columnas_orden)\n\n# Agregar el DataFrame 'df_valores_unicos' a 'df_structure' con una nueva clave\ndf_structure['BD_Dim_Tipo_Afiliado'] = df_valores_unicos\ndf_structure['BD_Dim_Tipo_Afiliado']\n</code></pre> ID_REGISTRO ID_TIPO_AFILIADO TIPO_AFILIADO 0 0 T Trabajadores 1 1 B Beneficiarios 2 2 E Empresas"},{"location":"seccion/cola_documentacion/#renombrado-de-columnas-en-la-tabla-gener08","title":"Renombrado de Columnas en la Tabla <code>gener08</code>","text":"<p>Este c\u00f3digo actualiza los nombres de columnas en el DataFrame <code>gener08</code> dentro de <code>df_structure</code> para mejorar la claridad y consistencia de los datos. Las columnas <code>codciu</code>, <code>detciu</code>, y <code>clarur</code> se renombraron a <code>CODIGO_CIUDAD</code>, <code>NOMBRE_CIUDAD</code>, y <code>TIPO_ZONA</code>, respectivamente. Esto facilita la identificaci\u00f3n de cada campo en <code>gener08</code>, estandarizando su nomenclatura antes de realizar an\u00e1lisis o cargar los datos en el Data Warehouse.</p> <pre><code># Cambiar nombre de la columna\ndf_structure['gener08'].rename(columns={\n    'codciu': 'CODIGO_CIUDAD',\n    'detciu': 'NOMBRE_CIUDAD',\n    'clarur': 'TIPO_ZONA'\n    }, inplace=True)\n</code></pre>"},{"location":"seccion/cola_documentacion/#consultas-para-dimensiones-de-estado-de-afiliacion-y-calidad-de-aportante-en-dwh","title":"Consultas para Dimensiones de Estado de Afiliaci\u00f3n y Calidad de Aportante en DWH","text":"<p>Este c\u00f3digo define consultas en <code>qr_structure_dwh</code> para cargar datos de dos dimensiones en el Data Warehouse: <code>DimAuxEstadoAfiliacion</code> y <code>DimTipoVinculacion</code>. Estas consultas extraen los estados de afiliaci\u00f3n (<code>COD_EST_AFIL</code> y <code>ESTADO_AFILIACION</code>) y las categor\u00edas de calidad del aportante (<code>COD_CALIDAD_SUCURSAL</code>, <code>CALIDAD_APORTANTE_1</code>, <code>CALIDAD_APORTANTE_2</code>, <code>CALIDAD_APORTANTE_SUBSI15</code>). Cada consulta se guarda en <code>df_structure_dwh</code> bajo nombres espec\u00edficos definidos en <code>dim_names_dwh</code>, que facilita su identificaci\u00f3n en el DWH.</p> <p>La conexi\u00f3n a la base de datos DWH se establece usando <code>motor_dwh</code>, y las tablas se cargan mediante <code>cargar_tablas</code>, permitiendo integrar estas dimensiones clave para an\u00e1lisis posteriores.</p> <pre><code>#Lista de querys DWH\nqr_structure_dwh = {\n    \"DimAuxEstadoAfiliacion\" : '''select \n        estado_old as COD_EST_AFIL, \n        estado_new as ESTADO_AFILIACION\n        from dwh.gb_DimAuxEstadoAfiliacion''',\n    \"DimTipoVinculacion\" : '''select \n        Tipo as COD_CALIDAD_SUCURSAL,\n        Agrupador1 as CALIDAD_APORTANTE_1,\n        Agrupador2 as CALIDAD_APORTANTE_2,\n        AgrupadorSubsi15 as CALIDAD_APORTANTE_SUBSI15\n        from dwh.gb_DimTipoVinculacion'''\n}\n\ndim_names_dwh = {\n    \"DimAuxEstadoAfiliacion\":'BD_Dim_Estados_Afiliacion',\n    \"DimTipoVinculacion\":'BD_Dim_Calidad_Aportante'\n}\ndf_structure_dwh = dict()\nlogger.info('LECTURA DE QUERYS DWH')\n\n#Conexion a base DWH\nmotor_dwh = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor_dwh, qr_structure_dwh, df_structure_dwh, logger)\n</code></pre> <pre><code>2024-10-30 14:35:43,014 - INFO - LECTURA DE QUERYS DWH\n2024-10-30 14:35:43,017 - INFO - CONEXION A BASE DWH\n2024-10-30 14:35:43,319 - INFO - Cargando DimAuxEstadoAfiliacion \n2024-10-30 14:35:43,346 - INFO - Cargada DimAuxEstadoAfiliacion, 4 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:35:43,348 - INFO - Cargando DimTipoVinculacion \n2024-10-30 14:35:43,378 - INFO - Cargada DimTipoVinculacion, 11 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 14:35:43,429 - INFO - CARGUE TABLAS DESDE MYSQL --- DimTipoVinculacion --- 0.41 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#estandarizacion-y-reorganizacion-de-direcciones-en-sucursales","title":"Estandarizaci\u00f3n y Reorganizaci\u00f3n de Direcciones en Sucursales","text":"<p>Este c\u00f3digo realiza una limpieza y estandarizaci\u00f3n de las direcciones en <code>df_sucursales</code> (cargado desde <code>df_structure['xml4c085']</code>), que contiene datos de sucursales de empresas. Primero, renombra <code>DIRECCION_SUCURSAL</code> a <code>DIRECCION_ORIGINAL</code> y asegura que sea de tipo texto. Luego, aplica <code>estandarizar_direccion</code> para generar una columna <code>DIRECCION_LIMPIA</code> con una versi\u00f3n normalizada de las direcciones.</p> <p>La funci\u00f3n <code>marcar_direcciones_estandarizadas</code> verifica si <code>DIRECCION_LIMPIA</code> comienza con ciertas abreviaturas estandarizadas, a\u00f1adiendo una columna <code>DIRECCION_ESTANDARIZADA</code> donde esto se cumple. Despu\u00e9s, se reorganizan las columnas en el orden especificado para mejorar la organizaci\u00f3n del DataFrame.</p> <p>Finalmente, se eliminan los registros donde <code>ID_SUCURSAL</code> es nulo, manteniendo solo las filas con un identificador de sucursal v\u00e1lido. La tabla <code>df_structure['xml4c085']</code> se actualiza con el DataFrame modificado.</p> <pre><code>df_sucursales = df_structure['xml4c085'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_sucursales.rename(columns={'DIRECCION_SUCURSAL':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_sucursales['DIRECCION_ORIGINAL'] = df_sucursales['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_sucursales['DIRECCION_LIMPIA'] = df_sucursales['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_sucursales, 'DIRECCION_LIMPIA')\n\n# Reordenamos las columnas\n\ncolumnas_sucursales = ['ID_SUCURSAL', 'ID_AFILIADO', 'NIT_SUCURSAL', 'COD_SUCURSAL',\n       'NOMBRE_SUCURSAL', 'DIRECCION_ORIGINAL', 'DIRECCION_LIMPIA','DIRECCION_ESTANDARIZADA', 'COD_CIU', 'TELEFONO_SUCURSAL',\n       'FAX_SUCURSAL', 'CODZON_SUCURSAL', 'COD_OFICINA', 'NOMBRE_CONTACTO','EMAIL_SUCURSAL', 'COD_CALIDAD_SUCURSAL', 'COD_ACTIVIDAD',\n       'COD_INDEPENDIENTES', 'TRABAJADORES_APORTANTES', 'VALOR_APORTES','ACTA_APROBACION', 'PERIODO_AFILIACION', 'ULT_FECHA_AFILIACION',\n       'ESTADO_SUCURSAL', 'FECHA_RESP_ESTADO', 'COD_ESTADO', 'FECHA_ESTADO','TOTAL_APORTES', 'OBSERVACIONES']\ndf_sucursales = df_sucursales.reindex(columns=columnas_sucursales)\n\n# Eliminar registros donde ID_SUCURSAL es nulo\ndf_sucursales = df_sucursales.dropna(subset=['ID_SUCURSAL'])\n\ndf_structure['xml4c085'] = df_sucursales.copy()\ndf_structure['xml4c085']\n</code></pre> ID_SUCURSAL ID_AFILIADO NIT_SUCURSAL COD_SUCURSAL NOMBRE_SUCURSAL DIRECCION_ORIGINAL DIRECCION_LIMPIA DIRECCION_ESTANDARIZADA COD_CIU TELEFONO_SUCURSAL ... VALOR_APORTES ACTA_APROBACION PERIODO_AFILIACION ULT_FECHA_AFILIACION ESTADO_SUCURSAL FECHA_RESP_ESTADO COD_ESTADO FECHA_ESTADO TOTAL_APORTES OBSERVACIONES 0 CC1000063770_001 CC1000063770 1000063770 001 PALOMO JIMENEZ DANIEL DAVID MZ K1 CS 14  CONCEPCION MZ K1 CS 14 CONCEPCION Si NaN 6054333300 ... 0.0 NaN 202310 NaN A NaN 53 2023-11-30 0.0 NaN 1 CC1000521027_001 CC1000521027 1000521027 001 CARDONA BIJALBA JESUS ALFONSO KM 38 LT 4  VRDA EMNDIHUACA  CARRERA TRONCAL D... KM 38 LT 4 VRDA EMNDIHUACA CR TRONCAL DEL CARIBE Si NaN 3165233112 ... 0.0 NaN 202309 NaN A NaN 53 2024-01-30 0.0 NaN 2 CC1000697509_001 CC1000697509 1000697509 001 VELEZ HERNANDEZ NICOLAS CARR MINCA CR 1 367 CA MINCA CR 1 367 Si NaN 3232274086 ... 0.0 NaN 202407 NaN A NaN NaN NaN 0.0 NaN 3 CC1001832447_001 CC1001832447 1001832447 001 DE HORTA SANCHEZ JEAN CARLOS CL 31 B  #54   -77      BARRIO OLAYA HERRERA CL 31 B 54 77 BR OLAYA HERRERA Si NaN 3123030265 ... 0.0 NaN 202408 NaN A NaN NaN NaN 0.0 NaN 4 CC1001875363_001 CC1001875363 1001875363 001 ANAYA JIMENEZ EDWIN CR 59 B  #6 A  -105 CR 59 B 6 A 105 Si NaN 3016527856 ... 0.0 NaN 202302 NaN A NaN 14 2024-05-15 0.0 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 20633 CC9877093_001 CC9877093 9877093 001 PAYARES ARIZA JAIRO LUIS CL 30   #26   -56      SAN PEDRO CL 30 26 56 SAN PEDRO Si NaN 3102934954 ... 0.0 NaN 202310 NaN A NaN NaN NaN 0.0 NaN 20634 CC98772638_001 CC98772638 98772638 001 URE?A VESGA DIEGO ANDRES CALLE 17A N? 9-80 BRR PUEBLITO CL 17A 9 80 BR PUEBLITO Si NaN 4215294 ... 0.0 NaN 201812 NaN A NaN 53 2024-04-03 0.0 MIGRACION 20635 CC9877977_001 CC9877977 9877977 001 VASQUEZ CANTILLO WDEYMER CRISTOBAL CLL 3 N\u00b0 11 -75 CLL 3 11 75 No NaN 4158431 ... 0.0 NaN NaN 2019-07-01 I NaN 53 2019-07-31 0.0 MIGRACION 20636 CC9878088_001 CC9878088 9878088 001 CANDANOZA VALENCIA YEISON ENRIQUE CR 5 #15 -25 PRADERA CR 5 15 25 PRADERA Si NaN 3022035444 ... 0.0 NaN 202209 2023-04-20 A NaN 98 2022-09-30 0.0 NaN 20637 CC9878126_001 CC9878126 9878126 001 DE LA CRUZ OROZCO JULIO CESAR MANZANA B CASA 4 MZ B CS 4 Si NaN 5555555 ... 0.0 NaN NaN 2014-09-26 I NaN 110 2019-09-10 0.0 MIGRACION <p>20615 rows \u00d7 29 columns</p>"},{"location":"seccion/cola_documentacion/#guardar-en-base-de-datos-dwh","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 14:35:44,416 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 14:35:44,418 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 14:35:44,724 - INFO - Almacenando tabla sat25 en DWH como BD_Dim_Tipo_Persona\n2024-10-30 14:35:45,488 - INFO - Tabla sat25 almacenada correctamente como BD_Dim_Tipo_Persona.\n2024-10-30 14:35:45,489 - INFO - Almacenando tabla subsi04 en DWH como BD_Dim_Actividad\n2024-10-30 14:35:46,154 - INFO - Tabla subsi04 almacenada correctamente como BD_Dim_Actividad.\n2024-10-30 14:35:46,156 - INFO - Almacenando tabla subsi36 en DWH como BD_Dim_Estados_Inactivacion\n2024-10-30 14:35:46,703 - INFO - Tabla subsi36 almacenada correctamente como BD_Dim_Estados_Inactivacion.\n2024-10-30 14:35:46,704 - INFO - Almacenando tabla SK13 en DWH como BD_Dim_Zona\n2024-10-30 14:35:47,243 - INFO - Tabla SK13 almacenada correctamente como BD_Dim_Zona.\n2024-10-30 14:35:47,244 - INFO - Almacenando tabla gener18 en DWH como BD_Dim_Codigo_Documento\n2024-10-30 14:35:47,856 - INFO - Tabla gener18 almacenada correctamente como BD_Dim_Codigo_Documento.\n2024-10-30 14:35:47,857 - WARNING - La clave subsi02 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-30 14:35:47,858 - WARNING - La clave subsi15 no est\u00e1 presente en table_names, no se guard\u00f3 en la base de datos.\n2024-10-30 14:35:47,859 - INFO - Almacenando tabla xml4c085 en DWH como BD_Dim_Empresas_Sucursales\n2024-10-30 14:35:53,588 - INFO - Tabla xml4c085 almacenada correctamente como BD_Dim_Empresas_Sucursales.\n2024-10-30 14:35:53,589 - INFO - Almacenando tabla gener08 en DWH como BD_Dim_Ciudades\n2024-10-30 14:35:54,155 - INFO - Tabla gener08 almacenada correctamente como BD_Dim_Ciudades.\n2024-10-30 14:35:54,156 - INFO - Almacenando tabla BD_Dim_Tipo_Afiliado en DWH como BD_Dim_Tipo_Afiliado\n2024-10-30 14:35:54,848 - INFO - Tabla BD_Dim_Tipo_Afiliado almacenada correctamente como BD_Dim_Tipo_Afiliado.\n2024-10-30 14:35:54,912 - INFO - ALMACENAMIENTO ---  --- 10.50 seconds ---\n2024-10-30 14:35:54,913 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>guardar_en_dwh(df_structure_dwh, dim_names_dwh, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 14:35:54,921 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 14:35:54,922 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 14:35:55,214 - INFO - Almacenando tabla DimAuxEstadoAfiliacion en DWH como BD_Dim_Estados_Afiliacion\n2024-10-30 14:35:55,756 - INFO - Tabla DimAuxEstadoAfiliacion almacenada correctamente como BD_Dim_Estados_Afiliacion.\n2024-10-30 14:35:55,757 - INFO - Almacenando tabla DimTipoVinculacion en DWH como BD_Dim_Calidad_Aportante\n2024-10-30 14:35:56,482 - INFO - Tabla DimTipoVinculacion almacenada correctamente como BD_Dim_Calidad_Aportante.\n2024-10-30 14:35:56,540 - INFO - ALMACENAMIENTO ---  --- 1.62 seconds ---\n2024-10-30 14:35:56,542 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 14:35:56,554 - INFO - FINAL ETL --- 207.26 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#16-fact_historico_afiliacion_empresas","title":"1.6-Fact_Historico_Afiliacion_Empresas","text":""},{"location":"seccion/cola_documentacion/#16-fact_historico_afiliacion_empresas_1","title":"1.6-Fact_Historico_Afiliacion_Empresas","text":""},{"location":"seccion/cola_documentacion/#16-fact-historico-afiliacion-empresas","title":"1.6 Fact Historico Afiliacion Empresas","text":""},{"location":"seccion/cola_documentacion/#introduccion","title":"Introducci\u00f3n","text":"<p>En este proceso ETL se consolidan y transforman datos relacionados con la afiliaci\u00f3n de empresas y sus empleados, para ser almacenados en una tabla de hechos dentro del Data Warehouse (DWH), denominada <code>BD_Fact_Historico_Afiliacion_Empresas</code>. Se extrajeron datos desde las tablas <code>xml4c085</code> y <code>op_EmpresasSubsi02_v2</code>, que incluyen informaci\u00f3n detallada sobre el estado de afiliaci\u00f3n de las empresas, as\u00ed como la cantidad de trabajadores, su distribuci\u00f3n por g\u00e9nero, y otros detalles relacionados con el c\u00f3digo de documento y zonas geogr\u00e1ficas. Estos datos permitieron analizar el comportamiento hist\u00f3rico de afiliaci\u00f3n, identificar periodos de ingreso y retiro, y consolidar informaci\u00f3n demogr\u00e1fica clave de las empresas afiliadas.</p> <p>La salida final, <code>BD_Fact_Historico_Afiliacion_Empresas</code>, almacena estos datos, permitiendo analizar el estado de las empresas afiliadas en diferentes periodos, el n\u00famero de empleados y la distribuci\u00f3n por g\u00e9nero. El proceso incluy\u00f3 la limpieza de datos, la estandarizaci\u00f3n de nombres de columnas, y la creaci\u00f3n de identificadores \u00fanicos para los registros de empresas y sus empleados.</p>"},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl_1","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\ntitle Diagrama de Secuencia del Proceso ETL para Afiliaci\u00f3n de Empresas\nautonumber\nparticipant \ud83d\udc64 Usuario\nparticipant Script as Script ETL\nparticipant DB as Bases de Datos\nparticipant DWH as Data Warehouse\n\nUsuario-&gt;&gt;Script: Solicitar ETL de Hist\u00f3rico de Afiliaci\u00f3n\nScript-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c085, op_EmpresasSubsi02_v2\nDB--&gt;&gt;Script: Retornar datos extra\u00eddos\nScript-&gt;&gt;Script: Limpiar, estandarizar y transformar los datos\nScript-&gt;&gt;DWH: Cargar datos en BD_Fact_Historico_Afiliacion_Empresas\nDWH--&gt;&gt;Script: Confirmaci\u00f3n de carga exitosa\nScript--&gt;&gt;Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este diagrama muestra las diferentes etapas del proceso ETL, desde la extracci\u00f3n de los datos de las bases hasta la consolidaci\u00f3n de la informaci\u00f3n en el DWH, lo cual facilita el an\u00e1lisis del comportamiento de afiliaci\u00f3n de las empresas y sus empleados.</p>"},{"location":"seccion/cola_documentacion/#etl_1","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones_1","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este script facilita el manejo y la transformaci\u00f3n de datos en una base de datos usando SQLAlchemy y Pandas, complementado con funciones personalizadas que optimizan el flujo de trabajo. Con SQLAlchemy, se gestiona la conexi\u00f3n a bases de datos mediante <code>create_engine</code>, y Pandas permite la manipulaci\u00f3n de datos en formato tabular, incluyendo transformaciones y almacenamiento. La biblioteca <code>time</code> mide la duraci\u00f3n de la ejecuci\u00f3n para monitoreo de rendimiento, mientras que <code>logging</code> permite la captura de eventos importantes y posibles errores. Desde el m\u00f3dulo <code>Funciones.py</code>, se importan funciones clave como <code>convertir_columnas_mayusculas</code> (posiblemente para estandarizar los nombres de las columnas), <code>guardar_en_dwh</code> (para almacenar datos en un Data Warehouse), <code>cargar_tablas</code> (para cargar datos desde diversas fuentes), <code>obtener_conexion</code> (para establecer una conexi\u00f3n), <code>testfunciones</code> (posiblemente para pruebas), y <code>setup_logger</code> (para configurar el sistema de logging). Este enfoque modular asegura un procesamiento de datos flexible y escalable para flujos ETL o an\u00e1lisis.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import convertir_columnas_mayusculas, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-del-logger-y-comienzo-del-proceso-etl","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>Empresas.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='Empresas.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 11:48:51,409 - INFO - Importacion de funciones correcta, 30-10-2024 11:48\n2024-10-30 11:48:51,411 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-sql-para-consolidacion-de-datos-de-empresas-y-trabajadores","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Empresas y Trabajadores","text":"<p>Esta consulta SQL extrae y combina informaci\u00f3n de empresas y sus trabajadores a partir de m\u00faltiples tablas en la base <code>xml4</code>. La consulta principal (<code>p1</code>) selecciona datos como <code>NIT</code>, <code>PERIODO</code>, <code>TIPIDE</code>, <code>ESTADO</code>, <code>CODZON</code>, <code>NOMBRE_EMPRESA</code>, y <code>CODCIU</code>, entre otros. La subconsulta interna (<code>p</code>) extrae informaci\u00f3n relacionada con el per\u00edodo de afiliaci\u00f3n y retiro (<code>perafi</code>, <code>perret</code>) y atributos de la empresa (<code>tipemp</code>, <code>tipsoc</code>, <code>nomcom</code>). La subconsulta secundaria (<code>p2</code>) agrega datos sobre el n\u00famero de trabajadores (<code>NumTrab</code>) y el desglose por g\u00e9nero (<code>NumFem</code>, <code>NumMas</code>), permitiendo una vista completa de las caracter\u00edsticas y composici\u00f3n de los trabajadores en cada per\u00edodo.</p> <p>La consulta final une <code>p1</code> con <code>p2</code> en <code>nit</code> y <code>periodo</code>, creando un conjunto de datos consolidado que permite el an\u00e1lisis detallado de empresas y sus trabajadores en per\u00edodos espec\u00edficos, con un enfoque en la segmentaci\u00f3n por g\u00e9nero y afiliaci\u00f3n.</p> <pre><code>qr_structure = {'query': '''\n    SELECT\n        p1.nit, p1.periodo, p1.tipide, p1.tipper, p1.estado, p1.divpol as codzon, p1.cod,\n        p1.nombre as calemp, p1.tipapo, p1.perafi, p1.perret, p2.NumTrab, p1.codciu, \n        p2.NumFem, p2.NumMas, p1.tipemp, p1.tipsoc, p1.nomcom, p1.razsoc, p1.codact, \n        p1.fecsis, p1.persis, p1.codest\n    FROM (\n        SELECT\n            p.nit, p.periodo, p.tipide, p.cod, p.tipper, p.estado, p.divpol,\n            p.nombre, p.tipapo, p.codciu, p.tipemp, p.tipsoc, p.nomcom, p.razsoc, \n            p.codact, p.perafi, p.perret, p.fecsis, p.persis, p.codest\n        FROM (\n            SELECT\n                b.nit, b.periodo, b.tipide, b.cod, b.tipper, b.estado, b.divpol,\n                c.perafi, a.nombre, b.tipapo, b.fecret as perret, s.codciu, \n                d.nombre as tipemp, s.tipsoc, s.nomcom, b.razsoc, b.codact, \n                s.fecsis, s.persis, s.codest\n            FROM (\n                SELECT\n                    nit, tipide, CONCAT(tipide, nit) as cod,\n                    periodo, estado, divpol, IF(tipide=7, 'J', 'N') as tipper,\n                    tipapo, razsoc, codact, tipsec,\n                    IF(estado = 2 OR estado = 4, periodo, NULL) as fecret\n                FROM xml4.xml4c085\n                #WHERE nit='891780093'\n\n                GROUP BY cod, periodo\n            ) as b\n            LEFT JOIN xml4.xml4b072 as a ON b.tipapo = a.tipapo\n            LEFT JOIN xml4.xml4b001 as d ON b.tipsec = d.tipsec\n            LEFT JOIN (\n                SELECT \n                    CONCAT(\n                        CAST(\n                            CASE\n                                WHEN coddoc = 'CC' THEN 1\n                                WHEN coddoc = 'CE' THEN 4\n                                WHEN coddoc = 'NI' THEN 7\n                                WHEN coddoc = 'PA' THEN 6\n                                WHEN coddoc = 'RC' THEN 3\n                                ELSE coddoc\n                            END AS CHAR\n                        ),\n                        nit\n                    ) AS cod, codciu, tipsoc, nomcom, fecsis, \n                    CONCAT(SUBSTRING(fecsis, 1, 4), SUBSTRING(fecsis, 6, 2)) as persis, \n                    codest\n                FROM subsidio.subsi02\n                GROUP BY cod\n            ) as s ON b.cod = s.cod\n            LEFT JOIN (\n                SELECT CONCAT(tipide, nit) as cod, MIN(periodo) as perafi \n                FROM xml4.xml4c085 \n                WHERE estado=1 \n                GROUP BY CONCAT(tipide, nit)\n            ) as c ON b.cod = c.cod\n        ) as p\n        WHERE  \n            (\n                CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n                AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n            ) OR (\n                CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n                AND perret IS NULL\n            )\n    ) as p1\n    LEFT JOIN (\n        SELECT \n            periodo, nit, COUNT(DISTINCT(CONCAT(coddoc, cedtra))) as NumTrab, \n            SUM(sexfem) as NumFem, SUM(sexmas) as NumMas\n        FROM (\n            SELECT \n                periodo, nit, coddoc, cedtra, \n                IF(tipgen = 2, 1, 0) as sexfem, \n                IF(tipgen = 1, 1, 0) as sexmas \n            FROM xml4.xml4c086 \n            GROUP BY coddoc, cedtra, nit, periodo\n        ) as tra\n        GROUP BY nit, periodo\n    ) as p2 ON p1.nit = p2.nit AND p1.periodo = p2.periodo;\n'''}\n\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:48:51,419 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-sql-para-consolidacion-de-datos-de-empresas-con-segmentacion-por-periodo","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Empresas con Segmentaci\u00f3n por Periodo","text":"<p>Esta consulta en <code>qr_structure2</code> selecciona y agrupa informaci\u00f3n de empresas en diferentes per\u00edodos, consolidando datos de estado de afiliaci\u00f3n y caracter\u00edsticas adicionales. La consulta selecciona campos como <code>periodo</code>, <code>tipide</code>, <code>tipper</code>, <code>codzon</code>, <code>codest</code>, y <code>estado</code>, adem\u00e1s de columnas derivadas como <code>seRetiro</code>, <code>seAfilio</code>, y <code>nuevo</code> que indican cambios de afiliaci\u00f3n por per\u00edodo.</p> <p>La subconsulta (<code>hh</code>) filtra los registros en un rango de 18 meses desde la fecha actual, generando indicadores sobre si la empresa se retir\u00f3 o afili\u00f3 en un per\u00edodo espec\u00edfico. La consulta final agrupa estos datos por <code>periodo</code>, <code>Tipo</code>, <code>cod</code>, y <code>estado</code>, creando una estructura resumida para an\u00e1lisis y segmentaci\u00f3n de empresas en funci\u00f3n de su estado y afiliaci\u00f3n en el Data Warehouse.</p> <pre><code>qr_structure2 = {'query':'''\n    SELECT\n        periodo, nit as id, tipide, tipper, MAX(codzon) as codzon,\n        MAX(codest) as codest, Tipo, cod, estado,\n        MAX(seRetiro) as seRetiro, MAX(seAfilio) as seAfilio,  MAX(nuevo) as nuevo,\n        MAX(calsuc) as calsuc, MAX(tipapo) as tipapo,\n        MAX(NumTrab), MAX(codciu) as codciu, MAX(tipemp) as tipemp , MAX(tipsoc) as tipsoc , \n        MAX(nomcom) as nomcom, MAX(razsoc) as razsoc,\n        MAX(NumFem) as NumFem, MAX(NumMas) as NumMas, MAX(codact) as codact, MIN(fecsis) as fecsis\n    FROM (\n        SELECT nit, tipide, tipper, estado, periodo, codzon, codest, cod, 'Empresa' as Tipo,\n            calemp as calsuc, tipapo, fecsis, perafi, persis, NumTrab, codciu, NumFem, NumMas, \n            tipemp, tipsoc, nomcom, razsoc, codact,\n            IF(perret = periodo, 1, 0) as seRetiro,\n            IF(perafi = periodo, 1, 0) as seAfilio,\n            IF(periodo = persis, 1, 0) as nuevo\n        FROM dwh.op_EmpresasSubsi02_v2 as p\n        WHERE\n            CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= DATE_FORMAT(CURDATE(), '%Y-%m-01')\n            AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= DATE_FORMAT(DATE_SUB(CURDATE(), INTERVAL 18 MONTH), '%Y-%m-01')\n    ) as hh\n    GROUP BY periodo, Tipo, cod, estado;\n'''}\n\ndf_structure2 = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:48:51,428 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-a-bases-de-datos-y-carga-de-tablas","title":"Conexi\u00f3n a Bases de Datos y Carga de Tablas","text":"<p>Este c\u00f3digo establece conexiones con dos bases de datos distintas, Minerva y DWH, usando SQLAlchemy para manejar las conexiones y un <code>logger</code> para registrar eventos importantes, como el inicio de las conexiones. Primero, se conecta a la base Minerva utilizando <code>obtener_conexion</code>, y luego carga las consultas definidas en <code>qr_structure</code> en el DataFrame <code>df_structure</code> mediante la funci\u00f3n <code>cargar_tablas</code>. Posteriormente, se realiza una conexi\u00f3n similar con la base de datos DWH, donde se ejecutan las consultas definidas en <code>qr_structure2</code> para cargar los datos en <code>df_structure2</code>. Este proceso asegura que los datos de ambas bases se procesen correctamente y permite el registro eficiente de eventos durante las operaciones.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n\n#Conexion a base Minerva\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\ncargar_tablas(motor2, qr_structure2, df_structure2, logger)\n</code></pre> <pre><code>2024-10-30 11:48:51,616 - INFO - CONEXION A BASE MINERVA\n2024-10-30 11:48:51,966 - INFO - Cargando query \n2024-10-30 11:49:49,195 - INFO - Cargada query, 565,159 registros finales obtenidos. --- 57.23 seconds ---\n2024-10-30 11:49:49,323 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 57.70 seconds ---\n2024-10-30 11:49:49,325 - INFO - CONEXION A BASE DWH\n2024-10-30 11:49:49,641 - INFO - Cargando query \n2024-10-30 11:50:02,441 - INFO - Cargada query, 199,034 registros finales obtenidos. --- 12.80 seconds ---\n2024-10-30 11:50:02,533 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 13.21 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#estandarizacion-y-creacion-de-identificadores-unicos-en-datos-de-empresas","title":"Estandarizaci\u00f3n y Creaci\u00f3n de Identificadores \u00danicos en Datos de Empresas","text":"<p>Este c\u00f3digo realiza la estandarizaci\u00f3n de nombres de columnas y la creaci\u00f3n de un identificador \u00fanico <code>ID_AFILIADO</code> en el DataFrame <code>op_sec_EmpresasSubsi02_v2</code>. Primero, utiliza un diccionario (<code>_names</code>) para renombrar las columnas clave con <code>convertir_columnas_mayusculas</code>, asegurando consistencia en los nombres y formatos.</p> <p>Luego, define una serie de condiciones (<code>conditions</code>) y valores (<code>choices</code>) para crear el prefijo de documento basado en el c\u00f3digo <code>CODIGO_DOCUMENTO</code>. La funci\u00f3n <code>np.select</code> aplica esta l\u00f3gica de selecci\u00f3n m\u00faltiple, asignando a cada tipo de documento su abreviatura correspondiente (por ejemplo, <code>CC</code> para c\u00e9dula de ciudadan\u00eda, <code>TI</code> para tarjeta de identidad). El resultado, <code>DOC_PREFIX</code>, se concatena con <code>ID</code> para formar el campo <code>ID_AFILIADO</code>, eliminando posteriormente la columna auxiliar <code>DOC_PREFIX</code>.</p> <p>Por \u00faltimo, <code>op_sec_EmpresasSubsi02_v2</code> se reorganiza para incluir columnas seleccionadas, y <code>print(df_standar['PERIODO'].unique())</code> muestra los per\u00edodos \u00fanicos en el DataFrame, permitiendo validar la consistencia de los datos a trav\u00e9s del tiempo.</p> <pre><code>op_EmpresasSubsi02_v2 = df_structure['query']\nop_sec_EmpresasSubsi02_v2 = df_structure2['query']\nop_sec_EmpresasSubsi02_v2\n\n_names = {\n    'tipide': 'CODIGO_DOCUMENTO', \n    'tipper': 'TIPO_PErSONA',  \n    'codzon': 'Codigo_Zona',             \n    'codest': 'COD_EST_INAC',           \n    'seRetiro': 'se_Retiro',             \n    'seAfilio': 'se_Afilio',       \n    'calsuc': 'Calificacion_Sucursal', \n    'codciu': 'Codigo_Ciudad',   \n    'fecsis': 'Fecha_Sistema', \n    'MAX(NumTrab)':  'NUM_TRAB',\n    'estado' : 'COD_EST_AFIL'\n}\n\ndf_standar = convertir_columnas_mayusculas(op_sec_EmpresasSubsi02_v2,_names)\nimport numpy as np\n\n# Definir las condiciones y los valores para la l\u00f3gica CASE\nconditions = [\n    df_standar['CODIGO_DOCUMENTO'] == 1,\n    df_standar['CODIGO_DOCUMENTO'] == 2,\n    df_standar['CODIGO_DOCUMENTO'] == 3,\n    df_standar['CODIGO_DOCUMENTO'] == 4,\n    df_standar['CODIGO_DOCUMENTO'] == 5,\n    df_standar['CODIGO_DOCUMENTO'] == 6,\n    df_standar['CODIGO_DOCUMENTO'] == 7,\n    df_standar['CODIGO_DOCUMENTO'] == 8,\n    df_standar['CODIGO_DOCUMENTO'] == 9,\n    df_standar['CODIGO_DOCUMENTO'] == 15\n]\n\n# Valores correspondientes a cada condici\u00f3n\nchoices = ['CC', 'TI', 'RC', 'CE', 'NP', 'PA', 'NI', 'CD', 'PE', 'PT']\n\n# Aplicar las condiciones con np.select y crear el prefijo correspondiente\ndf_standar['DOC_PREFIX'] = np.select(conditions, choices, default=df_standar['CODIGO_DOCUMENTO'].astype(str))\n\n# Concatenar el prefijo y cedtra para crear la columna ID_AFILIADO\ndf_standar['ID_AFILIADO'] = df_standar['DOC_PREFIX'] + df_standar['ID'].astype(str)\n\n# Eliminar la columna auxiliar DOC_PREFIX si no se necesita\ndf_standar = df_standar.drop(columns=['DOC_PREFIX'])\n\nop_sec_EmpresasSubsi02_v2 = df_standar[[\n    'PERIODO',\n    'ID_AFILIADO',\n    'ID',\n    'CODIGO_DOCUMENTO',\n    'TIPO_PERSONA',\n    'CODIGO_ZONA',\n    'COD_EST_INAC',\n    'TIPO',\n    'COD',\n    'COD_EST_AFIL',\n    'SE_RETIRO',\n    'SE_AFILIO',\n    'NUEVO',\n    'CALIFICACION_SUCURSAL',\n    'TIPAPO',\n    'NUM_TRAB',\n    'CODIGO_CIUDAD',\n    'TIPEMP',\n    'TIPSOC',\n    'NOMCOM',\n    'RAZSOC',\n    'NUMFEM',\n    'NUMMAS',\n    'CODACT',\n    'FECHA_SISTEMA'\n    ]]\n\nprint(df_standar['PERIODO'].unique())\n</code></pre> <pre><code>['202310' '202311' '202312' '202401' '202402' '202403' '202404' '202309'\n '202407' '202408' '202409' '202304' '202305' '202306' '202307' '202308'\n '202405' '202406']\n</code></pre>"},{"location":"seccion/cola_documentacion/#almacenamiento-de-datos-en-el-data-warehouse","title":"Almacenamiento de Datos en el Data Warehouse","text":"<p>En esta \u00faltima etapa, el c\u00f3digo utiliza la funci\u00f3n <code>guardar_en_dwh</code> para almacenar los DataFrames <code>op_EmpresasSubsi02_v2</code> y <code>op_sec_EmpresasSubsi02_v2</code> en el Data Warehouse. La primera llamada guarda <code>op_EmpresasSubsi02_v2</code> en una tabla con el mismo nombre en el DWH, y la segunda llamada almacena <code>op_sec_EmpresasSubsi02_v2</code> en la tabla <code>BD_Fact_Historico_Afiliacion_Empresas</code>. Los par\u00e1metros <code>multiple=False</code> y <code>if_exists='replace'</code> indican que las tablas se reemplazar\u00e1n si ya existen y que no se manejar\u00e1n datos en m\u00faltiples bloques, optimizando el proceso de inserci\u00f3n. Esta operaci\u00f3n asegura que la informaci\u00f3n est\u00e9 disponible y actualizada para futuras consultas o an\u00e1lisis.</p> <pre><code>guardar_en_dwh(op_EmpresasSubsi02_v2, 'op_EmpresasSubsi02_v2', logger, multiple=False, if_exists='replace')\nguardar_en_dwh(op_sec_EmpresasSubsi02_v2, 'BD_Fact_Historico_Afiliacion_Empresas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 11:50:03,089 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:50:03,091 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:50:03,388 - INFO - Almacenando tabla \u00fanica en DWH como op_EmpresasSubsi02_v2\n2024-10-30 11:51:50,695 - INFO - Tabla almacenada correctamente. 565,159 registros finales obtenidos.\n2024-10-30 11:51:51,500 - INFO - ALMACENAMIENTO ---  --- 1.81 minutes ---\n2024-10-30 11:51:51,501 - INFO - Finalizando proceso de almacenamiento en DWH.\n2024-10-30 11:51:51,503 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:51:51,506 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:51:51,839 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Historico_Afiliacion_Empresas\n2024-10-30 11:52:26,714 - INFO - Tabla almacenada correctamente. 199,034 registros finales obtenidos.\n2024-10-30 11:52:26,925 - INFO - ALMACENAMIENTO ---  --- 35.42 seconds ---\n2024-10-30 11:52:26,927 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>op_sec_EmpresasSubsi02_v2['PERIODO'].unique()\n</code></pre> <pre><code>array(['202310', '202311', '202312', '202401', '202402', '202403',\n       '202404', '202309', '202407', '202408', '202409', '202304',\n       '202305', '202306', '202307', '202308', '202405', '202406'],\n      dtype=object)\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 11:52:26,991 - INFO - FINAL ETL --- 215.59 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#18-fact_historicobeneficiarios","title":"1.8-Fact_HistoricoBeneficiarios","text":""},{"location":"seccion/cola_documentacion/#18-fact_historicobeneficiarios_1","title":"1.8-Fact_HistoricoBeneficiarios","text":""},{"location":"seccion/cola_documentacion/#18-fact-historico-beneficiarios","title":"1.8 Fact Historico Beneficiarios","text":""},{"location":"seccion/cola_documentacion/#introduccion_1","title":"Introducci\u00f3n","text":"<p>En el proceso ETL de la tabla <code>Fact_HistoricoBeneficiarios</code>, se extraen y transforman datos clave sobre beneficiarios desde el sistema de subsidios y se almacenan en un Data Warehouse (DWH). La estructura de entrada est\u00e1 conformada principalmente por datos detallados de afiliaci\u00f3n y retiro, as\u00ed como estados de beneficiarios de diferentes tablas de origen. Las consultas SQL procesan informaci\u00f3n de afiliados para agregar valores como categor\u00eda, estado y c\u00f3digos de afiliaci\u00f3n, entre otros, optimizando la informaci\u00f3n para su an\u00e1lisis a lo largo del tiempo.</p> <p>La salida se estructura en dos tablas de hechos en el DWH: <code>BD_Fact_Beneficiarios_p1</code> y <code>BD_Fact_Beneficiarios_p2</code>. Estas tablas consolidan datos hist\u00f3ricos de los beneficiarios, permitiendo una visibilidad completa sobre el comportamiento de la afiliaci\u00f3n, incluyendo el estado de retiro o continuidad de los beneficiarios y la categorizaci\u00f3n en niveles de educaci\u00f3n y ciudad, entre otros aspectos clave.</p>"},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl_2","title":"Diagrama de Secuencia del Proceso ETL","text":"<p>El diagrama a continuaci\u00f3n resume el flujo de datos desde la extracci\u00f3n y transformaci\u00f3n de los beneficiarios hasta su consolidaci\u00f3n en el DWH:</p> <pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL para Fact Hist\u00f3rico Beneficiarios\n    autonumber\n    participant \ud83d\udc64 Usuario\n    participant Script as Script ETL\n    participant DB as Bases de Datos\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario-&gt;&gt;Script: Solicitar ETL de Fact Hist\u00f3rico Beneficiarios\n    Script-&gt;&gt;DB: Conectar y extraer datos de &lt;br&gt; xml4c087, subsi22, BD_Fact_Beneficiarios_p1\n    DB--&gt;&gt;Script: Retornar datos extra\u00eddos\n    Script-&gt;&gt;Script: Procesar y transformar datos\n    Script-&gt;DWH: Cargar datos en BD_Fact_Beneficiarios_p1\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga p1 exitosa\n    Script-&gt;DWH: Cargar datos en BD_Fact_Beneficiarios_p2\n    DWH--&gt;&gt;Script: Confirmaci\u00f3n de carga p2 exitosa\n    Script--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre> <p>Este flujo ilustra la secuencia de transformaci\u00f3n y carga en el DWH, donde las tablas de salida <code>BD_Fact_Beneficiarios_p1</code> y <code>BD_Fact_Beneficiarios_p2</code> contienen el registro hist\u00f3rico de afiliaci\u00f3n y retiro de beneficiarios, permitiendo un an\u00e1lisis detallado en m\u00faltiples dimensiones.</p>"},{"location":"seccion/cola_documentacion/#etl_2","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones_2","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo realiza las siguientes tareas principales: establece la conexi\u00f3n a una base de datos mediante SQLAlchemy, importa m\u00f3dulos y funciones espec\u00edficas para manipular y transformar datos en Pandas y NumPy, y ejecuta una serie de funciones personalizadas para cargar, transformar y guardar los datos en un Data Warehouse (DWH).</p> <p>La importaci\u00f3n del m\u00f3dulo <code>Funciones.py</code> incluye funciones que probablemente se encargan de conectar a la base de datos (<code>obtener_conexion</code>), transformar datos (<code>convertir_columnas_mayusculas</code>), y manejar el guardado en el DWH (<code>guardar_en_dwh</code>). Adem\u00e1s, el logger (<code>setup_logger</code>) ayuda a monitorear la ejecuci\u00f3n y errores del script, y <code>testfunciones</code> podr\u00eda verificar el estado de estas funciones o probar su correcto funcionamiento.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport logging\nstart_time = time.time()\n\n#Import de modulo funciones\nimport sys\nimport os\n\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n\n# Importa las funciones desde el archivo Funciones.py\n\nfrom Funciones import guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger, convertir_columnas_mayusculas\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-del-logger-y-comienzo-del-proceso-etl_1","title":"Configuraci\u00f3n del logger y comienzo del proceso ETL","text":"<p>En esta celda se configura el logger, que se utiliza para registrar mensajes de informaci\u00f3n (logging). El logger est\u00e1 configurado para guardar un archivo log con el nombre <code>nombre.log</code>, y en esta etapa, se inicia el proceso ETL, registrando el comienzo del mismo.</p> <pre><code>#---------------------------------------------\nlogger = setup_logger(log_filename='nombre.log', log_level=logging.INFO)  # Cambiado a logging.INFO\nlogger.info(testfunciones())\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 11:08:56,051 - INFO - Importacion de funciones correcta, 30-10-2024 11:08\n2024-10-30 11:08:56,053 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-sql-para-extraccion-de-datos-de-beneficiarios-y-periodos-de-afiliacion","title":"Consulta SQL para Extracci\u00f3n de Datos de Beneficiarios y Per\u00edodos de Afiliaci\u00f3n","text":"<p>Esta consulta SQL selecciona y consolida datos de beneficiarios y sus per\u00edodos de afiliaci\u00f3n en funci\u00f3n de m\u00faltiples uniones entre tablas (<code>xml4c087</code>, <code>subsi22</code>, <code>subsi23</code>, <code>subsi15</code>). La consulta construye un identificador <code>docben</code> a partir de los campos <code>coddocben</code> y <code>documento</code>, y calcula <code>persis</code>, que representa el a\u00f1o y mes del campo <code>fecsis</code>.</p> <p>Se extraen campos como <code>codben</code>, <code>periodo</code>, <code>estado</code>, <code>sexo</code>, <code>nivedu</code>, y <code>codzon</code>, as\u00ed como indicadores de afiliaci\u00f3n (<code>perafi</code>) y retiro (<code>perret</code>). El filtro final asegura que los resultados se encuentren en un rango de 18 meses desde la fecha actual, permitiendo analizar los beneficiarios activos y el historial de afiliaciones o retiros recientes.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"query\": \"\"\"\n\nSELECT * FROM(\n    SELECT p2.*, CONCAT(p2.coddocben,p2.documento) as docben, CONCAT( SUBSTRING(p2.fecsis,1,4) , SUBSTRING(p2.fecsis,6,2) ) as persis, p3.codciu \n FROM  (SELECT p1.*, d5.fecsis   \n FROM  (SELECT d2.codben, periodo, 'A' as estado, d1.codtra as coddoc, d1.cedtra, d2.codest, d1.documento, d1.coddoc as coddocben, d1.tipgen as sexo, d1.codigo_dicap as captra, d2.nivedu, d1.fecnac, d1.parent, d1.codcat, d1.divpol AS codzon,\n                                d4.perafi, d3.perret, d5.nombre as calsuc, d1.tipafi, SUM(d1.numcuo) as numcuo, SUM(d1.subliq) as subliq\n                                FROM\n                                xml4.xml4c087 as d1 \n        LEFT JOIN (SELECT documento, codben, codest, nivedu\nfrom subsidio.subsi22\ngroup by documento) as d2\n\n                                ON d1.documento=d2.documento\n                                LEFT JOIN (select coddoc, documento,\n                                IF(MAX(periodo)&lt;(SELECT MAX(periodo) from xml4.xml4c087), MAX(periodo), NULL) as perret from xml4.xml4c087 GROUP BY coddoc, documento) as d3\nON d1.coddoc = d3.coddoc\nAND d1.documento = d3.documento\nLEFT JOIN (select coddoc, documento,\nMIN(periodo) as perafi from xml4.xml4c087 GROUP BY coddoc, documento) as d4\nON d1.coddoc = d4.coddoc\nAND d1.documento = d4.documento\n\nLEFT JOIN xml4.xml4b006 as d5\nON d1.tipafi = d5.tipafi\n\n#where d1.documento in('1085104776', '1084473954', '1084473707', '1104383175')\n\n                                GROUP BY \n    d1.documento, d1.coddoc, periodo) as p1\n     LEFT JOIN\n                                subsidio.subsi23 as d5\n                                ON p1.codben=d5.codben) as p2\n                                LEFT JOIN subsidio.subsi15 p3\n    ON p2.cedtra = p3.cedtra\n    AND p2.coddoc = p3.coddoc\n\n    WHERE  (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND CONCAT(SUBSTRING(perret, 1, 4), '-', SUBSTRING(perret, 5, 2), '-01') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR))\n        OR (CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND perret IS NULL)) as m\n WHERE CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &lt;= CURRENT_DATE()\n        AND CONCAT(SUBSTRING(periodo, 1, 4), '-', SUBSTRING(periodo, 5, 2), '-01') &gt;= cast(DATE_SUB(CURRENT_DATE(), INTERVAL 18 MONTH) as CHAR);    \n        \"\"\"\n}\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 11:08:56,060 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-sql-para-consolidacion-de-datos-de-beneficiarios-por-periodo","title":"Consulta SQL para Consolidaci\u00f3n de Datos de Beneficiarios por Per\u00edodo","text":"<p>Esta consulta en <code>qr_structure2</code> agrupa y consolida informaci\u00f3n de beneficiarios en el Data Warehouse para cada per\u00edodo (<code>PERIODO</code>). Se seleccionan datos clave, como <code>ID_AFILIADO</code>, <code>CODIGO_DOCUMENTO</code>, <code>CODIGO_BENEFICIARIO</code>, <code>CODIGO_ZONA</code>, <code>CODIGO_ESTADO</code>, y <code>SEXO</code>, junto con informaci\u00f3n de afiliaci\u00f3n y retiros (<code>SE_RETIRO</code>, <code>SE_AFILIO</code>). Indicadores adicionales, como <code>NUEVO</code> (si el beneficiario fue registrado en el sistema en el mismo per\u00edodo) y atributos personales (<code>CAPACIDAD_TRABAJO</code>, <code>NIVEL_EDUCATIVO</code>, <code>FECHA_NACIMIENTO</code>), tambi\u00e9n se incluyen.</p> <p>Los datos se agrupan para obtener valores \u00fanicos mediante la funci\u00f3n <code>MAX</code> en cada campo, y se establece un grupo por <code>PERIODO</code>, <code>TIPO</code>, <code>COD</code>, y <code>ESTADO</code>, generando una vista resumida de beneficiarios en el Data Warehouse. Esta estructura facilita el an\u00e1lisis de cambios en la afiliaci\u00f3n, retiros, y caracter\u00edsticas generales de la poblaci\u00f3n de beneficiarios.</p> <pre><code># Lista de consultas actualizada\nqr_structure2 = {\n    \"query\": \n    \"\"\"\n        SELECT PERIODO, \n               ID_AFILIADO,\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \n               MAX(ID) as ID, \n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \n               MAX(CODCAT) as CODIGO_CATEGORIA, \n               MAX(CODZON) as CODIGO_ZONA, \n               MAX(CODEST) as CODIGO_ESTADO, \n               TIPO, \n               COD, \n               ESTADO,\n               MAX(SE_RETIRO) as SE_RETIRO, \n               MAX(SE_AFILIO) as SE_AFILIO,  \n               MAX(NUEVO) as NUEVO, \n               MAX(SEXO) as SEXO, \n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \n               MAX(FECNAC) as FECHA_NACIMIENTO, \n               MAX(PARENT) as PARENTESCO, \n               MAX(CODCIU) as CODIGO_CIUDAD, \n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \n               MAX(TIPAFI) as TIPO_AFILIACION, \n               MAX(NUMCUO) as NUMERO_CUOTAS, \n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \n        FROM (\n            SELECT \n                   ID_AFILIADO,\n                   CODIGO_DOCUMENTO, \n                   DOCUMENTO as ID, \n                   CODIGO_BENEFICIARIO as CODBEN, \n                   ESTADO, \n                   PERIODO, \n                   CODIGO_ZONA as CODZON, \n                   CODIGO_ESTADO as CODEST, \n                   DOCUMENTO_BENEFICIARIO as COD, \n                   CODIGO_CATEGORIA as CODCAT, \n                   SEXO,\n                   CAPACIDAD_TRABAJO as CAPTRA,\n                   NIVEL_EDUCACION as NIVEL_EDU, \n                   FECHA_NACIMIENTO as FECNAC, \n                   PARENTESCO as PARENT, \n                   CODIGO_CIUDAD as CODCIU, \n                   'Beneficiario' as TIPO, \n                   CALIFICACION_SUCURSAL as CALSUC, \n                   TIPO_AFILIACION as TIPAFI, \n                   NUMERO_CUOTAS as NUMCUO, \n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\n        ) as h\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\n    \"\"\"\n}\n\ndf_structure2 = dict()\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-y-cargue-de-tablas-desde-sql-con-registro-de-duracion","title":"Conexi\u00f3n y cargue de tablas desde SQL con registro de duraci\u00f3n","text":"<p>Este bloque se conecta a la base de datos Minerva, carga las tablas definidas en <code>qr_structure</code>, y utiliza el <code>logger</code> para registrar tanto el tiempo individual de carga de cada tabla como el tiempo total del proceso. Esto proporciona visibilidad sobre el rendimiento de cada cargue y del proceso general, facilitando el monitoreo y la optimizaci\u00f3n del flujo de datos.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE ')\n\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 11:08:56,130 - INFO - CONEXION A BASE \n2024-10-30 11:08:56,427 - INFO - Cargando query \n2024-10-30 11:17:36,688 - INFO - Cargada query, 3,829,359 registros finales obtenidos. --- 8.67 minutes ---\n2024-10-30 11:17:37,068 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 8.68 minutes ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-codigos-de-documentos-y-conexion-a-base-de-datos-dwh","title":"Consulta de C\u00f3digos de Documentos y Conexi\u00f3n a Base de Datos DWH","text":"<p>Este bloque de c\u00f3digo ejecuta una consulta para obtener todos los registros de la tabla <code>BD_Dim_Codigo_Documento</code> en el Data Warehouse (<code>dwh</code>), con el objetivo de cargar los c\u00f3digos de documentos en un diccionario <code>df_codigos</code>. La funci\u00f3n <code>create_engine</code> se utiliza para establecer una conexi\u00f3n con la base de datos mediante la funci\u00f3n personalizada <code>obtener_conexion</code>, la cual devuelve la cadena de conexi\u00f3n al DWH.</p> <p>Una vez establecida la conexi\u00f3n, la funci\u00f3n <code>cargar_tablas</code> se ejecuta con los par\u00e1metros <code>motor2</code>, <code>qr_structure_cod</code>, <code>df_codigos</code> y <code>logger</code>, encarg\u00e1ndose de ejecutar la consulta definida en <code>qr_structure_cod</code> y almacenar los resultados en <code>df_codigos</code>. Esta funci\u00f3n probablemente maneja la ejecuci\u00f3n de la consulta y el procesamiento de los datos, almacen\u00e1ndolos en un DataFrame u otra estructura.</p> <pre><code>#Lista de querys\nqr_structure_cod = {\n    \"query\": \"\"\"\nSELECT * FROM dwh.BD_Dim_Codigo_Documento;\n    \"\"\"\n}\ndf_codigos = dict()\nlogger.info('LECTURA DE QUERYS')\nprint(qr_structure2['query'])\n\n\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n\ncargar_tablas(motor2, qr_structure_cod, df_codigos, logger)\n</code></pre> <pre><code>2024-10-30 11:17:37,079 - INFO - LECTURA DE QUERYS\n2024-10-30 11:17:37,081 - INFO - CONEXION A BASE DWH\n\n\n\n        SELECT PERIODO, \n               ID_AFILIADO,\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \n               MAX(ID) as ID, \n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \n               MAX(CODCAT) as CODIGO_CATEGORIA, \n               MAX(CODZON) as CODIGO_ZONA, \n               MAX(CODEST) as CODIGO_ESTADO, \n               TIPO, \n               COD, \n               ESTADO,\n               MAX(SE_RETIRO) as SE_RETIRO, \n               MAX(SE_AFILIO) as SE_AFILIO,  \n               MAX(NUEVO) as NUEVO, \n               MAX(SEXO) as SEXO, \n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \n               MAX(FECNAC) as FECHA_NACIMIENTO, \n               MAX(PARENT) as PARENTESCO, \n               MAX(CODCIU) as CODIGO_CIUDAD, \n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \n               MAX(TIPAFI) as TIPO_AFILIACION, \n               MAX(NUMCUO) as NUMERO_CUOTAS, \n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \n        FROM (\n            SELECT \n                   ID_AFILIADO,\n                   CODIGO_DOCUMENTO, \n                   DOCUMENTO as ID, \n                   CODIGO_BENEFICIARIO as CODBEN, \n                   ESTADO, \n                   PERIODO, \n                   CODIGO_ZONA as CODZON, \n                   CODIGO_ESTADO as CODEST, \n                   DOCUMENTO_BENEFICIARIO as COD, \n                   CODIGO_CATEGORIA as CODCAT, \n                   SEXO,\n                   CAPACIDAD_TRABAJO as CAPTRA,\n                   NIVEL_EDUCACION as NIVEL_EDU, \n                   FECHA_NACIMIENTO as FECNAC, \n                   PARENTESCO as PARENT, \n                   CODIGO_CIUDAD as CODCIU, \n                   'Beneficiario' as TIPO, \n                   CALIFICACION_SUCURSAL as CALSUC, \n                   TIPO_AFILIACION as TIPAFI, \n                   NUMERO_CUOTAS as NUMCUO, \n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\n        ) as h\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\n\n\n\n2024-10-30 11:17:37,377 - INFO - Cargando query \n2024-10-30 11:17:37,407 - INFO - Cargada query, 10 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 11:17:37,458 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 0.37 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#union-de-dataframes-para-vincular-informacion-de-codigos-de-documentos","title":"Uni\u00f3n de DataFrames para Vincular Informaci\u00f3n de C\u00f3digos de Documentos","text":"<p>Este c\u00f3digo transforma las columnas <code>coddoc</code> en <code>df</code> y <code>COD_SUPERSUBSIDIO</code> en <code>df_cod</code> a texto en may\u00fasculas para asegurar consistencia. Luego, realiza una uni\u00f3n (<code>merge</code>) entre <code>df</code> y <code>df_cod</code>, usando <code>coddoc</code> en <code>df</code> y <code>COD_SUPERSUBSIDIO</code> en <code>df_cod</code> para agregar datos de documentos desde <code>df_cod</code> a <code>df</code>.</p> <p>La uni\u00f3n es del tipo <code>left</code>, conservando todos los registros de <code>df</code> y anexando las coincidencias de <code>df_cod</code> cuando est\u00e1n disponibles. La funci\u00f3n <code>head()</code> muestra las primeras filas de <code>df_merged</code>, permitiendo verificar visualmente los resultados de la fusi\u00f3n.</p> <pre><code>df_cod = df_codigos['query']\n\ndf = df_structure['query']\n\n# Convertir 'coddoc' y 'CODDOC' a tipo string y a may\u00fasculas\ndf['coddoc'] = df['coddoc'].astype(str).str.upper()\ndf_cod['COD_SUPERSUBSIDIO'] = df_cod['COD_SUPERSUBSIDIO'].astype(str).str.upper()\n\n# Realizar el merge usando 'CODDOC' de df_cod\ndf_merged = df.merge(df_cod[['COD_SUPERSUBSIDIO', 'CODDOC']], left_on='coddoc', right_on='COD_SUPERSUBSIDIO', how='left')\n\n# Mostrar las primeras filas para verificar el resultado\ndf_merged.head()\n</code></pre> codben periodo estado coddoc cedtra codest documento coddocben sexo captra ... calsuc tipafi numcuo subliq fecsis docben persis codciu COD_SUPERSUBSIDIO CODDOC 0 406348.0 202305 A 1 1000120248 None 1084067407 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2020-02-08 51084067407 202002 None 1 CC 1 NaN 202305 A 1 1000120248 None 1128144889 1 2 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11128144889 None None 1 CC 2 NaN 202305 A 1 1000121261 None 1007860991 1 2 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11007860991 None None 1 CC 3 364243.0 202305 A 1 1000121261 None 1083040303 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2017-11-02 51083040303 201711 None 1 CC 4 364243.0 202305 A 1 1000121261 None 1083040303 5 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2023-08-28 51083040303 202308 None 1 CC <p>5 rows \u00d7 27 columns</p>"},{"location":"seccion/cola_documentacion/#creacion-de-identificador-unico-de-afiliado-y-estandarizacion-de-columnas","title":"Creaci\u00f3n de Identificador \u00danico de Afiliado y Estandarizaci\u00f3n de Columnas","text":"<p>Este c\u00f3digo combina los valores de <code>CODDOC</code> y <code>cedtra</code> para crear una columna <code>ID_AFILIADO</code> que representa un identificador \u00fanico de afiliado en <code>df_final</code>. Posteriormente, estandariza los nombres de columnas en <code>df_final</code> mediante un diccionario (<code>_names</code>) con la funci\u00f3n <code>convertir_columnas_mayusculas</code>, asegurando consistencia y claridad en la nomenclatura de campos.</p> <p>El DataFrame <code>df_standar</code> resultante se filtra para incluir \u00fanicamente las columnas deseadas en un orden espec\u00edfico, y <code>df_final.columns.to_list()</code> proporciona la lista de nombres de columnas finales, confirmando que los nombres y el orden sean correctos antes de proceder con el an\u00e1lisis o almacenamiento en el Data Warehouse.</p> <pre><code># Crear 'ID_AFILIADO' en un solo paso y asignar directamente a df_final\ndf_final = df_merged\ndf_final['ID_AFILIADO'] = df_final['CODDOC'].astype(str) + df_final['cedtra'].astype(str)\n\n# Mostrar el resultado\n_names = {\n    'codben': 'CODIGO_BENEFICIARIO',\n    'coddoc': 'Codigo_Documento',\n    'cedtra': 'Cedula_Trabajador',\n    'codest': 'Codigo_Estado',\n    'captra': 'Capacidad_Trabajo',\n    'nivedu': 'Nivel_Educacion',\n    'fecnac': 'Fecha_Nacimiento',\n    'parent': 'Parentesco',\n    'codcat': 'Codigo_Categoria',\n    'codzon': 'Codigo_Zona',\n    'perafi': 'Periodo_Afiliacion',\n    'perret': 'Periodo_Retiro',\n    'calsuc': 'Calificacion_Sucursal',\n    'tipafi': 'Tipo_Afiliacion',\n    'numcuo': 'Numero_Cuotas',\n    'subliq': 'Subsidio_Liquidado',\n    'fecsis': 'Fecha_Sistema',\n    'docben': 'Documento_Beneficiario',\n    'persis': 'Periodo_Sistema',\n    'codciu': 'Codigo_Ciudad',\n    'CODDOC': 'Codigo_Documento_Afiliado'\n}\n\n\ndf_standar = convertir_columnas_mayusculas(df_final,_names)\ndf_final = df_standar[[\n    'CODIGO_BENEFICIARIO',\n    'ID_AFILIADO',\n    'CEDULA_TRABAJADOR',\n    'PERIODO',\n    'ESTADO',\n    'CODIGO_DOCUMENTO',\n    'CODIGO_ESTADO',\n    'DOCUMENTO',\n    'CODDOCBEN',\n    'SEXO',\n    'CAPACIDAD_TRABAJO',\n    'NIVEL_EDUCACION',\n    'FECHA_NACIMIENTO',\n    'PARENTESCO',\n    'CODIGO_CATEGORIA',\n    'CODIGO_ZONA',\n    'PERIODO_AFILIACION',\n    'PERIODO_RETIRO',\n    'CALIFICACION_SUCURSAL',\n    'TIPO_AFILIACION',\n    'NUMERO_CUOTAS',\n    'SUBSIDIO_LIQUIDADO',\n    'FECHA_SISTEMA',\n    'DOCUMENTO_BENEFICIARIO',\n    'PERIODO_SISTEMA',\n    'CODIGO_CIUDAD',\n    'COD_SUPERSUBSIDIO',\n    'CODIGO_DOCUMENTO_AFILIADO',\n    ]]\ndf_final.columns.to_list()\n</code></pre> <pre><code>['CODIGO_BENEFICIARIO',\n 'ID_AFILIADO',\n 'CEDULA_TRABAJADOR',\n 'PERIODO',\n 'ESTADO',\n 'CODIGO_DOCUMENTO',\n 'CODIGO_ESTADO',\n 'DOCUMENTO',\n 'CODDOCBEN',\n 'SEXO',\n 'CAPACIDAD_TRABAJO',\n 'NIVEL_EDUCACION',\n 'FECHA_NACIMIENTO',\n 'PARENTESCO',\n 'CODIGO_CATEGORIA',\n 'CODIGO_ZONA',\n 'PERIODO_AFILIACION',\n 'PERIODO_RETIRO',\n 'CALIFICACION_SUCURSAL',\n 'TIPO_AFILIACION',\n 'NUMERO_CUOTAS',\n 'SUBSIDIO_LIQUIDADO',\n 'FECHA_SISTEMA',\n 'DOCUMENTO_BENEFICIARIO',\n 'PERIODO_SISTEMA',\n 'CODIGO_CIUDAD',\n 'COD_SUPERSUBSIDIO',\n 'CODIGO_DOCUMENTO_AFILIADO']\n</code></pre>"},{"location":"seccion/cola_documentacion/#guardar-en-base-de-datos-dwh_1","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>dfTotal</code> en la tabla <code>BD_nombre</code> de la base DWH usando <code>with</code> para garantizar el cierre autom\u00e1tico de la conexi\u00f3n. Se registra el tiempo de ejecuci\u00f3n en el log.</p> <pre><code>guardar_en_dwh(df_final, 'BD_Fact_Beneficiarios_p1', logger, multiple=False, if_exists='replace')\ndf_final.columns.to_list()\ndf_final\n</code></pre> <pre><code>2024-10-30 11:17:45,787 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:17:45,788 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:17:46,088 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Beneficiarios_p1\n2024-10-30 11:28:18,350 - INFO - Tabla almacenada correctamente. 3,832,674 registros finales obtenidos.\n2024-10-30 11:28:22,516 - INFO - ALMACENAMIENTO ---  --- 10.61 minutes ---\n2024-10-30 11:28:22,518 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> CODIGO_BENEFICIARIO ID_AFILIADO CEDULA_TRABAJADOR PERIODO ESTADO CODIGO_DOCUMENTO CODIGO_ESTADO DOCUMENTO CODDOCBEN SEXO ... CALIFICACION_SUCURSAL TIPO_AFILIACION NUMERO_CUOTAS SUBSIDIO_LIQUIDADO FECHA_SISTEMA DOCUMENTO_BENEFICIARIO PERIODO_SISTEMA CODIGO_CIUDAD COD_SUPERSUBSIDIO CODIGO_DOCUMENTO_AFILIADO 0 406348.0 CC1000120248 1000120248 202305 A 1 None 1084067407 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2020-02-08 51084067407 202002 None 1 CC 1 NaN CC1000120248 1000120248 202305 A 1 None 1128144889 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11128144889 None None 1 CC 2 NaN CC1000121261 1000121261 202305 A 1 None 1007860991 1 2 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 None 11007860991 None None 1 CC 3 364243.0 CC1000121261 1000121261 202305 A 1 None 1083040303 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2017-11-02 51083040303 201711 None 1 CC 4 364243.0 CC1000121261 1000121261 202305 A 1 None 1083040303 5 1 ... DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2023-08-28 51083040303 202308 None 1 CC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3832669 8647200.0 CC9910609 9910609 202409 A 1 None 1105473400 2 1 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 1.0 44491.0 2021-12-16 21105473400 202112 None 1 CC 3832670 NaN CC9910609 9910609 202409 A 1 None 49597057 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 None 149597057 None None 1 CC 3832671 314997.0 CC9910731 9910731 202409 A 1 None 1082877639 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 2015-05-07 11082877639 201505 None 1 CC 3832672 NaN CC9910731 9910731 202409 A 1 None 1082939020 1 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 0.0 0.0 None 11082939020 None None 1 CC 3832673 310091.0 CC9910731 9910731 202409 A 1 None 1083018512 2 2 ... DEPENDIENTES (NO INCLUYE: SERVICIO DOMESTICO N... 1 1.0 44491.0 2015-02-12 21083018512 201502 None 1 CC <p>3832674 rows \u00d7 28 columns</p>"},{"location":"seccion/cola_documentacion/#conexion-a-la-base-de-datos-dwh-y-carga-de-tablas","title":"Conexi\u00f3n a la Base de Datos DWH y Carga de Tablas","text":"<p>Este c\u00f3digo establece una conexi\u00f3n con la base de datos de Data Warehouse (DWH) utilizando <code>create_engine</code> con los detalles de conexi\u00f3n proporcionados por la funci\u00f3n <code>obtener_conexion</code>. La funci\u00f3n personalizada <code>cargar_tablas</code> se encarga de ejecutar las consultas definidas en <code>qr_structure2</code>, almacenando los resultados en el diccionario <code>df_structure2</code>.</p> <p>Al final, se ejecuta el m\u00e9todo <code>head()</code> en <code>df_structure2['query']</code> para mostrar las primeras filas del DataFrame resultante, permitiendo una verificaci\u00f3n r\u00e1pida de los datos cargados. Esto asegura que la conexi\u00f3n a la base de datos y la carga de datos se han realizado correctamente antes de proceder con el an\u00e1lisis o transformaci\u00f3n de datos.</p> <pre><code>qr_structure2\n</code></pre> <pre><code>{'query': \"\\n        SELECT PERIODO, \\n               ID_AFILIADO,\\n               MAX(CODIGO_DOCUMENTO) as CODIGO_DOCUMENTO, \\n               MAX(ID) as ID, \\n               MAX(CODBEN) as CODIGO_BENEFICIARIO,  \\n               MAX(CODCAT) as CODIGO_CATEGORIA, \\n               MAX(CODZON) as CODIGO_ZONA, \\n               MAX(CODEST) as CODIGO_ESTADO, \\n               TIPO, \\n               COD, \\n               ESTADO,\\n               MAX(SE_RETIRO) as SE_RETIRO, \\n               MAX(SE_AFILIO) as SE_AFILIO,  \\n               MAX(NUEVO) as NUEVO, \\n               MAX(SEXO) as SEXO, \\n               MAX(CAPTRA) as CAPACIDAD_TRABAJO, \\n               MAX(NIVEL_EDU) as NIVEL_EDUCATIVO, \\n               MAX(FECNAC) as FECHA_NACIMIENTO, \\n               MAX(PARENT) as PARENTESCO, \\n               MAX(CODCIU) as CODIGO_CIUDAD, \\n               MAX(CALSUC) as CALIFICACION_SUCURSAL, \\n               MAX(TIPAFI) as TIPO_AFILIACION, \\n               MAX(NUMCUO) as NUMERO_CUOTAS, \\n               MAX(SUBLIQ) as SUBSIDIO_LIQUIDADO  \\n        FROM (\\n            SELECT \\n                   ID_AFILIADO,\\n                   CODIGO_DOCUMENTO, \\n                   DOCUMENTO as ID, \\n                   CODIGO_BENEFICIARIO as CODBEN, \\n                   ESTADO, \\n                   PERIODO, \\n                   CODIGO_ZONA as CODZON, \\n                   CODIGO_ESTADO as CODEST, \\n                   DOCUMENTO_BENEFICIARIO as COD, \\n                   CODIGO_CATEGORIA as CODCAT, \\n                   SEXO,\\n                   CAPACIDAD_TRABAJO as CAPTRA,\\n                   NIVEL_EDUCACION as NIVEL_EDU, \\n                   FECHA_NACIMIENTO as FECNAC, \\n                   PARENTESCO as PARENT, \\n                   CODIGO_CIUDAD as CODCIU, \\n                   'Beneficiario' as TIPO, \\n                   CALIFICACION_SUCURSAL as CALSUC, \\n                   TIPO_AFILIACION as TIPAFI, \\n                   NUMERO_CUOTAS as NUMCUO, \\n                   SUBSIDIO_LIQUIDADO as SUBLIQ, \\n                   IF(PERIODO_RETIRO = PERIODO, 1, 0) as SE_RETIRO,\\n                   IF(PERIODO_AFILIACION = PERIODO, 1, 0) as SE_AFILIO,\\n                   IF(PERIODO = PERIODO_SISTEMA, 1, 0) as NUEVO\\n            FROM dwh.BD_Fact_Beneficiarios_p1 as p\\n        ) as h\\n        GROUP BY PERIODO, TIPO, COD, ESTADO;\\n    \"}\n</code></pre> <pre><code>#Conexion a base DWH\nmotor = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A DWH ')\ncargar_tablas(motor, qr_structure2, df_structure2, logger)\ndf_structure2['query'].head()\n</code></pre> <pre><code>2024-10-30 11:28:26,277 - INFO - CONEXION A DWH \n2024-10-30 11:28:26,577 - INFO - Cargando query \n2024-10-30 11:31:42,403 - INFO - Cargada query, 3,371,398 registros finales obtenidos. --- 3.26 minutes ---\n2024-10-30 11:31:42,714 - INFO - CARGUE TABLAS DESDE MYSQL --- query --- 3.27 minutes ---\n</code></pre> PERIODO ID_AFILIADO CODIGO_DOCUMENTO ID CODIGO_BENEFICIARIO CODIGO_CATEGORIA CODIGO_ZONA CODIGO_ESTADO TIPO COD ... SEXO CAPACIDAD_TRABAJO NIVEL_EDUCATIVO FECHA_NACIMIENTO PARENTESCO CODIGO_CIUDAD CALIFICACION_SUCURSAL TIPO_AFILIACION NUMERO_CUOTAS SUBSIDIO_LIQUIDADO 0 202305 CC1000120248 1 1084067407 406348.0 1 47001 None Beneficiario 51084067407 ... 1 2 01 2019-12-14 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 1 202305 CC1000120248 1 1128144889 NaN 1 47001 None Beneficiario 11128144889 ... 2 2 None 2002-03-07 5 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 2 202305 CC1000121261 1 1007860991 NaN 1 47001 None Beneficiario 11007860991 ... 2 2 None 1998-11-12 5 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 3 202305 CC1000121261 1 1083040303 364243.0 1 47001 None Beneficiario 51083040303 ... 1 2 01 2016-10-27 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 4 202305 CC1000121261 1 1084469342 8626426.0 1 47001 None Beneficiario 51084469342 ... 1 2 01 2019-10-22 1 None DEPENDIENTE CATEGORIA C CON DERECHO TEMPORAL A... 13 0.0 0.0 <p>5 rows \u00d7 24 columns</p> <pre><code>print(df_structure2['query']['PERIODO'].unique())\n</code></pre> <pre><code>['202305' '202306' '202307' '202308' '202309' '202310' '202311' '202312'\n '202401' '202402' '202403' '202404' '202405' '202406' '202407' '202408'\n '202409']\n</code></pre>"},{"location":"seccion/cola_documentacion/#guardado-de-datos-en-dwh-y-verificacion-de-columnas","title":"Guardado de Datos en DWH y Verificaci\u00f3n de Columnas","text":"<p>En este c\u00f3digo, el DataFrame <code>df_final2</code> (cargado desde <code>df_structure2['query']</code>) se guarda en la base de datos DWH en la tabla <code>BD_Fact_Beneficiarios_p2</code> utilizando la funci\u00f3n <code>guardar_en_dwh</code>. Esta funci\u00f3n recibe <code>multiple=False</code> para realizar una carga \u00fanica y <code>if_exists='replace'</code> para reemplazar la tabla si ya existe en la base de datos, asegurando que los datos m\u00e1s recientes reemplacen cualquier versi\u00f3n anterior.</p> <p>Al final, <code>df_final2.columns.to_list()</code> se usa para obtener una lista de los nombres de columnas en <code>df_final2</code>, permitiendo verificar r\u00e1pidamente que las columnas est\u00e1n en el orden y con los nombres esperados antes de completar el proceso de carga.</p> <pre><code>df_final2 = df_structure2['query']\n\nguardar_en_dwh(df_final2, 'BD_Fact_Beneficiarios_p2', logger, multiple=False, if_exists='replace')\ndf_final2.columns.to_list()\n</code></pre> <pre><code>2024-10-30 11:31:43,061 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 11:31:43,062 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 11:31:43,387 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Beneficiarios_p2\n2024-10-30 11:40:19,134 - INFO - Tabla almacenada correctamente. 3,371,398 registros finales obtenidos.\n2024-10-30 11:40:22,203 - INFO - ALMACENAMIENTO ---  --- 8.65 minutes ---\n2024-10-30 11:40:22,204 - INFO - Finalizando proceso de almacenamiento en DWH.\n\n\n\n\n\n['PERIODO',\n 'ID_AFILIADO',\n 'CODIGO_DOCUMENTO',\n 'ID',\n 'CODIGO_BENEFICIARIO',\n 'CODIGO_CATEGORIA',\n 'CODIGO_ZONA',\n 'CODIGO_ESTADO',\n 'TIPO',\n 'COD',\n 'ESTADO',\n 'SE_RETIRO',\n 'SE_AFILIO',\n 'NUEVO',\n 'SEXO',\n 'CAPACIDAD_TRABAJO',\n 'NIVEL_EDUCATIVO',\n 'FECHA_NACIMIENTO',\n 'PARENTESCO',\n 'CODIGO_CIUDAD',\n 'CALIFICACION_SUCURSAL',\n 'TIPO_AFILIACION',\n 'NUMERO_CUOTAS',\n 'SUBSIDIO_LIQUIDADO']\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 11:40:22,218 - INFO - FINAL ETL --- 1886.17 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#42-factcolegio","title":"4.2-FactColegio","text":""},{"location":"seccion/cola_documentacion/#42-factcolegio_1","title":"4.2-FactColegio","text":""},{"location":"seccion/cola_documentacion/#42-fact-colegio","title":"4.2 Fact Colegio","text":""},{"location":"seccion/cola_documentacion/#introduccion_2","title":"Introducci\u00f3n","text":"<p>El proceso ETL para <code>FactColegio</code> permite extraer, transformar y cargar informaci\u00f3n de estudiantes, acudientes, matr\u00edculas, y grados de una base de datos de colegio hacia un Data Warehouse (DWH). Este proceso se enfoca en estructurar datos personales y acad\u00e9micos, as\u00ed como en estandarizar las direcciones de estudiantes y acudientes. El flujo ETL emplea consultas SQL para extraer los datos, transformaciones para limpieza y deduplicaci\u00f3n, y una carga final en el DWH con nombres de tablas predefinidos. Este procedimiento asegura la disponibilidad de datos estandarizados y listos para an\u00e1lisis y reportes en el entorno del DWH.</p>"},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl_3","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactColegio\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para datos de estudiantes, acudientes, matr\u00edculas, y grados\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de entidades del colegio\n    ETL_Script -&gt;&gt; ETL_Script: Estandariza direcciones y elimina duplicados\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga tablas transformadas y dimensionales en el DWH\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre>"},{"location":"seccion/cola_documentacion/#etl_3","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones_3","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno para la manipulaci\u00f3n de datos en un Data Warehouse, importando librer\u00edas clave como <code>pandas</code>, <code>sqlalchemy</code> y <code>logging</code> para el manejo de datos, conexiones de base y registro de eventos. Tambi\u00e9n se cargan funciones personalizadas del m\u00f3dulo <code>Funciones.py</code>, incluyendo <code>guardar_en_dwh</code>, <code>StoreDuplicated</code>, <code>estandarizar_direccion</code>, y <code>marcar_direcciones_estandarizadas</code>, que facilitan la carga, estandarizaci\u00f3n y validaci\u00f3n de datos. La prueba <code>testfunciones()</code> asegura que estas funciones est\u00e1n correctamente importadas y operativas.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, StoreDuplicated, obtener_conexion, cargar_tablas, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 12:28\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-inicial-del-logger","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Colegio.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Colegio.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 12:28:00,767 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#definicion-de-consultas-para-datos-de-colegio","title":"Definici\u00f3n de Consultas para Datos de Colegio","text":"<p>Este c\u00f3digo configura las consultas SQL en el diccionario <code>qr_structure</code> para obtener datos de diversas tablas relacionadas con estudiantes, acudientes, matr\u00edculas, grados y niveles educativos en una base de datos de colegio. Cada consulta extrae y renombra columnas espec\u00edficas de las tablas <code>colegio12</code>, <code>colegio13</code>, <code>colegio15</code>, y <code>colegio03</code> para estandarizar los datos. El diccionario <code>dim_names</code> asigna nombres descriptivos para identificar cada consulta en <code>df_structure</code>, donde se almacenar\u00e1n los resultados. Esta configuraci\u00f3n permite la carga y procesamiento eficiente de datos escolares para an\u00e1lisis posteriores.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"colegio12\":'''select \n                ano as ANIO,\n                CONCAT(coddoc,numdoc) AS ID_AFILIADO,\n                coddoc AS TIPO_DOCUMENTO,\n                numdoc AS DOCUMENTO,\n                ciuexp AS CIUDAD_EXPEDICION,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                rh,\n                eps,\n                codsex AS CODSEX,\n                ciunac AS CIUDAD_NACIMIENTO,\n                fecnac AS FECHA_NACIMIENTO,\n                dirres AS DIRECCION,\n                barrio AS BARRIO,\n                estrato AS ESTRATO,\n                codciu AS CODIGO_CIUDAD,\n                telres AS TELEFONO,\n                email,\n                nota,\n                observacion,\n                docpad AS DOCUMENTO_PADRE,\n                docmad AS DOCUMENTO_MADRE,\n                docacu AS DOCUMENTO_ACUDIENTE,\n                codben,\n                codcat,\n                tipo AS TIPO_ESTUDIANTE\n                from colegio.colegio12''',\n    \"colegio13\":'''select \n                CONCAT(coddoc,numdoc) AS ID_AFILIADO,\n                coddoc AS TIPO_DOCUMENTO,\n                numdoc AS DOCUMENTO,\n                ciuexp AS CIUDAD_EXPEDICION,\n                priape AS PRIMER_APELLIDO,\n                segape AS SEGUNDO_APELLIDO,\n                prinom AS PRIMER_NOMBRE,\n                segnom AS SEGUNDO_NOMBRE,\n                codpar AS COD_PARENTESCO,\n                profesion,\n                empresa,\n                ocupacion,\n                codsex,\n                direccion,\n                codciu AS CODIGO_CIUDAD,\n                telefono,\n                email\n                from colegio.colegio13''',\n    \"colegio15\":'''select \n                c15.ano as ANIO,\n                c15.nummat AS NUMERO_MATRICULA,\n                concat(c12.coddoc,c15.numdoc) AS ID_AFILIADO,\n                c15.codgra AS COD_GRADO,\n                c15.codgru AS COD_GRUPO,\n                c15.fecmat AS FECHA_MATRICULA,\n                c15.colant AS COLEGIO_ANTERIOR,\n                c15.numlib AS NUMERO_LIBRO_REGISTRO,\n                c15.numfol AS NUMERO_FOLIO_REGISTRO,\n                c15.codcon AS COD_CONVENIO,\n                c15.codest AS COD_ESTADO,\n                c15.codcat,\n                c15.estado,\n                c15.fecest AS FECHA_ESTADO,\n                c15.motivo AS MOTIVO_NOVEDAD,\n                c15.usuario,\n                c15.estbol AS ESTADO_BOLETIN\n                from colegio.colegio15 c15\n            LEFT JOIN colegio.colegio12 c12 on c15.ano = c12.ano and c15.numdoc = c12.numdoc''',\n    \"colegio03\": '''SELECT \n                c03.codgra AS COD_GRADO,\n                c03.detalle AS GRADO,\n                c02.detalle AS NIVEL_EDUCATIVO,\n                c03.numcup AS NUMERO_CUPOS\n            FROM colegio.colegio03 c03\n            LEFT JOIN colegio.colegio02 c02 on c03.nivedu = c02.nivedu    \n    '''\n               }\ndim_names = {\n    \"colegio12\":'BD_Fact_Colegio_Estudiantes',\n    \"colegio13\":'BD_Fact_Colegio_Acudientes',\n    \"colegio15\":'BD_Fact_Colegio_Matriculas',\n    \"BD_Dim_Colegio_Calendario\" : \"BD_Dim_Colegio_Calendario\",\n    \"colegio03\": \"BD_Dim_Colegio_Grados\"\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 12:28:00,782 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#carga-de-tablas-desde-sql","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n a la base de datos de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code>#Conexion a base Minerva\nmotor = create_engine(obtener_conexion('minerva'))\nlogger.info('CONEXION A BASE MINERVA')\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 12:28:00,852 - INFO - CONEXION A BASE MINERVA\n2024-10-30 12:28:01,177 - INFO - Cargando colegio12 \n2024-10-30 12:28:01,640 - INFO - Cargada colegio12, 2,501 registros finales obtenidos. --- 0.46 seconds ---\n2024-10-30 12:28:01,641 - INFO - Cargando colegio13 \n2024-10-30 12:28:01,815 - INFO - Cargada colegio13, 1,291 registros finales obtenidos. --- 0.17 seconds ---\n2024-10-30 12:28:01,816 - INFO - Cargando colegio15 \n2024-10-30 12:28:01,970 - INFO - Cargada colegio15, 2,243 registros finales obtenidos. --- 0.15 seconds ---\n2024-10-30 12:28:01,971 - INFO - Cargando colegio03 \n2024-10-30 12:28:02,003 - INFO - Cargada colegio03, 13 registros finales obtenidos. --- 0.03 seconds ---\n2024-10-30 12:28:02,062 - INFO - CARGUE TABLAS DESDE MYSQL --- colegio03 --- 1.21 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#validacion-y-eliminacion-de-campos-duplicados-en-tablas","title":"Validaci\u00f3n y Eliminaci\u00f3n de Campos Duplicados en Tablas","text":"<p>Este c\u00f3digo recorre las tablas almacenadas en <code>df_structure</code> y realiza una validaci\u00f3n para detectar y registrar valores duplicados en cada tabla. Para cada tabla, genera una lista de columnas a comparar (<code>ColumnsToCompare</code>), excluyendo el campo <code>id</code>. La funci\u00f3n <code>StoreDuplicated</code> verifica duplicados en funci\u00f3n de estas columnas y genera un archivo de trazabilidad con el nombre <code>\\trazaDuplicados_</code> seguido del nombre de la tabla.</p> <p>Una vez verificados los duplicados, se eliminan las filas duplicadas, dejando solo registros \u00fanicos. Los tiempos de ejecuci\u00f3n de esta operaci\u00f3n se registran en <code>validador_time</code>, proporcionando un resumen del proceso en segundos al finalizar.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:02,124 - INFO - VALIDADOR TABLA: colegio12\n2024-10-30 12:28:02,148 - INFO - VALIDADOR TABLA: colegio13\n2024-10-30 12:28:02,171 - INFO - VALIDADOR TABLA: colegio15\n2024-10-30 12:28:02,178 - INFO - VALIDADOR TABLA: colegio03\n2024-10-30 12:28:02,180 - INFO - VALIDADOR DUPLICADOS --- 0.11 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#transformacion-y-limpieza-de-datos-en-tablas","title":"Transformaci\u00f3n y Limpieza de Datos en Tablas","text":"<p>Este c\u00f3digo realiza una serie de transformaciones de limpieza en cada tabla dentro de <code>df_structure</code>. Primero, convierte todos los nombres de columnas a may\u00fasculas para asegurar consistencia en el formato. Luego, se aplica una transformaci\u00f3n en todas las columnas de tipo texto para pasar su contenido a may\u00fasculas y eliminar espacios en blanco al inicio y al final de cada valor, manteniendo la uniformidad en el formato.</p> <p>Finalmente, reemplaza los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por <code>NaN</code> de NumPy, asegurando que los datos faltantes est\u00e9n correctamente etiquetados. El tiempo de ejecuci\u00f3n de estas transformaciones se registra y se muestra en <code>limpieza_time</code>.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA  --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:02,326 - INFO - LIMPIEZA  --- 0.14 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#creacion-de-tabla-dimensional-para-anos-de-calendario","title":"Creaci\u00f3n de Tabla Dimensional para A\u00f1os de Calendario","text":"<p>Este c\u00f3digo extrae la columna <code>ANIO</code> de la tabla de matr\u00edculas (<code>colegio15</code>) en <code>df_structure</code>, obtiene los valores \u00fanicos de esa columna y crea un nuevo DataFrame <code>df_valores_unicos</code> con estos valores. Este DataFrame sirve como una tabla dimensional de a\u00f1os (<code>BD_Dim_Colegio_Calendario</code>), almacenando los distintos a\u00f1os de calendario de las matr\u00edculas en <code>df_structure</code> para su uso en an\u00e1lisis de tiempo o relaciones entre tablas.</p> <pre><code># Obtener la tabla original desde df_structure\ntabla = df_structure['colegio15']\n\n# Obtener los valores \u00fanicos de la columna 'TIPO'\nvalores_unicos = pd.unique(tabla['ANIO'])\n\n# Crear un DataFrame a partir de los valores \u00fanicos con la columna renombrada\ndf_valores_unicos = pd.DataFrame(valores_unicos, columns=['ANIO'])\n\n# Crear Dim\ndf_structure['BD_Dim_Colegio_Calendario'] = df_valores_unicos.copy()\ndf_structure['BD_Dim_Colegio_Calendario']\n</code></pre> ANIO 0 2020 1 2021 2 2022 3 2023 4 2024"},{"location":"seccion/cola_documentacion/#estandarizacion-de-direcciones-para-estudiantes-y-acudientes","title":"Estandarizaci\u00f3n de Direcciones para Estudiantes y Acudientes","text":"<p>Este bloque de c\u00f3digo realiza la estandarizaci\u00f3n de direcciones en los DataFrames <code>df_estudiantes</code> y <code>df_acudientes</code>, extra\u00eddos de <code>df_structure</code>. Primero, renombra la columna <code>DIRECCION</code> a <code>DIRECCION_ORIGINAL</code> y asegura que sea de tipo texto para ambos DataFrames. Luego, aplica la funci\u00f3n <code>estandarizar_direccion</code> en <code>DIRECCION_ORIGINAL</code> para generar una columna <code>DIRECCION_LIMPIA</code> con las direcciones en un formato estandarizado.</p> <p>Posteriormente, utiliza <code>marcar_direcciones_estandarizadas</code> para agregar una columna que indique si la direcci\u00f3n estandarizada comienza con una de las abreviaturas esperadas. Finalmente, las tablas estandarizadas se actualizan en <code>df_structure</code>, dejando las direcciones listas para su an\u00e1lisis y asegurando consistencia en el formato de direcci\u00f3n.</p> <pre><code># Para estudiantes\ndf_estudiantes = df_structure['colegio12'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_estudiantes.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_estudiantes['DIRECCION_ORIGINAL'] = df_estudiantes['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_estudiantes['DIRECCION_LIMPIA'] = df_estudiantes['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_estudiantes, 'DIRECCION_LIMPIA')\n\ndf_structure['colegio12'] = df_estudiantes.copy()\n#df_structure['colegio12']\n</code></pre> <pre><code># Para acudientes\ndf_acudientes = df_structure['colegio13'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_acudientes.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_acudientes['DIRECCION_ORIGINAL'] = df_acudientes['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_acudientes['DIRECCION_LIMPIA'] = df_acudientes['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_acudientes, 'DIRECCION_LIMPIA')\n\ndf_structure['colegio13'] = df_acudientes.copy()\n#df_structure['colegio13']\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-a-la-base-de-datos-dwh","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\nmotor2 = create_engine(obtener_conexion('dwh'))\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 12:28:02,430 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/cola_documentacion/#guardar-en-base-de-datos-dwh_2","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla del diccionario <code>df_structure</code> en su respectiva tabla en la base de datos DWH, utilizando los nombres definidos en <code>dim_names</code>. El contenido de cada tabla se reemplaza si ya existe en la base de datos. El tiempo total de almacenamiento se registra en el log.</p> <pre><code>#Conexion a base dwh\n\nguardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 12:28:02,440 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 12:28:02,442 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 12:28:02,730 - INFO - Almacenando tabla colegio12 en DWH como BD_Fact_Colegio_Estudiantes\n2024-10-30 12:28:04,964 - INFO - Tabla colegio12 almacenada correctamente como BD_Fact_Colegio_Estudiantes.\n2024-10-30 12:28:04,965 - INFO - Almacenando tabla colegio13 en DWH como BD_Fact_Colegio_Acudientes\n2024-10-30 12:28:06,820 - INFO - Tabla colegio13 almacenada correctamente como BD_Fact_Colegio_Acudientes.\n2024-10-30 12:28:06,820 - INFO - Almacenando tabla colegio15 en DWH como BD_Fact_Colegio_Matriculas\n2024-10-30 12:28:08,041 - INFO - Tabla colegio15 almacenada correctamente como BD_Fact_Colegio_Matriculas.\n2024-10-30 12:28:08,042 - INFO - Almacenando tabla colegio03 en DWH como BD_Dim_Colegio_Grados\n2024-10-30 12:28:08,625 - INFO - Tabla colegio03 almacenada correctamente como BD_Dim_Colegio_Grados.\n2024-10-30 12:28:08,627 - INFO - Almacenando tabla BD_Dim_Colegio_Calendario en DWH como BD_Dim_Colegio_Calendario\n2024-10-30 12:28:09,123 - INFO - Tabla BD_Dim_Colegio_Calendario almacenada correctamente como BD_Dim_Colegio_Calendario.\n2024-10-30 12:28:09,181 - INFO - ALMACENAMIENTO ---  --- 6.74 seconds ---\n2024-10-30 12:28:09,182 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 12:28:09,191 - INFO - FINAL ETL --- 8.44 seconds ---\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"seccion/cola_documentacion/#52-factserviciosocial","title":"5.2-FactServicioSocial","text":""},{"location":"seccion/cola_documentacion/#52-factserviciosocial_1","title":"5.2-FactServicioSocial","text":""},{"location":"seccion/cola_documentacion/#52-fact-servicio-social","title":"5.2 Fact Servicio Social","text":""},{"location":"seccion/cola_documentacion/#introduccion_3","title":"Introducci\u00f3n","text":"<p>El proceso ETL <code>FactServicioSocial</code> permite extraer, transformar y cargar informaci\u00f3n clave sobre estudiantes y sus parientes en un Data Warehouse. Las tablas origen se extraen de dos bases (<code>nsijec</code> y <code>saipi</code>), se realizan validaciones de duplicados, limpieza de texto y estandarizaci\u00f3n de direcciones, asegurando que los datos cargados en el DWH sean completos, consistentes y listos para an\u00e1lisis posteriores.</p>"},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl_4","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n    title Diagrama de Secuencia del Proceso ETL - FactServicioSocial\n    participant \ud83d\udc64 Usuario\n    participant ETL_Script\n    participant Minerva_DB as Base de Datos Minerva\n    participant Neith_DB as Base de Datos Neith\n    participant DWH as Data Warehouse\n\n    \ud83d\udc64 Usuario -&gt;&gt; ETL_Script: Ejecuta el script ETL\n    ETL_Script -&gt;&gt; ETL_Script: Configura entorno y logger\n    ETL_Script -&gt;&gt; Minerva_DB: Conecta a BD Minerva\n    Minerva_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Minerva_DB: Ejecuta consulta SQL para extraer `estudiantes`\n    Minerva_DB --&gt;&gt; ETL_Script: Retorna datos de `estudiantes`\n    ETL_Script -&gt;&gt; Neith_DB: Conecta a BD Neith\n    Neith_DB --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; Neith_DB: Ejecuta consultas SQL para extraer `personas_saipi`, `acudientes`, `registros`, `valoracion`\n    Neith_DB --&gt;&gt; ETL_Script: Retorna datos de `saipi`\n    ETL_Script -&gt;&gt; ETL_Script: Valida duplicados y transforma datos\n    ETL_Script -&gt;&gt; ETL_Script: Estandariza direcciones\n    ETL_Script -&gt;&gt; DWH: Conecta a BD DWH\n    DWH --&gt;&gt; ETL_Script: Conexi\u00f3n exitosa\n    ETL_Script -&gt;&gt; DWH: Carga datos en tablas de DWH (`Fact_nsijec_Estudiantes`, `Dim_saipi_Personas`, etc.)\n    DWH --&gt;&gt; ETL_Script: Confirmaci\u00f3n de carga exitosa\n    ETL_Script -&gt;&gt; \ud83d\udc64 Usuario: Finaliza proceso ETL y registra tiempo</code></pre> <p>Este diagrama muestra c\u00f3mo el proceso ETL asegura que los datos de los estudiantes y sus parientes est\u00e9n consolidados en el DWH, permitiendo mantener una base de datos uniforme y de calidad.</p>"},{"location":"seccion/cola_documentacion/#etl_4","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones_4","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Se importan las librer\u00edas necesarias (<code>sqlalchemy</code>, <code>pandas</code>, <code>numpy</code>, <code>dateutil</code>, entre otras) y se cargan funciones personalizadas desde el archivo <code>Funciones.py</code>. Adem\u00e1s, se ajusta el <code>sys.path</code> para incluir el directorio correspondiente, garantizando la correcta importaci\u00f3n del m\u00f3dulo de funciones.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport pymysql\nimport time\nimport numpy as np\nimport time\nimport os\nimport logging\nfrom datetime import date\nstart_time = time.time()\nfrom dateutil.relativedelta import relativedelta\n#---------------------------------------------\n#Import de modulo funciones\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import StoreDuplicated, guardar_en_dwh, cargar_tablas, obtener_conexion, testfunciones, setup_logger,estandarizar_direccion,marcar_direcciones_estandarizadas\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 10:45\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-inicial-del-logger_1","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>ServicioSocial.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='ServicioSocial.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 10:45:43,742 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#consultas-para-extraer-datos-de-estudiantes-y-registros-de-saipi","title":"Consultas para Extraer Datos de Estudiantes y Registros de SAIPI","text":"<p>Este c\u00f3digo configura las consultas SQL para extraer datos de estudiantes y beneficiarios desde las bases de datos <code>nsijec</code> y <code>saipi</code>. El diccionario <code>qr_structure</code> contiene una consulta para la tabla de <code>estudiantes</code> en <code>nsijec</code>, extrayendo detalles personales y de inscripci\u00f3n. <code>qr_structureNeith</code> agrupa consultas para tablas en <code>saipi</code>, incluyendo <code>personas_saipi</code>, <code>acudientes</code>, <code>registros</code>, y <code>valoracion</code>, cada una de las cuales selecciona informaci\u00f3n detallada sobre identificaci\u00f3n, parentesco, registros de vinculaci\u00f3n y valoraciones m\u00e9dicas.</p> <p>El diccionario <code>dim_names</code> define nombres descriptivos para los resultados en <code>df_structure</code>, donde se almacenar\u00e1n estos datos para an\u00e1lisis y reportes. Esta configuraci\u00f3n asegura una carga organizada y estandarizada para un an\u00e1lisis de datos integral sobre los beneficiarios y sus relaciones en ambas bases de datos.</p> <pre><code>#Lista de querys\nqr_structure = {\n    \"estudiantes\":'''select \n                e.id AS ID_REGISTRO,\n                CONCAT(t.tipo,p.identificacion) AS ID_AFILIADO,\n                DATE_FORMAT(fecha_inscripcion, '%Y%m') AS PERIODO,\n                t.tipo as TIPO_DOCUMENTO,\n                p.identificacion as DOCUMENTO,\n                #e.persona AS ID_PERSONA, \n                #e.convenio AS NUMERO_CONVENIO,\n                con.convenio as NOMBRE_CONVENIO,\n                e.enfermedad_alergia,\n                e.estado,\n                e.fecha_inscripcion,\n                #e.grado,\n                gr.descripcion as GRADO,\n                #e.institucion,\n                ins.institucion,\n                ins.codigo as CODIGO_DANE_INSTITUCION,\n                e.nombre_enfermedad_alergia,\n                e.nombres_medicamentos,\n                e.observacion,\n                e.pob_estudiante,\n                e.toma_medicamentos,\n                e.trabajador_adolecente AS TRABAJADOR_ADOLESCENTE,\n                e.updated_at as FECHA_ACTUALIZACION\n                from nsijec.estudiantes e\n            INNER JOIN nsijec.personas p ON e.persona = p.id\n            INNER JOIN nsijec.tipos_idens t on p.tipo_identificacion = t.id\n            INNER JOIN nsijec.convenios con on e.convenio = con.id\n            INNER JOIN nsijec.grados gr on e.grado = gr.id\n            INNER JOIN nsijec.instituciones_educativas ins on e.institucion = ins.id\n\n\n            WHERE fecha_inscripcion &gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and fecha_inscripcion &lt;= curdate();'''\n    #,\n    #Tabla parientes vac\u00eda\n    #\"colegio13\":'''select \n    #            id AS ID_REGISTRO,\n    #            acudiente AS NOMBRE_ACUDIENTE,\n    #            created_at,\n    #            created_by,\n    #            direccion,\n    #            email,\n    #            estrato,\n    #            estudiante,\n    #            municipio,\n    #            parentesco,\n    #            pariente,\n    #            telefono,\n    #            updated_at,\n    #            updated_by\n    #            from nsijec.parientes'''                \n               }\n\n#Lista de querys Neith\nqr_structureNeith = {\n    \"personas_saipi\":'''select \n                p.id as ID_PERSONA,\n                CONCAT(t.referencia,p.identificacion) AS ID_AFILIADO,               \n                p.direccion,\n                p.telefono,\n                mun.codigo as CODIGO_CIUDAD,\n                p.estado,\n                p.created_at AS FECHA_CREACION,\n                p.updated_at AS FECHA_ACTUALIZACION\n                from saipi.personas p\n            INNER JOIN saipi.tipos_identificacion t on p.tipiden = t.id\n            INNER JOIN saipi.municipios mun on p.municipio_nacimiento = mun.id''',\n\n    \"acudientes\":'''SELECT \n                a.id as ID_ACUDIENTE,\n                a.registro_id AS ID_REGISTRO,\n                a.persona_id AS ID_PERSONA,\n                pa.detalle as PARENTESCO\n                FROM saipi.acudientes a\n                left join saipi.parentescos pa on a.parentesco_id = pa.id ''',\n\n    \"registros\":'''SELECT \n                    r.id AS ID_REGISTRO,\n                    r.persona AS ID_PERSONA,\n                    DATE_FORMAT(r.fecvin, '%Y%m') AS PERIODO,\n                    #r.convenio,\n                    c.detalle as CONVENIO,\n                    #r.modalidad,\n                    m.detalle as MODALIDAD,\n                    #r.tipben,\n                    b.detalle as TIPO_BENEFICIARIO,\n                    r.fecvin AS FECHA_VINCULACION,\n                    #r.municipio,\n                    mun.codigo as CODIGO_CIUDAD,\n                    #r.caracteristica_poblacion,\n                    cp.detalle as CARACTERISTICA_POBLACION,\n                    #r.poblacion,\n                    pb.detalle as POBLACION,\n                    r.sisben,\n                    r.puntaje AS PUNTAJE_SISBEN,\n                    r.icbf,\n                    r.discapacidad,\n                    r.afiliado,\n                    r.categoria,\n                    r.estafi AS ESTADO_AFILIACION,\n                    #r.pueblo,\n                    r.estado,\n                    r.created_at AS FECHA_CREACION,\n                    r.updated_at AS FECHA_ACTUALIZACION\n                FROM saipi.registros r\n            INNER JOIN saipi.convenios c on r.convenio = c.id\n            INNER JOIN saipi.modalidades m on r.modalidad = m.id\n            INNER JOIN saipi.beneficiarios b on r.tipben = b.id\n            INNER JOIN saipi.municipios mun on r.municipio = mun.id\n            INNER JOIN saipi.caracteristica_poblaciones cp on r.caracteristica_poblacion = cp.id\n            INNER JOIN saipi.poblaciones pb on r.poblacion = pb.id\n            WHERE DATE_FORMAT(r.fecvin, '%Y%m%01')&gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and r.fecvin &lt;=CURDATE()''',\n\n    \"valoracion\":'''SELECT \n            v.id AS ID_VALORACION,\n            v.registro_id AS ID_REGISTRO,\n            v.fecval AS FECHA_VALORACION,\n            v.talla,\n            v.peso,\n            v.p_braquial,\n            v.p_cefalico,\n            v.observaciones,\n            v.estado,\n            v.created_at AS FECHA_CREACION,\n            v.updated_at AS FECHA_ACTUALIZACION\n        FROM saipi.valoracion v\n        INNER JOIN (select \n        r.id, r.fecvin \n        FROM saipi.registros r\n        WHERE DATE_FORMAT(r.fecvin, '%Y%m%01')&gt;= DATE_SUB(CURDATE(), INTERVAL 18 MONTH) and r.fecvin &lt;=CURDATE() ) reg\n        on reg.id = v.registro_id'''\n    }\n\ndim_names = {\n    \"estudiantes\":'BD_Fact_nsijec_Estudiantes',\n    #\"colegio13\":'BD_Fact_nsijec_Parientes',\n    \"personas_saipi\" : 'BD_Dim_saipi_Personas',\n    \"acudientes\":'BD_Fact_saipi_Acudientes',\n    \"registros\":'BD_Fact_saipi_Registros',\n    \"valoracion\":'BD_Fact_saipi_Valoraciones'\n               }\ndf_structure = dict()\nlogger.info('LECTURA DE QUERYS')\n</code></pre> <pre><code>2024-10-30 10:45:43,759 - INFO - LECTURA DE QUERYS\n</code></pre>"},{"location":"seccion/cola_documentacion/#carga-de-tablas-desde-sql_1","title":"Carga de tablas desde SQL","text":"<p>Se ejecutan las consultas SQL definidas en <code>qr_structure</code> y los resultados se almacenan en el diccionario <code>df_structure</code>. Para cada consulta, se utiliza un bloque <code>with</code> para manejar la conexi\u00f3n de manera segura. El nombre de cada tabla cargada se registra en el log junto con el tiempo total del proceso de carga desde MySQL.</p> <pre><code># Cargar tablas desde la base 'minerva'\nmotor = create_engine(obtener_conexion('minerva'))\ncargar_tablas(motor, qr_structure, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 10:45:44,205 - INFO - Cargando estudiantes \n2024-10-30 10:45:45,745 - INFO - Cargada estudiantes, 19,777 registros finales obtenidos. --- 1.54 seconds ---\n2024-10-30 10:45:45,813 - INFO - CARGUE TABLAS DESDE MYSQL --- estudiantes --- 2.02 seconds ---\n</code></pre> <pre><code># Cargar tablas desde la base 'neith'\nmotorNeith = create_engine(obtener_conexion('neith'))\ncargar_tablas(motorNeith, qr_structureNeith, df_structure, logger)\n</code></pre> <pre><code>2024-10-30 10:45:46,301 - INFO - Cargando personas_saipi \n2024-10-30 10:45:46,973 - INFO - Cargada personas_saipi, 15,210 registros finales obtenidos. --- 0.67 seconds ---\n2024-10-30 10:45:46,975 - INFO - Cargando acudientes \n2024-10-30 10:45:47,122 - INFO - Cargada acudientes, 8,178 registros finales obtenidos. --- 0.15 seconds ---\n2024-10-30 10:45:47,123 - INFO - Cargando registros \n2024-10-30 10:45:47,727 - INFO - Cargada registros, 8,182 registros finales obtenidos. --- 0.60 seconds ---\n2024-10-30 10:45:47,728 - INFO - Cargando valoracion \n2024-10-30 10:45:48,264 - INFO - Cargada valoracion, 15,211 registros finales obtenidos. --- 0.54 seconds ---\n2024-10-30 10:45:48,330 - INFO - CARGUE TABLAS DESDE MYSQL --- valoracion --- 2.50 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#validador-de-campos-repetidos_1","title":"Validador de campos repetidos","text":"<p>Se verifica la existencia de registros duplicados en todas las tablas de <code>df_structure</code>, excluyendo la columna <code>id</code>. Los duplicados se almacenan utilizando la funci\u00f3n <code>StoreDuplicated</code>, y luego se eliminan los duplicados de cada tabla con <code>drop_duplicates()</code>. El proceso se registra en el log para cada tabla, junto con el tiempo total del validador de duplicados.</p> <pre><code>#Validador de campos repetidos \nvalidador_time = time.time()\nfor ky in list(df_structure.keys()):   \n    ColumnsToCompare = [x for x in df_structure[ky].columns.tolist() if x not in [ 'id' ] ]\n    StoreDuplicated('N/A' , ColumnsToCompare, df_structure[ky], r'\\trazaDuplicados_' + ky )\n    #Limpieza inicial quitar duplicados\n    df_structure[ky] = df_structure[ky].drop_duplicates()\n    logger.info('VALIDADOR TABLA: '+ ky)\nlogger.info(f'VALIDADOR DUPLICADOS --- {time.time() - validador_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:45:48,489 - INFO - VALIDADOR TABLA: estudiantes\n2024-10-30 10:45:48,530 - INFO - VALIDADOR TABLA: personas_saipi\n2024-10-30 10:45:48,547 - INFO - VALIDADOR TABLA: acudientes\n2024-10-30 10:45:48,602 - INFO - VALIDADOR TABLA: registros\n2024-10-30 10:45:48,629 - INFO - VALIDADOR TABLA: valoracion\n2024-10-30 10:45:48,631 - INFO - VALIDADOR DUPLICADOS --- 0.29 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#transformacion-y-limpieza-de-texto","title":"Transformaci\u00f3n y limpieza de texto","text":"<p>Se realiza una limpieza de las columnas de texto en cada tabla de <code>df_structure</code>. Las columnas de texto se convierten a may\u00fasculas (<code>UPPER</code>), se eliminan los espacios en blanco al inicio y al final, y se reemplazan los valores <code>\"NAN\"</code> y <code>\"NONE\"</code> por valores nulos (<code>NaN</code>). El tiempo total del proceso de limpieza se registra en el log.</p> <pre><code>#Transformaci\u00f3n 2\nlimpieza_time = time.time()\nfor ky in list(df_structure.keys()):\n    #print(ky)\n    # Convertir todos los nombres de las columnas a may\u00fasculas\n    df_structure[ky].columns = df_structure[ky].columns.str.upper()\n    #Pasar las columnas tipo texto a UPPER\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.upper())\n    #En las columnas tipo texto, eliminar espacios al comienzo y al final\n    df_structure[ky] = df_structure[ky].apply( lambda x: x.astype(str).str.strip())\n    #Reemplazar los \"NAN\" o \"NONE\" por NaN\n    df_structure[ky] = df_structure[ky].replace('NAN', np.nan)\n    df_structure[ky] = df_structure[ky].replace('NONE', np.nan)\nlogger.info(f'LIMPIEZA --- {time.time() - limpieza_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:45:49,671 - INFO - LIMPIEZA --- 1.03 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#estandarizar-direcciones","title":"Estandarizar direcciones","text":"<p>Este bloque de c\u00f3digo llama la funci\u00f3n estandarizaci\u00f3n de direcciones, lo cual permite limpiar las direcciones y marcar aquellas que empiezan con las siglas de carrera, calle, diagonal, transversal, kilometro, manzana, lote, carretera, finca o las que no registran como estandarizadas.</p> <pre><code>df_personas = df_structure['personas_saipi'].copy()\n\n# Asegurarse de que la columna DIRECCION_ORIGINAL es de tipo texto\ndf_personas.rename(columns={'DIRECCION':'DIRECCION_ORIGINAL'}, inplace=True)\ndf_personas['DIRECCION_ORIGINAL'] = df_personas['DIRECCION_ORIGINAL'].astype(str)\n\n# Aplicamos la estandarizaci\u00f3n a todas las direcciones y creamos una nueva columna con las direcciones estandarizadas\n\ndf_personas['DIRECCION_LIMPIA'] = df_personas['DIRECCION_ORIGINAL'].apply(estandarizar_direccion)\n\n# Agregar columna 'DIRECCION_ESTANDARIZADA' si la direccion empieza con algunas de las abreviaturas estandarizadas\nmarcar_direcciones_estandarizadas(df_personas, 'DIRECCION_LIMPIA')\n\ndf_structure['personas_saipi'] = df_personas.copy()\n#df_structure['personas_saipi']\n</code></pre>"},{"location":"seccion/cola_documentacion/#guardar-en-base-de-datos-dwh_3","title":"Guardar en base de datos DWH","text":"<p>Se guarda cada tabla de <code>df_structure</code> en la base de datos DWH utilizando los nombres definidos en <code>dim_names</code>. Si la clave de la tabla no est\u00e1 presente en <code>dim_names</code>, se genera una advertencia en el log. El tiempo total del proceso de almacenamiento se registra, junto con los nombres de las tablas que se almacenaron correctamente.</p> <pre><code>guardar_en_dwh(df_structure, dim_names, logger, multiple=True, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:45:50,048 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:45:50,051 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:45:50,462 - INFO - Almacenando tabla estudiantes en DWH como BD_Fact_nsijec_Estudiantes\n2024-10-30 10:45:55,969 - INFO - Tabla estudiantes almacenada correctamente como BD_Fact_nsijec_Estudiantes.\n2024-10-30 10:45:55,971 - INFO - Almacenando tabla personas_saipi en DWH como BD_Dim_saipi_Personas\n2024-10-30 10:45:59,065 - INFO - Tabla personas_saipi almacenada correctamente como BD_Dim_saipi_Personas.\n2024-10-30 10:45:59,066 - INFO - Almacenando tabla acudientes en DWH como BD_Fact_saipi_Acudientes\n2024-10-30 10:46:00,581 - INFO - Tabla acudientes almacenada correctamente como BD_Fact_saipi_Acudientes.\n2024-10-30 10:46:00,582 - INFO - Almacenando tabla registros en DWH como BD_Fact_saipi_Registros\n2024-10-30 10:46:03,558 - INFO - Tabla registros almacenada correctamente como BD_Fact_saipi_Registros.\n2024-10-30 10:46:03,559 - INFO - Almacenando tabla valoracion en DWH como BD_Fact_saipi_Valoraciones\n2024-10-30 10:46:07,558 - INFO - Tabla valoracion almacenada correctamente como BD_Fact_saipi_Valoraciones.\n2024-10-30 10:46:07,655 - INFO - ALMACENAMIENTO ---  --- 17.61 seconds ---\n2024-10-30 10:46:07,656 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:46:07,668 - INFO - FINAL ETL --- 23.94 seconds ---\n</code></pre>"},{"location":"seccion/cola_documentacion/#61-factencuestas","title":"6.1-FactEncuestas","text":""},{"location":"seccion/cola_documentacion/#61-factencuestas_1","title":"6.1-FactEncuestas","text":""},{"location":"seccion/cola_documentacion/#61-fact-encuestas","title":"6.1 Fact Encuestas","text":""},{"location":"seccion/cola_documentacion/#introduccion_4","title":"Introducci\u00f3n","text":"<p>El proceso ETL se enfoca en la obtenci\u00f3n, transformaci\u00f3n y almacenamiento de datos de encuestas en el Data Warehouse (DWH). Para ello, se realiza la extracci\u00f3n de datos desde diferentes tablas relacionadas con encuestas (<code>lime_questions</code>, <code>lime_answers</code>, entre otras) de bases de datos de encuestas y Neith, se transforman mediante limpieza y normalizaci\u00f3n, y finalmente, se cargan en la tabla <code>BD_Fact_Encuestas</code>. </p> <p>Se establecen conexiones a diferentes bases de datos para el acceso a tablas de encuestas y metadatos de usuarios, permitiendo crear relaciones entre identificadores, c\u00f3digos y respuestas de encuestas. El flujo del proceso de datos asegura que los datos se integren correctamente, permitiendo un an\u00e1lisis posterior en el DWH.</p>"},{"location":"seccion/cola_documentacion/#diagrama-de-secuencia-del-proceso-etl_5","title":"Diagrama de Secuencia del Proceso ETL","text":"<pre><code>sequenceDiagram\n  title Diagrama de Secuencia del Proceso ETL para la Tabla FactEncuestas\n  autonumber\n  participant \ud83d\udc64 Usuario\n  participant ETL\n  participant BD_DWH as Base de Datos DWH\n  participant BD_Neith as Base de Datos Neith\n  \ud83d\udc64 Usuario-&gt;&gt;ETL: Solicitar procesamiento de datos\n  ETL-&gt;&gt;BD_Neith: Extraer encuestas y preguntas\n  BD_Neith--&gt;&gt;ETL: Datos de encuestas y preguntas\n  ETL-&gt;&gt;BD_DWH: Extraer inventario de tablas y diccionario de campos\n  BD_DWH--&gt;&gt;ETL: Inventario y diccionario\n  ETL-&gt;&gt;ETL: Transformar datos (limpieza, estandarizaci\u00f3n)\n  ETL-&gt;&gt;BD_DWH: Cargar datos transformados en `BD_Fact_Encuestas`\n  BD_DWH--&gt;&gt;ETL: Confirmaci\u00f3n de carga exitosa\n  ETL--&gt;&gt;\ud83d\udc64 Usuario: Proceso ETL completado con \u00e9xito</code></pre>"},{"location":"seccion/cola_documentacion/#etl_5","title":"ETL","text":""},{"location":"seccion/cola_documentacion/#importacion-de-librerias-y-funciones_5","title":"Importaci\u00f3n de librer\u00edas y funciones","text":"<p>Este c\u00f3digo configura el entorno necesario para la manipulaci\u00f3n y carga de datos en un Data Warehouse, importando librer\u00edas esenciales (<code>pandas</code>, <code>sqlalchemy</code>, <code>BeautifulSoup</code> para procesamiento de HTML, y <code>logging</code>). Tambi\u00e9n incluye funciones personalizadas del archivo <code>Funciones.py</code>, como <code>guardar_en_dwh</code> para almacenar datos en el DWH, <code>Conexion_dwh</code> y <code>Conexion_neith</code> para la conexi\u00f3n a diferentes bases de datos, y <code>setup_logger</code> para el registro de eventos. La prueba <code>testfunciones()</code> verifica que las funciones se hayan importado correctamente y est\u00e1n listas para su uso.</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import create_engine, MetaData, Table, insert\nimport pandas as pd\nimport time \nimport os \nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport logging\nfrom datetime import date\n\nimport sys\nimport os\n# Agrega el directorio al sys.path\nsys.path.append(os.path.abspath('../funciones'))\n# Importa las funciones desde el archivo Funciones.py\nfrom Funciones import guardar_en_dwh, testfunciones, setup_logger, Conexion_dwh, Conexion_neith\n\nprint(testfunciones())\n</code></pre> <pre><code>Importacion de funciones correcta, 30-10-2024 10:26\n</code></pre>"},{"location":"seccion/cola_documentacion/#configuracion-inicial-del-logger_2","title":"Configuraci\u00f3n inicial del logger","text":"<p>Se configura el logger para registrar eventos del proceso ETL en el archivo <code>Encuestas.log</code>, utilizando el nivel de detalle <code>INFO</code>. Tambi\u00e9n se registra el inicio del proceso, y se almacena la hora de inicio para medir el tiempo total de ejecuci\u00f3n.</p> <pre><code>logger = setup_logger(log_filename='Encuestas.log', log_level=logging.INFO)\nlogger.info('COMIENZO ETL')\n</code></pre> <pre><code>2024-10-30 10:26:52,904 - INFO - COMIENZO ETL\n</code></pre>"},{"location":"seccion/cola_documentacion/#funciones-para-limpieza-de-html-y-carga-de-consultas-en-paralelo","title":"Funciones para Limpieza de HTML y Carga de Consultas en Paralelo","text":"<p>Este c\u00f3digo define dos funciones para manejar datos: <code>limpiar_html</code> usa <code>BeautifulSoup</code> para extraer el contenido de texto de un HTML, eliminando etiquetas para dejar solo texto limpio. La funci\u00f3n <code>load_query</code> ejecuta una consulta SQL y carga los resultados en un DataFrame (<code>df_query</code>). Dise\u00f1ada para uso con <code>ThreadPoolExecutor</code>, permite cargar varias consultas en paralelo y devuelve <code>None</code> en caso de error, sin interrumpir el flujo principal.</p> <pre><code>def limpiar_html(texto_html):\n    soup = BeautifulSoup(texto_html, 'html.parser')\n    return soup.get_text()\n###Librerias para paralelizar\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef load_query(query):\n    try: \n        df_query = pd.read_sql_query(query, motor_consulta)\n        return df_query\n    except:\n        return None\n</code></pre> <pre><code>#Medir tiempos\nstart_time = time.time()\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-a-la-base-de-datos-dwh_1","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion_pruebas = Conexion_dwh()\nmotor_pruebas = create_engine(cadena_conexion_pruebas)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 10:26:53,000 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-inventario-de-tablas-en-base-de-datos-de-encuestas","title":"Consulta de Inventario de Tablas en Base de Datos de Encuestas","text":"<p>Este bloque de c\u00f3digo consulta el inventario de tablas en la base de datos <code>encuestas</code> para identificar aquellas incluidas en la bodega de datos (<code>SeIncluyeEnBodega = \"Si\"</code>). La consulta se ejecuta en la tabla de inventario <code>gb_Dim_Bodega_Inventario de Tablas</code> en el Data Warehouse (<code>dwh</code>), filtrando las tablas del servidor con <code>IdServidor = 3</code>. El resultado se almacena en <code>df_tablas</code>, que contendr\u00e1 los nombres de las bases de datos y tablas relevantes para su posterior uso en an\u00e1lisis o cargas de datos en el entorno de encuestas.</p> <pre><code>##1. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla FROM dwh.`gb_Dim_Bodega_Inventario de Tablas`  \n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_tablas = pd.read_sql_query(sa.text(qr_structure), conn)\n#df_tablas\n</code></pre>"},{"location":"seccion/cola_documentacion/#extraccion-de-codigos-de-encuestas-desde-nombres-de-tablas","title":"Extracci\u00f3n de C\u00f3digos de Encuestas desde Nombres de Tablas","text":"<p>Este c\u00f3digo extrae los c\u00f3digos de encuestas (<code>sid</code>) a partir de la columna <code>NombreTabla</code> en <code>df_tablas</code>. Utiliza el m\u00e9todo <code>str.split</code> con <code>\"_\"</code> como delimitador y expande el resultado en varias columnas, seleccionando el tercer elemento (<code>[2]</code>) como el c\u00f3digo de encuesta. Luego, convierte <code>sid</code> a tipo <code>str</code> para asegurar la consistencia de formato, facilitando su uso en an\u00e1lisis o filtrado posterior.</p> <pre><code>## Lista de codigos de encuestas\n#df_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\")\ndf_tablas['sid'] = df_tablas['NombreTabla'].str.split(\"_\",expand = True)[2]\ndf_tablas[\"sid\"] = df_tablas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-a-la-base-de-datos-neith","title":"Conexi\u00f3n a la base de datos Neith","text":"<p>Se establece la conexi\u00f3n con la base de datos Neith utilizando la funci\u00f3n <code>Conexion_neith()</code> para generar la cadena de conexi\u00f3n y <code>create_engine()</code> para crear el motor de conexi\u00f3n. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base Neith\ncadena_conexion_neith = Conexion_neith()\nmotor_neith = create_engine(cadena_conexion_neith)\nlogger.info('CONEXION A BASE Neith')\n</code></pre> <pre><code>2024-10-30 10:26:53,541 - INFO - CONEXION A BASE Neith\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-nombres-de-encuestas-en-la-base-de-datos-de-limesurvey","title":"Consulta de Nombres de Encuestas en la Base de Datos de LimeSurvey","text":"<p>Este c\u00f3digo extrae los nombres de las encuestas desde la tabla <code>lime_surveys_languagesettings</code> en la base de datos <code>encuestas</code>. La consulta selecciona el ID de la encuesta (<code>surveyls_survey_id</code> como <code>sid</code>) y el t\u00edtulo (<code>surveyls_title</code> como <code>NombreEncuesta</code>). El resultado se almacena en el DataFrame <code>df_nombre_encuestas</code>, con la columna <code>sid</code> convertida a tipo <code>str</code> para facilitar su uso en futuras uniones o consultas. </p> <p>Este DataFrame proporciona una referencia de los nombres y IDs de las encuestas, \u00fatil para an\u00e1lisis de respuestas o estructuras en encuestas de LimeSurvey.</p> <pre><code>##2. Consultar nombres de las encuestas\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT surveyls_survey_id as sid, surveyls_title as NombreEncuesta \n    FROM encuestas.lime_surveys_languagesettings \"\"\"\n    df_nombre_encuestas = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_nombre_encuestas[\"sid\"] = df_nombre_encuestas[\"sid\"].astype(str)\n</code></pre>"},{"location":"seccion/cola_documentacion/#fusion-de-codigos-y-nombres-de-encuestas-y-exportacion-a-excel","title":"Fusi\u00f3n de C\u00f3digos y Nombres de Encuestas y Exportaci\u00f3n a Excel","text":"<p>Este c\u00f3digo combina los DataFrames <code>df_tablas</code> y <code>df_nombre_encuestas</code> mediante una uni\u00f3n izquierda (<code>merge</code>) en la columna <code>sid</code>, agregando los nombres de las encuestas (<code>NombreEncuesta</code>) a sus respectivos c\u00f3digos en <code>df_tablas</code>. Las filas con valores nulos resultantes de la combinaci\u00f3n se eliminan, y el \u00edndice se reinicia para un formato uniforme. </p> <p>El DataFrame resultante <code>df_tablas_nombres</code> se exporta a un archivo Excel (<code>df_tablas.xlsx</code>) para facilitar su visualizaci\u00f3n y an\u00e1lisis externo, mientras que <code>df_tablas_codigos</code> se define como una vista simplificada con solo las columnas <code>NombreBaseDeDatos</code>, <code>NombreTabla</code> y <code>sid</code>.</p> <pre><code>## Nombres de las tablas de encuestas\ndf_tablas_nombres = pd.merge(df_tablas,df_nombre_encuestas, how ='left', on = 'sid')\ndf_tablas_nombres = df_tablas_nombres.dropna().copy().reset_index(drop =True)\n</code></pre> <pre><code>df_tablas_nombres.to_excel('df_tablas.xlsx',index=False)\n</code></pre> <pre><code>df_tablas_codigos = df_tablas_nombres[['NombreBaseDeDatos', 'NombreTabla', 'sid']]\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-del-diccionario-de-campos-de-la-base-de-encuestas","title":"Consulta del diccionario de campos de la base de encuestas","text":"<p>Se ejecuta una consulta SQL para obtener los campos de las tablas de la base de datos <code>encuestas</code>, que est\u00e1n incluidas en la bodega de datos. La consulta selecciona los campos <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, y <code>NombreCampo</code> desde la tabla <code>gb_Dim_Bodega_Diccionario de Campos</code> en el DWH, filtrando por el servidor con <code>IdServidor = 3</code> y asegurando que los campos est\u00e9n marcados como incluidos en la bodega (<code>SeIncluyeEnBodega = \"Si\"</code>).</p> <p>El resultado de la consulta se carga en el dataframe <code>df_campos</code>.</p> <pre><code>##3. Consultar el diccionario con las tablas de la base de encuestas\nwith motor_pruebas.begin() as conn:\n    qr_structure = \"\"\"SELECT NombreBaseDeDatos,NombreTabla, NombreCampo FROM dwh.`gb_Dim_Bodega_Diccionario de Campos`\n    where IdServidor = 3 and SeIncluyeEnBodega = \"Si\" and NombreBaseDeDatos = \"encuestas\" \"\"\"\n    df_campos = pd.read_sql_query(sa.text(qr_structure), conn)\n</code></pre> <pre><code>df_campos_codigo = pd.merge(df_campos,df_tablas_codigos, how ='right', on = ['NombreBaseDeDatos', 'NombreTabla'])\n</code></pre>"},{"location":"seccion/cola_documentacion/#extraccion-y-transformacion-de-codigos-qid-y-parent_qid","title":"Extracci\u00f3n y transformaci\u00f3n de c\u00f3digos <code>qid</code> y <code>parent_qid</code>","text":"<p>Se crea la columna <code>qid</code> en el dataframe <code>df_campos_codigo</code>, extrayendo parte del nombre de campo dividiendo el texto por \"X\". Luego, se limpian los datos y se convierten los valores a formato <code>str</code>. Adem\u00e1s, se genera la columna <code>parent_qid</code> tomando los primeros d\u00edgitos del c\u00f3digo <code>qid</code>, mientras que <code>qid</code> contiene los \u00faltimos cinco caracteres.</p> <pre><code>df_campos_codigo['qid'] = df_campos_codigo['NombreCampo'].str.split(\"X\",expand = True)[2]\ndf_campos_codigo_qid = df_campos_codigo.dropna().copy().reset_index(drop =True)\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid[\"qid\"].astype(str)\ndf_campos_codigo_qid[\"parent_qid\"] = df_campos_codigo_qid['qid'].str[:-5]\ndf_campos_codigo_qid[\"qid\"] = df_campos_codigo_qid['qid'].str[-5:]\n</code></pre> <pre><code>#df_campos_codigo_qid.to_excel('t1.xlsx',index=False)\n</code></pre> <pre><code>#df_campos_codigo_qid['qid'].str[-5:]\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-la-tabla-lime_questions","title":"Consulta de la tabla <code>lime_questions</code>","text":"<p>Se ejecuta una consulta SQL para obtener los datos de la tabla <code>lime_questions</code> en la base de datos <code>encuestas</code>. La consulta selecciona las columnas <code>qid</code>, <code>parent_qid</code>, <code>sid</code>, y <code>title</code>. Posteriormente, las columnas <code>qid</code>, <code>parent_qid</code>, y <code>sid</code> se convierten al tipo <code>str</code> para asegurar una correcta manipulaci\u00f3n de datos. El resultado de la consulta se almacena en el dataframe <code>df_lime_questions</code>.</p> <pre><code>##4. Consultar la tabla lime_questions\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT qid, parent_qid, sid,title FROM encuestas.lime_questions \"\"\"\n    df_lime_questions = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions[\"qid\"] = df_lime_questions[\"qid\"].astype(str)\ndf_lime_questions[\"parent_qid\"] = df_lime_questions[\"parent_qid\"].astype(str)\ndf_lime_questions[\"sid\"] = df_lime_questions[\"sid\"].astype(str)\ndf_lime_questions\n</code></pre> qid parent_qid sid title 0 216 186 832591 SQ004 1 217 186 832591 SQ005 2 218 186 832591 SQ006 3 219 186 832591 SQ007 4 220 186 832591 SQ008 ... ... ... ... ... 1526 2753 0 354133 Q22 1527 2754 0 354133 Q23 1528 2755 0 354133 SUMRALO 1529 2756 0 354133 SUMTOTAL 1530 2757 0 354133 RESULTADO <p>1531 rows \u00d7 4 columns</p>"},{"location":"seccion/cola_documentacion/#ajuste-de-qid-para-las-tablas","title":"Ajuste de <code>qid</code> para las tablas","text":"<p>Se realiza un proceso de combinaci\u00f3n entre los dataframes <code>df_campos_codigo_qid</code> y <code>df_lime_questions</code> para arreglar los campos <code>qid</code>. Se ajustan los nombres de columnas y se completan los valores faltantes de <code>qid</code> utilizando los valores de respaldo. El dataframe resultante, <code>df_campos_2</code>, contiene las columnas clave como <code>NombreBaseDeDatos</code>, <code>NombreTabla</code>, <code>NombreCampo</code>, <code>sid</code>, y el <code>qid</code> corregido.</p> <pre><code>#Arreglar qid para las tablas\ndf_campos_1 = pd.merge( df_campos_codigo_qid , df_lime_questions , how ='left', on = ['sid', 'qid'])\ndf_campos_1.rename(columns={'title': 'title_1', 'parent_qid_x':'parent_qid'}, inplace=True)\ndf_campos_1['title'] = df_campos_1['qid']\ndf_campos_2 = pd.merge(df_campos_1,df_lime_questions, how ='left', on = ['sid', 'title', 'parent_qid'])\ndf_campos_2['qid_y'].fillna(df_campos_2['qid_x'], inplace=True)\ndf_campos_2 = df_campos_2[['NombreBaseDeDatos', 'NombreTabla', 'NombreCampo', 'sid', 'qid_y']].copy()\ndf_campos_2.rename(columns={'qid_y':'qid'}, inplace=True)\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-y-limpieza-de-preguntas-de-encuestas-lime_questions_l10ns","title":"Consulta y limpieza de preguntas de encuestas (<code>lime_questions_l10ns</code>)","text":"<p>Se consulta la tabla <code>lime_question_l10ns</code> de la base de datos <code>encuestas</code> para obtener informaci\u00f3n detallada sobre las preguntas de encuestas. Se combina este resultado con el dataframe <code>df_campos_2</code>, eliminando columnas irrelevantes como <code>help</code>, <code>script</code>, y <code>language</code>. Luego, se limpian las preguntas usando la funci\u00f3n <code>limpiar_html</code> para eliminar etiquetas HTML, y se eliminan los caracteres especiales, como corchetes. Finalmente, se filtran las filas para excluir registros con <code>qid</code> no v\u00e1lidos como <code>'count'</code> y <code>'other'</code>, y el dataframe se reorganiza con los datos limpios.</p> <pre><code>##4. Consultar la tabla lime_questions q10\nwith motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT * FROM encuestas.lime_question_l10ns \"\"\"\n    df_lime_questions_names = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_questions_names[\"qid\"] = df_lime_questions_names[\"qid\"].astype(str)\ndf_sol = pd.merge(df_campos_2,df_lime_questions_names, how='left', on='qid')\ndf_sol = df_sol.drop(['help', 'script', 'language'], axis=1)\ndf_sol['question'] = df_sol['question'].astype(str)\ndf_sol['question'] = df_sol['question'].str.strip()\ndf_sol['question_clean'] = df_sol['question'].apply(limpiar_html)\ndf_sol['question_clean'] = df_sol['question_clean'].str.replace(r'[\\[\\]]', '', regex=True)\ndf_clean = df_sol[~df_sol['qid'].isin(['count', 'other'])].copy().reset_index(drop =True)\n</code></pre> <pre><code>C:\\Users\\Consultor_QCS\\AppData\\Local\\Temp\\ipykernel_8392\\1878959599.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(texto_html, 'html.parser')\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-grupos-de-encuestas-lime_groups","title":"Consulta de grupos de encuestas (<code>lime_groups</code>)","text":"<p>Se ejecuta una consulta SQL para obtener los grupos de encuestas desde la tabla <code>lime_groups</code> y su descripci\u00f3n desde la tabla <code>lime_group_l10ns</code>. La consulta selecciona el <code>gid</code>, <code>sid</code>, y el nombre del grupo (<code>group_name</code>) como <code>DetalleTabla</code>. El resultado se almacena en el dataframe <code>df_lime_groups</code>, asegurando que la columna <code>sid</code> est\u00e9 en formato <code>str</code>.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT e1.gid, e1.sid, e2.group_name as DetalleTabla FROM encuestas.lime_groups as e1\n                    LEFT JOIN encuestas.lime_group_l10ns as e2\n                    ON e1.gid= e2.gid\"\"\"\n    df_lime_groups = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_groups['sid'] = df_lime_groups['sid'].astype(str)\n</code></pre> <pre><code>df_clean = pd.merge( df_clean , df_lime_groups , how='left', on='sid')\n</code></pre> <pre><code>df_clean\n</code></pre> NombreBaseDeDatos NombreTabla NombreCampo sid qid id question question_clean gid DetalleTabla 0 encuestas lime_survey_124282 124282X29X488 124282 488 488.0 &lt;p style=\"text-align: center;\"&gt;&lt;span style=\"fo... PLANCHA N\u00b01 \\n\\n\\n\\nPRINCIPALES\\n\\n\\n\\n\\n\\n\\n\\... 29 REPRESENTANTES 1 encuestas lime_survey_161825 161825X51X1103 161825 1103 1103.0 \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... \u00bfC\u00d3MO CALIFICA EN GENERAL LA CALIDAD DEL SERVI... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 2 encuestas lime_survey_161825 161825X51X1104 161825 1104 1104.0 \u00a1SU OPINION NOS INTERESA! Si desea registrar u... \u00a1SU OPINION NOS INTERESA! Si desea registrar u... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 3 encuestas lime_survey_161825 161825X51X1105SQ001 161825 1113 1113.0 LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT LA ATENCION DURANTE EL CHECK IN Y EL CHECK OUT 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 4 encuestas lime_survey_161825 161825X51X1105SQ002 161825 1114 1114.0 LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... LA ENTREGA DE LA CABA\u00d1A Y ATENCI\u00d3N BRINDADA PO... 51 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO ... ... ... ... ... ... ... ... ... ... ... 915 encuestas lime_survey_995137 995137X42X861SQ007 995137 877 877.0 E-mail: E-mail: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 916 encuestas lime_survey_995137 995137X42X862 995137 862 862.0 ESTADO LLAMADA: ESTADO LLAMADA: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 917 encuestas lime_survey_995137 995137X42X863 995137 863 863.0 A\u00d1O: A\u00d1O: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 918 encuestas lime_survey_995137 995137X42X864 995137 864 864.0 SEMESTRE: SEMESTRE: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 919 encuestas lime_survey_995137 995137X42X865 995137 865 865.0 MES: MES: 42 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE <p>920 rows \u00d7 10 columns</p> <pre><code>#df_sol.to_excel('df_sol.xlsx',index=False)\n#df_clean.to_excel('df_clean.xlsx',index=False)\n</code></pre>"},{"location":"seccion/cola_documentacion/#consulta-de-respuestas-de-encuestas-lime_answers","title":"Consulta de respuestas de encuestas (<code>lime_answers</code>)","text":"<p>Se ejecuta una consulta SQL para obtener las respuestas de encuestas desde la tabla <code>lime_answers</code> y sus descripciones desde la tabla <code>lime_answer_l10ns</code>. La consulta selecciona el <code>aid</code>, <code>qid</code>, <code>code</code>, y la respuesta (<code>answer</code>). El resultado de la consulta se almacena en el dataframe <code>df_lime_answers</code>, asegurando que las columnas <code>qid</code> y <code>aid</code> est\u00e9n en formato <code>str</code> para facilitar su manipulaci\u00f3n.</p> <pre><code>with motor_neith.begin() as conn:\n    qr_structure = \"\"\"SELECT a1.aid, a1.qid, a1.code, a2.answer \n                    FROM encuestas.lime_answers as a1\n                    LEFT JOIN encuestas.lime_answer_l10ns as a2\n                    ON a1.aid = a2.aid\"\"\" \n\n    df_lime_answers = pd.read_sql_query(sa.text(qr_structure), conn)\ndf_lime_answers['qid'] = df_lime_answers['qid'].astype(str)\ndf_lime_answers['aid'] = df_lime_answers['aid'].astype(str)\n</code></pre> <pre><code>tables_survey_names = df_clean['NombreTabla'].unique().tolist()\n</code></pre>"},{"location":"seccion/cola_documentacion/#proceso-de-limpieza-y-estandarizacion-de-datos-en-encuestas","title":"Proceso de Limpieza y Estandarizaci\u00f3n de Datos en Encuestas","text":"<p>Este bloque de c\u00f3digo extrae y estandariza datos de m\u00faltiples tablas de encuestas (<code>tables_survey_names</code>), realizando operaciones de limpieza, validaci\u00f3n, y normalizaci\u00f3n de campos relevantes como identificadores y fechas. Primero, cada tabla de encuestas se carga en <code>df_survey</code> y se mapea su estructura con <code>df_clean</code> y <code>df_lime_answers</code>, combinando campos y respuestas relacionadas. Solo se conservan los campos con valores v\u00e1lidos, descartando aquellos sin relaci\u00f3n en <code>df_lime_answers</code>.</p> <p>Para cada tabla de encuestas, se renombra y estandariza los nombres de las columnas identificadoras (<code>C\u00e9dula:</code>, <code>N\u00famero Documento de Identidad</code>, etc.) a <code>documento</code> y <code>tipo_documento</code>. Luego, se utiliza la funci\u00f3n <code>melt</code> para transformar las tablas en un formato largo (<code>Pregunta</code>, <code>Respuesta</code>), facilitando su an\u00e1lisis y manipulaci\u00f3n. Adem\u00e1s, se eliminan espacios y tabulaciones no deseados en los valores de texto, reemplazando valores faltantes con <code>NaN</code>.</p> <p>Las tablas finales son agregadas en <code>tablesToConcat</code> para unificaci\u00f3n y exportaci\u00f3n, mientras que las que no cumplen con los criterios se guardan como archivos CSV en el directorio actual para su revisi\u00f3n. La columna <code>DetalleEncuesta</code> tambi\u00e9n se a\u00f1ade a cada tabla, proporcionando contexto adicional.</p> <pre><code>df_survey = dict()\ncolumnsStatic = ['C\u00e9dula:','NIT:','Tipo Documento de Identidad','N\u00famero Documento de Identidad','Tipo Documento Identidad', 'Numero de identificaci\u00f3n']\ncolumnsId = ['C\u00e9dula:', 'N\u00famero Documento de Identidad', 'Numero de identificaci\u00f3n' ]\ncolumnsTipoId = ['Tipo Documento de Identidad', 'Tipo Documento Identidad' ]\ntablesToConcat = []\n\nfor table_name in tables_survey_names:\n    print(table_name)\n    with motor_neith.begin() as conn:\n        qr_structure = \"\"\"SELECT * FROM encuestas. \"\"\" + table_name\n        df_survey[table_name] = pd.read_sql_query(sa.text(qr_structure), conn)\n\n    dfColumns = pd.DataFrame({'NombreCampo':df_survey[table_name].columns.tolist()})\n    dfColumns_p2 = pd.merge( dfColumns , df_clean , how = 'left' , on = ['NombreCampo'] )\n    dfColumns_p2 = dfColumns_p2[ ~dfColumns_p2['NombreTabla'].isna() ]\n    dfColumns_p2 = dfColumns_p2[['NombreCampo','NombreTabla','sid','qid','question_clean']]\n    dfToValidateAnswer = pd.merge( dfColumns_p2 , df_lime_answers , how = 'left' , on = ['qid'] )\n    dfToValidateAnswer = dfToValidateAnswer[~dfToValidateAnswer['aid'].isna()]\n    columnsTVA = dfToValidateAnswer['NombreCampo'].unique().tolist()\n    df_survey[table_name] = df_survey[table_name][['submitdate'] + dfColumns_p2['NombreCampo'].unique().tolist()] \n    for col in df_survey[table_name].columns.tolist():\n        if col in columnsTVA:\n            df_survey[table_name][col + '_answer'] = pd.merge( df_survey[table_name][[col]] , dfToValidateAnswer[ dfToValidateAnswer['NombreCampo'] == col ] , how = 'left' , left_on = col, right_on = 'code' )['answer']\n            df_survey[table_name] = df_survey[table_name].drop([col], axis=1)\n            df_survey[table_name] = df_survey[table_name].rename({ col + '_answer': col }, axis=1)\n        newName = dfColumns_p2[ dfColumns_p2['NombreCampo'] == col ]['question_clean'].tolist()\n\n        if len(newName) != 0:\n            df_survey[table_name] = df_survey[table_name].rename({ col: newName[0]  }, axis=1)\n\n    columnsToMantain = [x for x in df_survey[table_name].columns.tolist() if x in columnsStatic]\n    df_survey[table_name].fillna(value=np.nan, inplace=True)\n    try:\n        if len(columnsToMantain) == 0:\n            print('No aplica', os.getcwd() +'\\\\' +table_name +'.csv')\n            df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n        elif 'FECHA:' in df_survey[table_name].columns.tolist():\n            print('tiene fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\",'FECHA:'] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        else:\n            print('sin fecha')\n            df_survey[table_name] =  df_survey[table_name].melt(id_vars=[\"submitdate\"] + columnsToMantain, var_name=\"Pregunta\", value_name=\"Respuesta\")\n            df_survey[table_name][\"Respuesta\"] = df_survey[table_name][\"Respuesta\"].astype(str).apply(lambda x: x.replace(\"\\t\" , \" \" ))\n            tablesToConcat.append(table_name)\n        for colClean in columnsToMantain:\n            df_survey[table_name][colClean] = df_survey[table_name][colClean].astype(str).apply(lambda x: x.replace(\"\\t\" , \"\" ))\n\n    except:\n        print('posible vacio')\n        df_survey[table_name].to_csv(os.getcwd() + '\\\\' + table_name +'.csv')\n\n\n    df_survey[table_name] = df_survey[table_name].replace('nan', np.nan)    \n    df_survey[table_name].dropna( subset = columnsToMantain , inplace=True)\n    df_survey[table_name]['DetalleEncuesta'] = df_clean[df_clean['NombreTabla'] == table_name]['DetalleTabla'].tolist()[0] \n\n    for col in columnsId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'documento' }, axis=1)\n    for col in columnsTipoId:\n        df_survey[table_name] = df_survey[table_name].rename({ col: 'tipo_documento' }, axis=1)\n</code></pre> <pre><code>lime_survey_124282\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_124282.csv\nlime_survey_161825\ntiene fecha\nlime_survey_189858\ntiene fecha\nlime_survey_267495\ntiene fecha\nlime_survey_385698\nsin fecha\nlime_survey_413658\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_413658.csv\nlime_survey_452416\ntiene fecha\nlime_survey_478847\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_478847.csv\nlime_survey_548818\ntiene fecha\nlime_survey_567215\nsin fecha\nlime_survey_576377\ntiene fecha\nlime_survey_586872\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_586872.csv\nlime_survey_599513\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_599513.csv\nlime_survey_699399\nsin fecha\nposible vacio\nlime_survey_757259\nsin fecha\nlime_survey_767954\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_767954.csv\nlime_survey_773674\nNo aplica C:\\Users\\Consultor_QCS\\Documents\\Local\\Cajamag\\02.Gobierno_De_Datos\\03.Ejecuci\u00f3n\\18.Bodega\\Entrega\\Bloques\\Bloque6\\lime_survey_773674.csv\nlime_survey_818378\nsin fecha\nlime_survey_832591\nsin fecha\nlime_survey_934423\ntiene fecha\nlime_survey_959511\ntiene fecha\nlime_survey_995137\ntiene fecha\n</code></pre>"},{"location":"seccion/cola_documentacion/#concatenacion-final-de-encuestas","title":"Concatenaci\u00f3n final de encuestas","text":"<p>Se filtran las tablas procesadas en <code>df_survey</code> que est\u00e1n presentes en la lista <code>tablesToConcat</code> y se almacenan en el diccionario <code>df_survey_2</code>. Luego, se concatenan todas las tablas seleccionadas en un \u00fanico dataframe <code>df_survey_final</code>, combinando los resultados y asegurando un \u00edndice continuo.</p> <pre><code>df_survey_2 = dict( filter(lambda item: item[0] in tablesToConcat, df_survey.items()) )\ndf_survey_final =  pd.concat( list(df_survey_2.values()) , ignore_index = True )\n</code></pre>"},{"location":"seccion/cola_documentacion/#conexion-transformacion-y-estandarizacion-de-datos-de-encuestas","title":"Conexi\u00f3n, Transformaci\u00f3n y Estandarizaci\u00f3n de Datos de Encuestas","text":"<p>Este c\u00f3digo conecta a la base de datos DWH, consulta datos de <code>BD_Dim_Datos_Fijos</code>, y realiza operaciones de limpieza y normalizaci\u00f3n en <code>df_survey_final</code>. </p> <p>Primero, se establece la conexi\u00f3n mediante <code>motor3</code> y se extrae <code>DOCUMENTO</code> y <code>CODDOC</code> de <code>BD_Dim_Datos_Fijos</code>, que luego se combina con <code>df_survey_final</code> para asignar el tipo de documento (<code>tipo_documento</code>), reemplazando valores faltantes por \"CC\". La columna <code>ID_AFILIADO</code> se crea concatenando <code>tipo_documento</code> y <code>documento</code>.</p> <p>A continuaci\u00f3n, las columnas se renombran para facilitar su identificaci\u00f3n y el campo <code>FECHA</code> se convierte al tipo <code>datetime</code>, permitiendo generar el campo <code>PERIODO</code> en formato <code>YYYYMM</code>. Las columnas se reordenan, colocando <code>ID_AFILIADO</code> al inicio para una organizaci\u00f3n clara, y finalmente se verifica el resultado en los logs.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion3 = Conexion_dwh()\nmotor3 = create_engine(cadena_conexion3)\nlogger.info('CONEXION A BASE DWH')\n\n\n# Bloque 1: Consulta y carga de datos\nlogger.info(\"Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\")\nquery = \"SELECT DOCUMENTO, CODDOC FROM BD_Dim_Datos_Fijos\"\ndf_datos_fijos = pd.read_sql(query, con=motor3)\nlogger.info(f\"Consulta a BD_Dim_Datos_Fijos completada: {df_datos_fijos.shape[0]} filas cargadas.\")\n\n# Bloque 2: Merge y limpieza de columnas\nlogger.info(\"Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\")\ndf_survey_final = pd.merge(df_survey_final, df_datos_fijos, how='left', left_on='documento', right_on='DOCUMENTO')\ndf_survey_final = df_survey_final.drop(columns=['DOCUMENTO'])\ndf_survey_final['tipo_documento'] = df_survey_final['CODDOC'].fillna('CC')\ndf_survey_final = df_survey_final.drop(columns=['CODDOC'])\n\n# Bloque 3: Generaci\u00f3n de ID y reorganizaci\u00f3n de columnas\nlogger.info(\"Generando 'ID_AFILIADO' y reorganizando columnas.\")\ndf_survey_final['ID_AFILIADO'] = df_survey_final['tipo_documento'].astype(str) + df_survey_final['documento'].astype(str)\n\n# Renombrar columnas\ndf_survey_final = df_survey_final.rename(columns={\n    'submitdate': 'FECHA_ENCUESTA',\n    'FECHA:' : 'FECHA',\n    'documento': 'DOCUMENTO',\n    'Pregunta': 'PREGUNTA',\n    'Respuesta': 'RESPUESTA',\n    'DetalleEncuesta': 'NOMBRE_ENCUESTA',\n    'NIT:': 'NIT_EMPRESA',\n    'tipo_documento': 'TIPO_DOCUMENTO',\n})\n\n# Asegurarse que FECHA es de tipo datetime\ndf_survey_final['FECHA'] = pd.to_datetime(df_survey_final['FECHA'])\n\n# Crear el campo PERIODO con el formato 'YYYYMM'\ndf_survey_final['PERIODO'] = df_survey_final['FECHA'].dt.strftime('%Y%m')\n\n\n# Reordenar las columnas colocando 'ID_AFILIADO' en la primera posici\u00f3n\ncolumnas = ['ID_AFILIADO', 'PERIODO', 'FECHA_ENCUESTA', 'FECHA', 'TIPO_DOCUMENTO','DOCUMENTO',\n       'NOMBRE_ENCUESTA','PREGUNTA', 'RESPUESTA', 'NIT_EMPRESA', ]\ndf_survey_final = df_survey_final.reindex(columns=columnas)\n\n# Mostrar las columnas finales\nlogger.info(f\"Proceso completado. Columnas finales: {df_survey_final.columns.tolist()}\")\n</code></pre> <pre><code>2024-10-30 10:27:02,768 - INFO - CONEXION A BASE DWH\n2024-10-30 10:27:02,770 - INFO - Iniciando consulta a BD_Dim_Datos_Fijos y carga de datos.\n2024-10-30 10:27:08,813 - INFO - Consulta a BD_Dim_Datos_Fijos completada: 970252 filas cargadas.\n2024-10-30 10:27:08,814 - INFO - Realizando merge entre df_survey_final y BD_Dim_Datos_Fijos, seguido de limpieza de columnas.\n2024-10-30 10:27:09,197 - INFO - Generando 'ID_AFILIADO' y reorganizando columnas.\n2024-10-30 10:27:09,687 - INFO - Proceso completado. Columnas finales: ['ID_AFILIADO', 'PERIODO', 'FECHA_ENCUESTA', 'FECHA', 'TIPO_DOCUMENTO', 'DOCUMENTO', 'NOMBRE_ENCUESTA', 'PREGUNTA', 'RESPUESTA', 'NIT_EMPRESA']\n</code></pre> <pre><code># Convertir el campo PERIODO a datetime para poder filtrar\ndf_survey_final['PERIODO_DT'] = pd.to_datetime(df_survey_final['PERIODO'] + '01', format='%Y%m%d')\n\n# Calcular la fecha l\u00edmite de 18 meses atr\u00e1s desde la fecha actual y ajustarla al primer d\u00eda del mes siguiente\nfecha_limite = pd.Timestamp.today() - pd.DateOffset(months=18)\nprimer_dia_mes_siguiente = (fecha_limite + pd.offsets.MonthBegin(1)).normalize()\n\n# Filtrar los resultados para los \u00faltimos 18 meses a partir del campo PERIODO\ndf_filtrado = df_survey_final[df_survey_final['PERIODO_DT'] &gt;= primer_dia_mes_siguiente]\n\n# Eliminar el campo 'PERIODO_DT'\ndf_survey_final = df_filtrado.drop(columns=['PERIODO_DT','NIT_EMPRESA']).copy()\ndf_survey_final\n</code></pre> ID_AFILIADO PERIODO FECHA_ENCUESTA FECHA TIPO_DOCUMENTO DOCUMENTO NOMBRE_ENCUESTA PREGUNTA RESPUESTA 0 CC4158282 202404 2024-04-10 10:53:55 2024-04-10 CC 4158282 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA LOPEZ LUIS 1 CC79690928 202404 2024-04-10 10:57:38 2024-04-10 CC 79690928 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: ESPINOSA TORO JOSE JAVIER 2 CC1032470737 202404 2024-04-10 11:00:04 2024-04-10 CC 1032470737 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN NEIFY ESPERANZA 3 CC1073609064 202404 2024-04-10 11:03:14 2024-04-10 CC 1073609064 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MONICA 4 CC1056029010 202404 2024-04-10 11:05:07 2024-04-10 CC 1056029010 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MARTA ... ... ... ... ... ... ... ... ... ... 282363 CC1081918879 202407 2024-07-17 10:02:42 2024-07-17 CC 1081918879 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282364 CC57293976 202407 2024-07-17 10:04:07 2024-07-17 CC 57293976 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282365 CC1081915588 202407 2024-07-17 10:06:40 2024-07-17 CC 1081915588 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282366 CC1082243440 202407 2024-07-17 10:08:00 2024-07-17 CC 1082243440 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 282367 CC1081916237 202407 2024-07-17 10:09:58 2024-07-17 CC 1081916237 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: sandra.perez <p>78778 rows \u00d7 9 columns</p>"},{"location":"seccion/cola_documentacion/#crear-dim-encuestas","title":"Crear Dim Encuestas","text":"<p>En esta parte se identifican las encuestas \u00fanicas y se asigna un indice que sirve como llave primaria. Luego se cruza con la df_survey_final para asignar la llave correspondiente</p> <pre><code># Encontrar valor unicos de nombres de encuestas\ndf_unique_nombre_encuesta = pd.DataFrame(df_survey_final['NOMBRE_ENCUESTA'].unique(), columns=['NOMBRE_ENCUESTA'])\n\n# Ordenar por orden alfab\u00e9tico\ndf_unique_nombre_encuesta = df_unique_nombre_encuesta.sort_values(by='NOMBRE_ENCUESTA').reset_index(drop=True)\n\n# Asignar un \u00edndice empezando desde 1\ndf_unique_nombre_encuesta['ID_ENCUESTA'] = df_unique_nombre_encuesta.index + 1\n\n# Reorganizar las columnas para tener 'Index' como la primera columna\ndf_unique_nombre_encuesta = df_unique_nombre_encuesta[['ID_ENCUESTA', 'NOMBRE_ENCUESTA']]\ndf_unique_nombre_encuesta\n</code></pre> ID_ENCUESTA NOMBRE_ENCUESTA 0 1 ENCUESTA DE SATISFACCION AFILIACION DE EMPLEAD... 1 2 ENCUESTA DE SATISFACCION DE AFILIACION DEL TRA... 2 3 ENCUESTA DE SATISFACCION ESCUELAS DEPORTIVAS 3 4 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE 4 5 ENCUESTA DE SATISFACCION SERVICIO DE ALIMENTOS... 5 6 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO 6 7 ENCUESTA DE SATISFACCION SERVICIO DE RECREACION 7 8 ENCUESTA DE SATISFACCION SERVICIO DE VACUNACION 8 9 ENCUESTA DE SATISFACCI\u00d3N DE VIVIENDA FOVIS <pre><code># Realizar el inner join con el DataFrame df_survey_final por el campo NOMBRE_ENCUESTA\ndf_survey_final = pd.merge(df_survey_final, df_unique_nombre_encuesta, on='NOMBRE_ENCUESTA', how='inner')\ndf_survey_final\n</code></pre> ID_AFILIADO PERIODO FECHA_ENCUESTA FECHA TIPO_DOCUMENTO DOCUMENTO NOMBRE_ENCUESTA PREGUNTA RESPUESTA ID_ENCUESTA 0 CC4158282 202404 2024-04-10 10:53:55 2024-04-10 CC 4158282 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA LOPEZ LUIS 6 1 CC79690928 202404 2024-04-10 10:57:38 2024-04-10 CC 79690928 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: ESPINOSA TORO JOSE JAVIER 6 2 CC1032470737 202404 2024-04-10 11:00:04 2024-04-10 CC 1032470737 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN NEIFY ESPERANZA 6 3 CC1073609064 202404 2024-04-10 11:03:14 2024-04-10 CC 1073609064 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MONICA 6 4 CC1056029010 202404 2024-04-10 11:05:07 2024-04-10 CC 1056029010 ENCUESTA DE SATISFACCION SERVICIO DE ALOJAMIENTO Nombres y Apellidos: MURCIA FLORIAN MARTA 6 ... ... ... ... ... ... ... ... ... ... ... 78773 CC1081918879 202407 2024-07-17 10:02:42 2024-07-17 CC 1081918879 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78774 CC57293976 202407 2024-07-17 10:04:07 2024-07-17 CC 57293976 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78775 CC1081915588 202407 2024-07-17 10:06:40 2024-07-17 CC 1081915588 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78776 CC1082243440 202407 2024-07-17 10:08:00 2024-07-17 CC 1082243440 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: katty.jimenez 4 78777 CC1081916237 202407 2024-07-17 10:09:58 2024-07-17 CC 1081916237 ENCUESTA DE SATISFACCION SERVICIO AL CLIENTE Nombre del Funcionario que aplic\u00f3 la encuesta: sandra.perez 4 <p>78778 rows \u00d7 10 columns</p>"},{"location":"seccion/cola_documentacion/#conexion-a-la-base-de-datos-dwh_2","title":"Conexi\u00f3n a la base de datos DWH","text":"<p>Se establece la conexi\u00f3n con la base de datos DWH utilizando la funci\u00f3n <code>Conexion_dwh()</code> y se crea el motor de conexi\u00f3n con <code>create_engine()</code>. El evento de conexi\u00f3n exitosa se registra en el log.</p> <pre><code>#---------------------------------------------\n#Conexion a base dwh\ncadena_conexion2 = Conexion_dwh()\nmotor2 = create_engine(cadena_conexion2)\nlogger.info('CONEXION A BASE DWH')\n</code></pre> <pre><code>2024-10-30 10:27:09,850 - INFO - CONEXION A BASE DWH\n</code></pre>"},{"location":"seccion/cola_documentacion/#guardar-en-base-de-datos-dwh_4","title":"Guardar en base de datos DWH","text":"<p>Se guarda el dataframe <code>df_survey_final</code> en la tabla <code>BD_Fact_Encuestas</code> de la base de datos DWH. Si la tabla ya existe, se reemplaza su contenido con los nuevos datos. Se registra el tiempo total de almacenamiento en el log y, una vez completado el proceso, tambi\u00e9n se registra el tiempo total de ejecuci\u00f3n del proceso ETL.</p> <pre><code>guardar_en_dwh(df_survey_final, 'BD_Fact_Encuestas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:27:09,860 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:27:09,862 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:27:10,261 - INFO - Almacenando tabla \u00fanica en DWH como BD_Fact_Encuestas\n2024-10-30 10:27:22,427 - INFO - Tabla almacenada correctamente. 78,778 registros finales obtenidos.\n2024-10-30 10:27:22,542 - INFO - ALMACENAMIENTO ---  --- 12.68 seconds ---\n2024-10-30 10:27:22,542 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>guardar_en_dwh(df_unique_nombre_encuesta, 'BD_Dim_Encuestas', logger, multiple=False, if_exists='replace')\n</code></pre> <pre><code>2024-10-30 10:27:22,549 - INFO - Intentando conexi\u00f3n a la base DWH...\n2024-10-30 10:27:22,551 - INFO - Conexi\u00f3n a la base DWH establecida con \u00e9xito.\n2024-10-30 10:27:22,940 - INFO - Almacenando tabla \u00fanica en DWH como BD_Dim_Encuestas\n2024-10-30 10:27:23,504 - INFO - Tabla almacenada correctamente. 9 registros finales obtenidos.\n2024-10-30 10:27:23,580 - INFO - ALMACENAMIENTO ---  --- 1.03 seconds ---\n2024-10-30 10:27:23,581 - INFO - Finalizando proceso de almacenamiento en DWH.\n</code></pre> <pre><code>logger.info(f'FINAL ETL --- {time.time() - start_time:.2f} seconds ---')\n</code></pre> <pre><code>2024-10-30 10:27:57,385 - INFO - FINAL ETL --- 64.46 seconds ---\n</code></pre>"}]}